[
  {
    "number": 14137,
    "title": "Misc. bug: llama-server drops multi-part content for final assistant message",
    "body": "### Name and Version\n\nversion: 5640 (2e89f76b)\nbuilt with Apple clang version 17.0.0 (clang-1700.0.13.3) for arm64-apple-darwin24.5.0\n\n### Operating systems\n\nMac, Windows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nllama-server -m <any> --chat-template chatml\n```\n\n### Problem description & steps to reproduce\n\nEndpoints that support OpenAI format seem to handle multi-part message content fine in most cases; however, when the final message is an assistant message, the content ends up being dropped. I tracked this down to the assistant prefill logic:\n\nhttps://github.com/ggml-org/llama.cpp/blob/2e89f76b7af2c0b827be785e445f2e2b3e52e1ca/tools/server/utils.hpp#L774\n\nI think this should handle `content_parts` unless there's a specific reason for it to be unsupported. I can see an argument that it would be odd for a prefill message to be anything but a single string, but it's still a surprising limitation (in my case, the input was a single string, but it's going through a conversion that always produces multi-part messages).\n\nWorks:\n```console\n$ curl -X POST --location \"http://localhost:8080/apply-template\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"messages\": [\n            {\n              \"role\": \"assistant\",\n              \"content\": \"Test assistant message\"\n            }\n          ]\n        }'\n```\n\nWorks:\n```console\n$ curl -X POST --location \"http://localhost:8080/apply-template\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"messages\": [\n            {\n              \"role\": \"assistant\",\n              \"content\": [{\"type\": \"text\", \"text\": \"Test assistant message\"}]\n            },\n            {\n              \"role\": \"user\",\n              \"content\": [{\"type\": \"text\", \"text\": \"Test user message\"}]\n            }\n          ]\n        }'\n```\n\nFails:\n```console\n$ curl -X POST --location \"http://localhost:8080/apply-template\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n          \"messages\": [\n            {\n              \"role\": \"user\",\n              \"content\": [{\"type\": \"text\", \"text\": \"Test user message\"}]\n            },\n            {\n              \"role\": \"assistant\",\n              \"content\": [{\"type\": \"text\", \"text\": \"Test assistant message\"}]\n            }\n          ]\n        }'\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-12T00:15:47+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14137"
  },
  {
    "number": 13549,
    "title": "Misc. bug: Potential out of bound in rerank",
    "body": "### Name and Version\n\nversion: 5387 (3198405e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n[llama_context](https://github.com/ggml-org/llama.cpp/blob/f5170c1d7a66222ca7c75d2022fec3ed87257e0b/src/llama-context.cpp#L807) resize the rerank output to size 1 while [here](https://github.com/ggml-org/llama.cpp/blob/017f10b5fa630a013ec4f9936e410a60d4f460d5/examples/embedding/embedding.cpp#L69) we still normalize it as if we have full embedding vector. I found this problem happened randomly in python binding but cannot reproduce it in cpp. Not sure if it is a bug in cpp side.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-14T19:50:38+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13549"
  },
  {
    "number": 14027,
    "title": "Feature Request: add a new repo for convertion of gguf",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nHello.\n\nI would like to split convert_hf_to_gguf.py to a new repo. Can you create another package just for conversion.\n\n### Motivation\n\nI am always frustruated when I want to convert gguf files because I have to run:\n\n```bash\ngit clone https://github.com/ggerganov/llama.cpp.git\npip install -r llama.cpp/requirements.txt\npython llama.cpp/convert_hf_to_gguf.py ./OUT-MODEL/ --outfile model.gguf --outtype q8_0\n```\n\nWhile I just want to convert. I do not need the llamacpp inference model.\n\n### Possible Implementation\n\nPython to a pypi package. Or if possible in C/C++/rust for performance. I personally love rust.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-05T11:34:53+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14027/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14027"
  },
  {
    "number": 13028,
    "title": "Feature Request: Ability to pack multiple GGUFs into single one",
    "body": "### Feature Description\n\nFrom an idea brought up by @ggerganov in this discussion: https://github.com/ggml-org/llama.cpp/discussions/11139#discussioncomment-11783418\n\nWhile it is **NOT** a good idea to pack both mmproj + text models (because vision support is still messy atm), we still have some interesting use cases:\n\n- For TTS models, this can be useful because some models may requires more than 2 GGUFs to run (for ex. Sesame CSM requires backbone, decoder and Mimi models)\n- For phi-4-mm model, while the mmproj can't be packed, it is still interesting to pack the LoRA adapters and the text model together\n- There are some techniques which use LoRA to recover quality loss due to quantization, it can be useful to pack LoRA with the model (though, I don't know how effective this can be, cc @compilade )\n- Some models having more than 1 modality (i.e.Phi-4-mm with both audio+vision input), so could be useful to pack audio encoder and vision encoder into single GGUF\n\n### Motivation\n\nI create this issue to discuss about possible implementation\n\n### Possible Implementation\n\nAn implementation could be to have \"namespace\" for KV metadata and tensor name, then have a \"super\" key for the list of namespaces\n\nFor example, with the case of Sesame CSM, given 2 GGUFs: backbone and decoder, the routine to pack these 2 GGUFs is as follow:\n- We create a blank GGUF\n- Add metadata `general.namespaces = [\"backbone\", \"decoder\"]`\n- Copy all metadata + tensors from backbone while adding `backbone.` prefix to the key name\n- Copy all metadata + tensors from decoder while adding `decoder.` prefix to the key name\n\nThese APIs will need to be added into `libllama`:\n- `int32_t llama_model_n_namespaces(llama_model * model)`: returns the number of namespaces, 0 meaning no namespace\n- `const char ** llama_model_list_namespaces(llama_model * model)`: returns the list of namespace as strings\n- `llama_model * llama_model_get_namespace(int idx)`: returns the sub `llama_model *` object corresponding to a namespace index\n\n### Problems\n\n1. For existing models (like TTS), how to we make a smooth transition to the new packed format? Or probably accept breaking changes since not many people are using it anyway?\n2. How can we design the API such that it implies the least change to user code?",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-19T19:09:58+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13028/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13028"
  },
  {
    "number": 14110,
    "title": "Eval bug: (MAC) fail in `GGML_METAL_ADD_KERNEL(GGML_METAL_KERNEL_TYPE_FLASH_ATTN_EXT_Q8_0_H96,         flash_attn_ext_q8_0_h96,         has_simdgroup_mm);`",
    "body": "### Name and Version\n\nB5727 \n\n### Operating systems\n\nMac\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nM4 Mac Studio \n\n### Models\n\nQwen 2.5 1.7b\n\n### Problem description & steps to reproduce\n\nload the model and crash at\n```\nGGML_METAL_ADD_KERNEL(GGML_METAL_KERNEL_TYPE_FLASH_ATTN_EXT_Q8_0_H96,         flash_attn_ext_q8_0_h96,         has_simdgroup_mm);\n```\n\nThread 7: EXC_BAD_ACCESS (code=1, address=0x4e29444af118)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama_model_load_from_file_impl: using device Metal (Apple M4 Max) - 49151 MiB free\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /Users/animax/Library/Developer/Xcode/DerivedData/LocalLLM-fpkqjzkzghleumgxashmyivglcsj/Build/Products/Debug/model.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = qwen2.5-1.5b-instruct\nllama_model_loader: - kv   3:                            general.version str              = v0.1\nllama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-1.5b-instruct\nllama_model_loader: - kv   5:                         general.size_label str              = 1.8B\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                          general.file_type u32              = 7\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\n\n...\n\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M4 Max\nggml_metal_init: picking default device: Apple M4 Max\nggml_metal_load_library: using embedded metal library\nEmbedding model loaded successfully\nWarning: Compilation succeeded with: \n\nprogram_source:485:28: warning: unused variable 'ksigns64' [-Wunused-const-variable]\nGGML_TABLE_BEGIN(uint64_t, ksigns64, 128)\n                           ^\nprogram_source:1080:26: warning: unused variable 'kvalues_iq4nl' [-Wunused-const-variable]\nGGML_TABLE_BEGIN(int8_t, kvalues_iq4nl, 16)\n                         ^\nWarning: Compilation succeeded with: \n\nprogram_source:485:28: warning: unused variable 'ksigns64' [-Wunused-const-variable]\nGGML_TABLE_BEGIN(uint64_t, ksigns64, 128)\n                           ^\nprogram_source:1080:26: warning: unused variable 'kvalues_iq4nl' [-Wunused-const-variable]\nGGML_TABLE_BEGIN(int8_t, kvalues_iq4nl, 16)\n                         ^\nggml_metal_init: GPU name:   Apple M4 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = true\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\nggml_metal_init: GPU name:   Apple M4 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = true\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n\n...\n\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3            0x600000a84900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4            0x600000a84a20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4            0x600000ab3d20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5            0x600000ab1980 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5            0x600000ab63a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2            0x600000a8e3a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2            0x600000a84b40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3            0x600000ab7ba0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3            0x600000a8f360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4            0x600000a984e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4            0x600000a802a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5            0x600000a8af40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5            0x600000a98600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2            0x600000a81260 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2            0x600000ab3f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3            0x600000ab3ea0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3            0x600000a8b7e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4            0x600000ab3d80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4            0x600000a99800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5            0x600000a45c80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5            0x600000a81bc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2            0x600000ac24c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2            0x600000a84960 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3            0x600000a85aa0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3            0x600000a910e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4            0x600000a0dc20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4            0x600000af3840 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5            0x600000acc180 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5            0x600000afd140 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2            0x600000af07e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2            0x600000ac9440 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3            0x600000afe3a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3            0x600000acc240 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4            0x600000a581e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4            0x600000acca20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5            0x600000a58240 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5            0x600000af3c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2          0x600000af1380 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2          0x600000a8fa80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3          0x600000af2b80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3          0x600000acb0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4          0x600000acb180 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4          0x600000af1bc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5          0x600000afcd20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5          0x600000a8f660 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q2_K_f32                     0x600000b69140 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q2_K_f32                     0x600000b6c360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q3_K_f32                     0x600000b6c3c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q3_K_f32                     0x600000b60300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q4_K_f32                     0x600000b6c420 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q4_K_f32                     0x600000acd020 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q5_K_f32                     0x600000afd500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q5_K_f32                     0x600000b6ca80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q6_K_f32                     0x600000b6ccc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q6_K_f32                     0x600000afd800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                  0x600000b698c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                  0x600000af3d80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                   0x600000b60420 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                   0x600000b6da40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                  0x600000acd1a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                  0x600000af0180 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                    0x600000b640c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                    0x600000b781e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                    0x600000b64780 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                    0x600000ace880 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                    0x600000b78300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                    0x600000ace820 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                    0x600000b789c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                    0x600000b6db00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                   0x600000b62700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                   0x600000b78ae0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                   0x600000b62f40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                   0x600000b648a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f32_f32                   0x600000b628e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f32_f32                   0x600000ace9a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f16_f32                   0x600000b791a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f16_f32                   0x600000acf180 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                  0x600000b6db60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                  0x600000b65020 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                  0x600000b63000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                  0x600000b79a40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                  0x600000acaf40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                  0x600000b6b6c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                  0x600000b6e2e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                  0x600000b7c0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                  0x600000b6e3a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                  0x600000b63840 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                  0x600000b7c7e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                  0x600000b65e60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                  0x600000aca280 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                  0x600000b6bde0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                  0x600000b7a9a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                  0x600000b66700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                  0x600000af2ac0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                  0x600000b6be40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                  0x600000b701e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                  0x600000b66d60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                  0x600000b6bf60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                  0x600000b70300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32               0x600000b7cf60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32               0x600000b74000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                0x600000b480c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                0x600000b6bea0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32               0x600000b74f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32               0x600000b70ae0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                 0x600000b7d860 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                 0x600000b66fa0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                 0x600000b69f80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                 0x600000ace7c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                 0x600000ace6a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                 0x600000b729a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                 0x600000b67ea0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                 0x600000b74fc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                0x600000b40600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                0x600000b75020 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                0x600000b488a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                0x600000b4c0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f32_f32                      0x600000b40660 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f32_f32                      0x600000b755c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f16_f32                      0x600000b49080 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f16_f32                      0x600000b4ce40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_bf16_f32                     0x600000b7fb40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_bf16_f32                     0x600000b75680 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_0_f32                     0x600000b73660 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_0_f32                     0x600000b406c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_1_f32                     0x600000b73ba0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_1_f32                     0x600000b49680 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_0_f32                     0x600000b4a100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_0_f32                     0x600000b4c900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_1_f32                     0x600000b41200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_1_f32                     0x600000b4c8a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q8_0_f32                     0x600000b412c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q8_0_f32                     0x600000b4d4a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q2_K_f32                     0x600000b41920 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q2_K_f32                     0x600000b4a1c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q3_K_f32                     0x600000b75d40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q3_K_f32                     0x600000b583c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_K_f32                     0x600000b76280 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_K_f32                     0x600000b4a220 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_K_f32                     0x600000b4e0a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_K_f32                     0x600000b41e60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q6_K_f32                     0x600000b41f20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q6_K_f32                     0x600000b58e40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                  0x600000b42580 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                  0x600000b45da0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                   0x600000b76a60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                   0x600000b58f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                  0x600000b42640 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                  0x600000b77060 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                    0x600000b4a2e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                    0x600000b4e6a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                    0x600000b4a880 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                    0x600000b46940 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                    0x600000b4ae20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                    0x600000b46f40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                    0x600000b430c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                    0x600000b72760 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                   0x600000b436c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                   0x600000b595c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                   0x600000b4b420 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                   0x600000b47480 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map0_f16                  0x600000b59bc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map0_f16                  0x600000b478a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map1_f32                  0x600000b5a280 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map1_f32                  0x600000b43840 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f32_f16                   0x600000b72100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f32_f16                   0x600000b4ebe0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f16_f16                   0x600000a3d860 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f16_f16                   0x600000b5c0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_bf16_f16                  0x600000b5cc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_bf16_f16                  0x600000b5a7c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                  0x600000b50360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                  0x600000b47600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                  0x600000b50960 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                  0x600000b4ba20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                  0x600000b5ccc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                  0x600000b5a880 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                  0x600000b4f3c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                  0x600000b540c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                  0x600000b4f9c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                  0x600000b51ec0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                  0x600000b283c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                  0x600000b5d200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                  0x600000b5ad60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                  0x600000b51e60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                  0x600000b5aee0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                  0x600000b54780 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                  0x600000b5af40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                  0x600000b28a80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                  0x600000b5b060 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                  0x600000b54d80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16               0x600000b52460 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16               0x600000b5d8c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                0x600000b529a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                0x600000b55380 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16               0x600000b28fc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16               0x600000b5e340 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                 0x600000b29020 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                 0x600000b53600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                 0x600000b29080 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                 0x600000b2c0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                 0x600000b5eee0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                 0x600000b55980 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                 0x600000b5f060 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                 0x600000b290e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                0x600000b51e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                0x600000b2d1a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                0x600000b29680 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                0x600000b20120 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f32                       0x600000b29560 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f32                       0x600000b47a80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f16                       0x600000b29e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f16                       0x600000b240c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f32                      0x600000b2b480 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f32                      0x600000b51ce0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f16                      0x600000b380c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f16                      0x600000b55ec0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f32                     0x600000b38c60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f32                     0x600000b2d380 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f16                     0x600000b2d980 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f16                     0x600000b39800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f32                       0x600000b3a3a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f32                       0x600000b2ef40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f16                       0x600000b2f0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f16                       0x600000b3af40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f16                          0x600000b2ff00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f16                          0x600000b223a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f32                          0x600000b53f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f32                          0x600000b2b540 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f16                      0x600000b2bae0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f16                      0x600000b3c480 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f32                      0x600000b3c5a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f32                      0x600000b55f20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x600000b562e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x600000b3cc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x600000b3cf00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x600000b22ee0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_upscale_f32                         0x600000b306c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_upscale_f32                         0x600000b22fa0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_f32                             0x600000b3d140 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_f32                             0x600000b30e40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x600000b57420 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x600000b23720 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_timestep_embedding_f32              0x600000b2fde0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_timestep_embedding_f32              0x600000b2bb40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_arange_f32                          0x600000b34000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_arange_f32                          0x600000b2bf60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x600000b3d860 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x600000b57d20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x600000b3d6e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x600000b34060 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_leaky_relu_f32                      0x600000b34180 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_leaky_relu_f32                      0x600000b57ea0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h64              0x600000b08000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h64              0x600000b342a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h80              0x600000b24ba0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h80              0x600000b31500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h96              0x600000b0c000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h96              0x600000b3d560 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h112             0x600000b25ec0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h112             0x600000b08120 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h128             0x600000b08ae0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h128             0x600000b25da0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h192             0x600000b323a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h192             0x600000b3de60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128      0x600000b0dc20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128      0x600000b093e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h256             0x600000b32ca0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h256             0x600000b3df80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512      0x600000ab2f40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512      0x600000b0dce0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64             0x600000b0edc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64             0x600000ab2700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80             0x600000b32d60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80             0x600000b361c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96             0x600000b35da0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96             0x600000b09e60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112            0x600000b35d40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112            0x600000ac1c20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128            0x600000a85b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128            0x600000a9c900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h192            0x600000a92220 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h192            0x600000a823a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_hk192_hv128     0x600000a82520 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_hk192_hv128     0x600000a85c20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256            0x600000a9d2c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256            0x600000a83e40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_hk576_hv512     0x600000a9dc80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_bf16_hk576_hv512     0x600000a94540 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64             0x600000a9e580 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64             0x600000a98f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80             0x600000b0e580 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80             0x600000a86ee0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96             0x600000a86640 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96             0x600000a9b9c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112            0x600000a9e6a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112            0x600000a96160 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128            0x600000a96280 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128            0x600000b05260 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192            0x600000b05ce0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192            0x600000a9f060 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128     0x600000b1c300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128     0x600000a83d80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256            0x600000a83d20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256            0x600000b06700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512     0x600000a83cc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512     0x600000b070c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64             0x600000b12640 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64             0x600000a83360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80             0x600000a87ba0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80             0x600000a83540 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96             0x600000b1c360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96             0x600000b181e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112            0x600000b1cc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112            0x600000b18b40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128            0x600000b1cd80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128            0x600000a87b40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192            0x600000b15260 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192            0x600000b18a20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128     0x600000b1d800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128     0x600000b19c80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256            0x600000b166a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256            0x600000b124c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512     0x600000b1e1c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512     0x600000a87a80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64             0x600000b11c20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64             0x600000be8240 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80             0x600000b1f480 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80             0x600000b11bc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96             0x600000b16820 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96             0x600000bec0c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112            0x600000bec9c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112            0x600000be9320 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128            0x600000beca20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128            0x600000b1f540 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192            0x600000bed440 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192            0x600000b1fe40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128     0x600000becba0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128     0x600000be44e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256            0x600000bed560 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256            0x600000b17900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512     0x600000be91a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512     0x600000b1a880 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64             0x600000be8fc0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64             0x600000bee760 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80             0x600000be55c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80             0x600000be00c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96             0x600000bef1e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96             0x600000b1aa00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112            0x600000bee940 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112            0x600000be09c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128            0x600000beb360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128            0x600000be0a20 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192            0x600000bfc9c0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192            0x600000bf8360 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128     0x600000bfcae0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128     0x600000bf07e0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256            0x600000bfcb40 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256            0x600000bf8de0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512     0x600000bf1980 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512     0x600000be54a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64             0x600000be0a80 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64             0x600000bfcc60 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80             0x600000bfa0a0 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80             0x600000be0ae0 | th_max = 1024 | th_width =   32\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-10T21:08:26+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14110/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14110"
  },
  {
    "number": 11717,
    "title": "Feature Request: (webui) read data from /props endpoint and use it on the webui",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nNot sure yet how we will use it, just noting this idea here so I don't forget\n\n### Motivation\n\nN/A\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "server/webui",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-02-06T16:27:15+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11717/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11717"
  },
  {
    "number": 14057,
    "title": "[How to serve lookahead decoding Qwen 3]",
    "body": "I know how to deploy and call an API using an LLM with speculative decoding and a draft model via llama-serve. \n```\n./build/bin/llama-server --model Qwen3-14B-Q8_0.gguf --reasoning-budget 0 --model-draft Qwen3-0.6B-Q8_0.gguf --n-gpu-layers 99 -ngld 99 -fa --draft-max 16 --draft-min 0 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 \n```\nBut how can I serve a model using lookahead decoding instead?\nThe command \n```\n./build/bin/llama-lookahead --model Qwen3-14B-Q8_0.gguf --n-gpu-layers 99\n```\ndoesn't work because it requires an input prompt. \n\nReference: https://github.com/ggml-org/llama.cpp/pull/4207\n\nThanks in advance. \n",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-07T10:56:00+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14057/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14057"
  },
  {
    "number": 14058,
    "title": "Cmake minor bug: Confusing ggml-cpu: -march=native log message when using explicit -march flags and LLAMA_NATIVE=OFF",
    "body": "### Git commit\n\nHi to all,\n\nI'm encountering a confusing message in the CMake configuration log when trying to compile llama.cpp with a specific target CPU architecture. This is particularly problematic for cross-compilation or when ensuring specific optimizations for deployment environments.\n\nMy Setup:\n- Build Environment CPU: AMD Ryzen 7 3700U (Zen 2 architecture).\n- Target Deployment CPU: AMD Ryzen 9 7945HX (Zen 4 architecture, znver4).\n- Goal: Compile llama.cpp binary (and its statically linked ggml components) to specifically target znver4 for optimal performance on the production node.\n\nCMake Command Used:\n```bash\ncmake .. \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DLLAMA_NATIVE=OFF \\\n    -DCMAKE_C_FLAGS=\"-march=znver4\" \\\n    -DCMAKE_CXX_FLAGS=\"-march=znver4\" \\\n    -DBUILD_SHARED_LIBS=OFF\n```\nObserved CMake Output (relevant section):\n```\n-- The C compiler identification is GNU 14.3.0\n-- The CXX compiler identification is GNU 14.3.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.49.0\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- Found CURL: /usr/lib64/libcurl.so (found version \"8.13.0\")\n-- Configuring done (1.3s)\n-- Generating done (0.2s)\n-- Build files have been written to: /home/me/Desktop/aia/Docker/app/app\n```\n\nThe Problem:\nThe line `Adding CPU backend variant ggml-cpu: -march=native` is highly confusing and misleading. Despite explicitly setting `-DLLAMA_NATIVE=OFF` and providing `-march=znver4` via `CMAKE_C_FLAGS` and `CMAKE_CXX_FLAGS`, this log message suggests that ggml-cpu is being compiled for native (i.e., the Zen 2 build machine's architecture), which contradicts my explicit intent.\n\nProof that -march=znver4 was indeed applied (and why the log is confusing):\nTo verify, I attempted to run the compiled llama-server binary on the same Zen 2 build machine where it was compiled:\n\n```Bash\nme@host ~/Desktop/aia/Docker/app/app $ bin/llama-server\nIllegal instruction (core dumped)\n```\n\nThe Illegal instruction (core dumped) error confirms that the binary was indeed compiled with znver4-specific instructions (e.g., AVX-512) which are not supported by the Zen 2 CPU. This clearly demonstrates that my -march=znver4 flags were **correctly applied, and the binary is not for native (Zen 2)**.\n\nExpected Behavior:\nThe CMake configuration log should accurately reflect the chosen target architecture. Ideally, the message would be something like:\n```\n-- Adding CPU backend variant ggml-cpu: -march=znver4\n```\nOr at least a clear indication that the user-provided -march flags are taking precedence over any automatic native detection.\n\nThis ambiguity creates unnecessary debugging effort and requires manual verification steps (like running the binary on the wrong architecture to confirm failure) to ensure the intended compilation flags were honored.\n\nThank you :)\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nCMake Command Used:\n```bash\ncmake .. \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DLLAMA_NATIVE=OFF \\\n    -DCMAKE_C_FLAGS=\"-march=znver4\" \\\n    -DCMAKE_CXX_FLAGS=\"-march=znver4\" \\\n    -DBUILD_SHARED_LIBS=OFF\n```\n\nLog of cmake:\n```\n[...]\n-- Adding CPU backend variant ggml-cpu: -march=native \n[...]\n```\n\nExpected:\n```\n[...]\n-- Adding CPU backend variant ggml-cpu: -march=znver4\n[...]\n```\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake .. \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DLLAMA_NATIVE=OFF \\\n    -DCMAKE_C_FLAGS=\"-march=znver4\" \\\n    -DCMAKE_CXX_FLAGS=\"-march=znver4\" \\\n    -DBUILD_SHARED_LIBS=OFF\n```\n\n### Relevant log output\n\n```shell\n-- The C compiler identification is GNU 14.3.0\n-- The CXX compiler identification is GNU 14.3.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.49.0\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- Found CURL: /usr/lib64/libcurl.so (found version \"8.13.0\")\n-- Configuring done (1.3s)\n-- Generating done (0.2s)\n-- Build files have been written to: /home/me/Desktop/aia/Docker/app/app\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-07T15:47:51+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14058"
  },
  {
    "number": 14019,
    "title": "Misc. bug: \"error: invalid argument: /bin/sh\" when using Docker image",
    "body": "### Name and Version\n\nImage: `ghcr.io/ggml-org/llama.cpp:server-cuda-b5583`\nPlatform: Google Cloud Run\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nI am using a small Dockerfile based on the official image (to include a model in the image, but for the purpose of this issue I removed that part since the bug doesn't depend on it):\n\n```dockerfile\nFROM ghcr.io/ggml-org/llama.cpp:server-cuda-b5583\n\nCMD [ \\\n  \"--gpu-layers\", \"999\", \\\n  \"--host\", \"0.0.0.0\", \\\n  \"-hf\", \"ggml-org/gemma-3-1b-it-GGUF\" \\\n  \"--port\", \"8080\", \\\n  \"--verbose\", \\\n]\n```\n\nI build it on Google Cloud Build using this config:\n\n```yaml\nsteps:\n  - name: \"gcr.io/cloud-builders/docker\"\n    args: [\"build\", \"--tag\", \"${_IMAGE}\", \".\"]\n\nimages: [\"${_IMAGE}\"]\n\nsubstitutions:\n  _IMAGE: \"us-central1-docker.pkg.dev/${PROJECT_ID}/repo/test:1\"\n\noptions:\n  dynamicSubstitutions: true\n  machineType: \"E2_HIGHCPU_32\"\n```\n\nAnd then deploy it using this command:\n\n```bash\ngcloud run deploy $SERVICE_NAME \\\n    --allow-unauthenticated \\\n    --cpu 4 \\\n    --gpu 1 \\\n    --gpu-type nvidia-l4 \\\n    --image us-central1-docker.pkg.dev/${PROJECT_ID}/repo/test:1 \\\n    --max-instances 3 \\\n    --memory 16Gi \\\n    --no-cpu-throttling \\\n    --no-gpu-zonal-redundancy \\\n    --port 8080 \\\n    --region us-central1 \\\n    --service-account $SERVICE_ACCOUNT \\\n    --startup-probe=timeoutSeconds=10,httpGet.path=/health,httpGet.port=8080 \\\n    --liveness-probe=timeoutSeconds=10,httpGet.path=/health,httpGet.port=8080 \\\n    --timeout=60\n```\n\nWhile deploying I get this error (the server never starts):\n\n```\n2025-06-04 10:41:58.588\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n2025-06-04 10:41:58.588\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n2025-06-04 10:41:58.588\nggml_cuda_init: found 1 CUDA devices:\n2025-06-04 10:41:58.588\n  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n2025-06-04 10:41:58.588\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\n2025-06-04 10:41:58.731\nload_backend: loaded CPU backend from /app/libggml-cpu-skylakex.so\n2025-06-04 10:41:58.732\nerror: invalid argument: /bin/sh\n```\n\nI checked and `/bin/sh` exists in the image, both during build and when running (I changed the entrypoint to `ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo 123\"]` and my image correctly printed out \"123\" when deployed).\n\nSince the error happens in the logs clearly when `llama-server` is already running I did a search for any invocations in the code base and found this: https://github.com/ggml-org/llama.cpp/blob/3e63a58ef7addec35408e2eb67850d7cdc935dc3/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp#L140\n\nI'm not very familiar with the code but I saw a conditional checking for PID 0 so I changed my entrypoint one more time to:\n\n```dockerfile\nENTRYPOINT [\"/bin/sh\", \"-c\", \"/app/llama-server \\\n  --gpu-layers 999 \\\n  --host 0.0.0.0 \\\n  -hf ggml-org/gemma-3-1b-it-GGUF \\\n  --port 8080 \\\n  --verbose \\\n\"]\n```\n\nThis did the trick and the server runs and responds to requests as expected. I'm not sure what exactly is going on but I thought I'd report it in case others encounter this issue or there is a simple fix.\n\nI suspect this might be specific to Google Cloud Run since I couldn't find anyone reporting such an error and the code `vulkan-shaders-gen.cpp` hasn't changed in months.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-05T00:22:13+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14019/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14019"
  },
  {
    "number": 14097,
    "title": "Eval bug: Gemma3 decode and update_slots fail with parallel slots",
    "body": "### Name and Version\n\nPS C:\\Sources\\llama.cpp\\build\\bin\\Release> .\\llama-server.exe --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9, VMM: yes\nversion: 5636 (c9c75ff8)\nbuilt with MSVC 19.43.34808.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRyzen 7 7800X3D + RTX 4070 Ti Super\n\n### Models\n\n[Gemma-3-27b-it](https://huggingface.co/unsloth/gemma-3-27b-it-GGUF)\n\n### Problem description & steps to reproduce\n\nWhen I run llama-server with np > 1 and Gemma3 model, it spams with `decode: failed to find a memory slot for batch of size` and `srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 256, ret = 1` when multiple slots are utilized. This does not happen with a single slot or when only one out of multiple slots is used.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nPS C:\\Sources\\llama.cpp\\build\\bin\\Release> .\\llama-server.exe -c 32768 -n -1 -m C:\\Temp\\gemma-3-27b-it-IQ3_M.gguf -ngl 99 -fa -ctk q8_0 -ctv q8_0 -a gemma3 --port 8083 -np 2\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9, VMM: yes\nbuild: 5636 (c9c75ff8) with MSVC 19.43.34808.0 for x64\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 16 | CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8083, http threads: 15\nmain: loading model\nsrv    load_model: loading model 'C:\\Temp\\gemma-3-27b-it-IQ3_M.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4070 Ti SUPER) - 15089 MiB free\nllama_model_loader: loaded meta data with 44 key-value pairs and 808 tensors from C:\\Temp\\gemma-3-27b-it-IQ3_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 27b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 27B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 27b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 5376\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 62\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 21504\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 32\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 128\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 16\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  38:               general.quantization_version u32              = 2\nllama_model_loader: - kv  39:                          general.file_type u32              = 27\nllama_model_loader: - kv  40:                      quantize.imatrix.file str              = /models_out/gemma-3-27b-it-GGUF/googl...\nllama_model_loader: - kv  41:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  42:             quantize.imatrix.entries_count i32              = 434\nllama_model_loader: - kv  43:              quantize.imatrix.chunks_count i32              = 129\nllama_model_loader: - type  f32:  373 tensors\nllama_model_loader: - type q4_K:  131 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllama_model_loader: - type iq3_s:  303 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = IQ3_S mix - 3.66 bpw\nprint_info: file size   = 11.68 GiB (3.71 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 5376\nprint_info: n_layer          = 62\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 16\nprint_info: n_rot            = 128\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 2048\nprint_info: n_embd_v_gqa     = 2048\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 7.7e-02\nprint_info: n_ff             = 21504\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 27B\nprint_info: model params     = 27.01 B\nprint_info: general.name     = Gemma 3 27b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 62 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 63/63 layers to GPU\nload_tensors:        CUDA0 model buffer size = 11959.34 MiB\nload_tensors:   CPU_Mapped model buffer size =  1102.50 MiB\n.....................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 32768\nllama_context: n_ctx_per_seq = 16384\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context: requested n_seq_max (2) > 1, but swa_full is not enabled -- performance may be degraded: https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573\nllama_context:  CUDA_Host  output buffer size =     2.00 MiB\nllama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 32768 cells\nllama_kv_cache_unified:      CUDA0 KV buffer size =  1360.00 MiB\nllama_kv_cache_unified: size = 1360.00 MiB ( 32768 cells,  10 layers,  2 seqs), K (q8_0):  680.00 MiB, V (q8_0):  680.00 MiB\nllama_kv_cache_unified_iswa: creating     SWA KV cache, size = 2560 cells\nllama_kv_cache_unified:      CUDA0 KV buffer size =   552.50 MiB\nllama_kv_cache_unified: size =  552.50 MiB (  2560 cells,  52 layers,  2 seqs), K (q8_0):  276.25 MiB, V (q8_0):  276.25 MiB\nllama_context:      CUDA0 compute buffer size =   522.50 MiB\nllama_context:  CUDA_Host compute buffer size =    79.51 MiB\nllama_context: graph nodes  = 2551\nllama_context: graph splits = 2\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 32768\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 2\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 16384\nslot         init: id  1 | task -1 | new slot n_ctx_slot = 16384\nmain: model loaded\nmain: chat template, chat_template: {{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n, example_format: '<start_of_turn>user\nYou are a helpful assistant\n\nHello<end_of_turn>\n<start_of_turn>model\nHi there<end_of_turn>\n<start_of_turn>user\nHow are you?<end_of_turn>\n<start_of_turn>model\n'\nmain: server is listening on http://127.0.0.1:8083 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: GET /v1/models 127.0.0.1 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  1 | task 0 | processing task\nslot update_slots: id  1 | task 0 | new prompt, n_ctx_slot = 16384, n_keep = 0, n_prompt_tokens = 21\nslot update_slots: id  1 | task 0 | kv cache rm [0, end)\nslot update_slots: id  1 | task 0 | prompt processing progress, n_past = 21, n_tokens = 21, progress = 1.000000\nslot update_slots: id  1 | task 0 | prompt done, n_past = 21, n_tokens = 21\nslot      release: id  1 | task 0 | stop processing: n_past = 24, truncated = 0\nslot print_timing: id  1 | task 0 |\nprompt eval time =     231.26 ms /    21 tokens (   11.01 ms per token,    90.81 tokens per second)\n       eval time =      78.92 ms /     4 tokens (   19.73 ms per token,    50.68 tokens per second)\n      total time =     310.19 ms /    25 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 5 | processing task\nslot update_slots: id  0 | task 5 | new prompt, n_ctx_slot = 16384, n_keep = 0, n_prompt_tokens = 2977\nslot update_slots: id  0 | task 5 | kv cache rm [0, end)\nslot update_slots: id  0 | task 5 | prompt processing progress, n_past = 2048, n_tokens = 2048, progress = 0.687941\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  1 | task 7 | processing task\nslot update_slots: id  0 | task 5 | kv cache rm [2048, end)\nslot update_slots: id  0 | task 5 | prompt processing progress, n_past = 2977, n_tokens = 929, progress = 1.000000\nslot update_slots: id  0 | task 5 | prompt done, n_past = 2977, n_tokens = 929\nslot update_slots: id  1 | task 7 | new prompt, n_ctx_slot = 16384, n_keep = 0, n_prompt_tokens = 2319\nslot update_slots: id  1 | task 7 | n_past = 4, cache_tokens.size() = 24, seq_id = 1, pos_min = 0, n_swa = 1024\nslot update_slots: id  1 | task 7 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nslot update_slots: id  1 | task 7 | kv cache rm [0, end)\nslot update_slots: id  1 | task 7 | prompt processing progress, n_past = 1119, n_tokens = 2048, progress = 0.482536\ndecode: failed to find a memory slot for batch of size 2048\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 1024\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 1024, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 1024\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 1024, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 1536, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 1536, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 1536, n_batch = 256, ret = 1\nslot update_slots: id  1 | task 7 | kv cache rm [1119, end)\nslot update_slots: id  1 | task 7 | prompt processing progress, n_past = 2319, n_tokens = 1201, progress = 1.000000\nslot update_slots: id  1 | task 7 | prompt done, n_past = 2319, n_tokens = 1201\ndecode: failed to find a memory slot for batch of size 1201\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 1024\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 1073\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 128, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 1024\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 128, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 128, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 128, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 945\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 256, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 945\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 256, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 256, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 256, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 817\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 384, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 817\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 384, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 384, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 384, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 689\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 512, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 689\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 512, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 512, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 512, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 561\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 640, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 561\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 640, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 512\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 640, n_batch = 256, ret = 1\ndecode: failed to find a memory slot for batch of size 256\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 640, n_batch = 128, ret = 1\ndecode: failed to find a memory slot for batch of size 433\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 768, n_batch = 1024, ret = 1\ndecode: failed to find a memory slot for batch of size 433\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 768, n_batch = 512, ret = 1\ndecode: failed to find a memory slot for batch of size 433\nsrv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 768, n_batch = 256, ret = 1\nsrv  params_from_: Chat format: Content-only\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-10T09:12:28+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14097/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14097"
  },
  {
    "number": 14150,
    "title": "Metrics should not include : in Prometheus metric names",
    "body": "Hi,\n\nI noticed that all Prometheus metrics exposed by the library are currently prefixed with `llamacpp:` (e.g., `llamacpp:prompt_tokens_total`). \nHowever, according to the [Prometheus metric naming guidelines](https://prometheus.io/docs/practices/naming/), I think the right (i.e. idiomatic) patter would be:\n`llamacpp_prompt_tokens_total`\n\nWould you consider updating the metric naming to follow this convention? I'd be happy to help contribute a PR if it's welcome.\n\nThanks!",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-12T13:18:44+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14150/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14150"
  },
  {
    "number": 14211,
    "title": "Eval bug: RWKV inference with llama-parallel gets wrong output with lmhead offloaded to GPU",
    "body": "### Name and Version\n\n$ ./build/bin/llama-cli --version                                                                       \nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\nversion: 5674 (d7da8dc8)\nbuilt with cc (GCC) 15.1.1 20250425 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRyzen 7900x + RTX4090\n\n### Models\n\nhttps://huggingface.co/zhiyuan8/RWKV-v7-0.1B-G1-GGUF/blob/main/rwkv7-0.1B-g1-F16.gguf\n\n### Problem description & steps to reproduce\n\n(Update: With Metal and Vulkan backends, offloading all layers with llama-parallel works flawlessly\nIt seems that this problem is CUDA-specific)\n\nUsing rwkv7-0.1B-g1-F16.gguf with 12 repeating layers and 1 output layer, it outputs correctly when running with `-ngl 12` (aka not offloading the output layer) :\n```\n./build/bin/llama-parallel -m ./rwkv7-0.1B-g1-F16.gguf -ns 8 -np 5 --top-k 1 -ngl 12\nmain: Simulating parallel requests from clients:\nmain: n_parallel = 5, n_sequences = 8, cont_batching = 1, system tokens = 266\n\nProcessing requests ...\n\nmain: clearing the KV cache\nClient   0, seq    0, junk =    0, started decoding ...\nClient   1, seq    1, junk =    0, started decoding ...\nClient   2, seq    2, junk =    0, started decoding ...\nClient   3, seq    3, junk =    0, started decoding ...\nClient   4, seq    4, junk =    0, started decoding ...\nClient   2, seq   2/  8, prompt  281 t, response   64 t, time  0.36 s, speed 957.14 t/s, cache miss 0  \n\nInput:    Recommend some interesting books to read.\nResponse: Some interesting books to read include \"The Secret Life of Bees\" by David Attenborough, \"The Secret Life of the Bees\" by David Attenborough, and \"The Secret Life of Bees\" by David Attenborough. These books are all about the fascinating world of bees and their behavior.<s>\n\nClient   2, seq    5, junk =    0, started decoding ...\nClient   4, seq   4/  8, prompt  285 t, response   71 t, time  0.40 s, speed 880.97 t/s, cache miss 0  \n\nInput:    What is the best way to learn a new language?\nResponse: The best way to learn a new language is to immerse yourself in the language and culture. This can be done through language learning apps, language classes, or by reading books and watching movies in the language. You can also practice speaking the language in a real-life situation and try to understand the culture and customs of the people you meet.\n\nClient   4, seq    6, junk =    0, started decoding ...\nClient   0, seq   0/  8, prompt  281 t, response  103 t, time  0.57 s, speed 679.36 t/s, cache miss 0  \n\nInput:    What is the meaning of life?\nResponse: The meaning of life is to live a meaningful and fulfilling life. It is a journey of self-discovery, personal growth, and the pursuit of happiness. It is a journey that involves learning from our experiences, making choices, and finding meaning in our lives. It is a journey that is not always easy, but it is worth it. The meaning of life is not about what we do, but about what we do for the sake of others. It is about living a life that is meaningful and fulfilling.<s>\n\nClient   1, seq   1/  8, prompt  281 t, response  103 t, time  0.57 s, speed 679.32 t/s, cache miss 0  \n\nInput:    What is the meaning of life?\nResponse: The meaning of life is to live a meaningful and fulfilling life. It is a journey of self-discovery, personal growth, and the pursuit of happiness. It is a journey that involves learning from our experiences, making choices, and finding meaning in our lives. It is a journey that is not always easy, but it is worth it. The meaning of life is not about what we do, but about what we do for the sake of others. It is about living a life that is meaningful and fulfilling.<s>\n\nClient   0, seq    7, junk =    0, started decoding ...\nClient   3, seq   3/  8, prompt  287 t, response  128 t, time  0.69 s, speed 602.23 t/s, cache miss 0  \n\nInput:    If you could have any superpower, what would it be?\nResponse: I don't have personal preferences or opinions, but I can provide information on some of the most popular superpowers. Some of the most popular superpowers include:\n1. Telepathy: Telepathy is the ability to communicate with others without using words or gestures. It is a powerful and mysterious ability that can be used for a variety of purposes, such as communication, information sharing, and even in the case of telekinesis.\n2. Telekinesis: Telekinesis is the ability to move objects with the use of a force. It is a powerful and mysterious ability that can be used for a variety of purposes, such as\n\nClient   4, seq   6/  8, prompt  283 t, response   64 t, time  0.32 s, speed 1080.18 t/s, cache miss 0  \n\nInput:    Tell me an interesting fact about llamas.\nResponse: Llamas are a type of camelid that is native to South America. They are known for their long, soft, and silky hair that is used for various purposes, including for clothing and bedding. They are also known for their ability to run fast and their ability to communicate with other llamas.\n\nClient   2, seq   5/  8, prompt  282 t, response   83 t, time  0.42 s, speed 876.79 t/s, cache miss 0  \n\nInput:    How to get a job at Google?\nResponse: To get a job at Google, you need to have a strong background in computer science, programming, and web development. You can also get a job in Google's Google Apps for Business platform, which includes Gmail, Google Drive, and Google Docs. You can also get a job in Google's Google Search Console, which is a tool that helps you find and fix issues in Google's search engine.\n\nClient   0, seq   7/  8, prompt  281 t, response  103 t, time  0.40 s, speed 966.95 t/s, cache miss 0  \n\nInput:    What is the meaning of life?\nResponse: The meaning of life is to live a meaningful and fulfilling life. It is a journey of self-discovery, personal growth, and the pursuit of happiness. It is a journey that involves learning from our experiences, making choices, and finding meaning in our lives. It is a journey that is not always easy, but it is worth it. The meaning of life is not about what we do, but about what we do for the sake of others. It is about living a life that is meaningful and fulfilling.<s>\n...\n```\nBut with `-ngl 13`, the outputs are getting weird:\n```\nmain: clearing the KV cache\nClient   0, seq    0, junk =    0, started decoding ...\nClient   1, seq    1, junk =    0, started decoding ...\nClient   2, seq    2, junk =    0, started decoding ...\nClient   3, seq    3, junk =    0, started decoding ...\nClient   4, seq    4, junk =    0, started decoding ...\nClient   2, seq   2/  8, prompt  281 t, response   64 t, time  0.25 s, speed 1368.61 t/s, cache miss 0  \n\nInput:    Recommend some interesting books to read.\nResponse: Some interesting books to read include \"The Secret Life of Bees\" by David Attenborough, \"The Secret Life of the Bees\" by David Attenborough, and \"The Secret Life of Bees\" by David Attenborough. These books are all about the fascinating world of bees and their behavior.<s>\n\nClient   2, seq    5, junk =    0, started decoding ...\nClient   0, seq   0/  8, prompt  281 t, response  128 t, time  0.48 s, speed 860.89 t/s, cache miss 0  \n\nInput:    What is the meaning of life?\nResponse: The recommend toly a to. language best\n learning. learnlearn learn\n\n...UserUser\n\n\ntoto\nUserUser\n.:..:::::::\n\n\n\n\n::\n:::to:::::\n\n\n\n\n\n::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\nClient   1, seq   1/  8, prompt  281 t, response  128 t, time  0.48 s, speed 860.86 t/s, cache miss 0  \n\nInput:    What is the meaning of life?\nResponse: TheUser ofelf most a to. meaningmeaning the the\n\n:..TheTheTheTheTheQuestion meaning of of of of the the of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of the the the of of of of of of of of of of of of of of of of of of of of of of of of of of\n\n...\n```\n\nI've tried debugging for a while but with no luck :(\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nMentioned above\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-16T10:24:15+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14211/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14211"
  },
  {
    "number": 13910,
    "title": "android built on GPU cannot comparable with CPU?",
    "body": "I tried to build on Android device with GPU env but fail at official documents.\n1.Termux env\n2.openCL \n\nI blocked here:\n\n![Image](https://github.com/user-attachments/assets/918e9903-f500-41ba-ae96-33ee73818009)\n\n![Image](https://github.com/user-attachments/assets/8673bcf2-8ac2-4a37-a02e-d684d6759cfe)\n\nSo, I changed to another build method as below:\n1.\nusing termux default cmake tool does not ninja\n2.\ncmake .. -DCMAKE_BUILD_TYPE=Release \\\n  -DOPENCL_ICD_LOADER_HEADERS_DIR=/data/data/com.termux/files/usr/include \\\n  -DCMAKE_C_COMPILER=/data/data/com.termux/files/usr/bin/clang \\\n  -DCMAKE_CXX_COMPILER=/data/data/com.termux/files/usr/bin/clang++ \\\n  -DCMAKE_C_FLAGS=\"--target=aarch64-linux-android24 -D_POSIX_C_SOURCE=200809L\" \\\n  -DCMAKE_CXX_FLAGS=\"--target=aarch64-linux-android24 -D_POSIX_C_SOURCE=200809L\"\n3.\ncmake .. -DBUILD_SHARED_LIBS=ON -DGGML_OPENCL=ON -DGGML_OPENCL_EMBED_KERNELS=ON -DGGML_OPENCL_USE_ADRENO_KERNELS=ON\n4.\ncmake --build build-android \ncmake --build . --config Release\n\n\nIt worked fine and built successfully but got lower performance than CPU.\n**GPU bench:**\n~/.../build-android/bin $ ./llama-bench -m /data/local/tmp/llama.cpp/SmolVLM2-500M-Video-Instruct-Q8_0.gguf\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 830 (OpenCL 3.0 Adreno(TM) 830)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.47.14.01\nggml_opencl: vector subgroup broadcast support: true\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels............................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 830 (OpenCL 3.0 Adreno(TM) 830)'\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | OpenCL     |  99 |           pp512 |        115.82 \u00b1 2.96 |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | OpenCL     |  99 |           tg128 |         14.31 \u00b1 0.19 |\n\nbuild: 53ae3064 (5528)\n\n\n**CPU bench:**\n~/.../build-android-cpu/bin $ ./llama-bench -m /data/local/tmp/llama.cpp/SmolVLM2-500M-Video-Instruct-Q8_0.gguf\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | CPU        |       8 |           pp512 |        404.00 \u00b1 3.75 |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | CPU        |       8 |           tg128 |        109.77 \u00b1 0.44 |\n\nbuild: 53ae3064 (5528)\n\n\nI really confused on it. Is it due to any errors in my compilation process or is it not optimized properly?\nopenCL version? or android ndk version? \nI am a newcomer, thank you.\n",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-30T04:47:18+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13910/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13910"
  },
  {
    "number": 14186,
    "title": "Misc. bug: evaluate_and_capture_cuda_graph NULL POINTER DEREFERENCE",
    "body": "### Name and Version\n\nRelease b5664\nggml-cuda.cu line 2659 exception during call to evaluate_and_capture_cuda_graph\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n\n`if (ggml_is_empty(node) || node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || \n    node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE || node->op == GGML_OP_NONE) {\n    continue;\n   }`\n\nRuntime exception attempting to dereference a null pointer node->buffer\n\nFunction does not check to evaluate if node->buffer is null, there are other evaluations here related to node->op, the first evaluation ggml_is_empty(node) might be the more appropriate place to check if the buffer is null. I'm not really sure. I implemented the change below in my local source, recompiled, and the issue vanished.\n\n`if (ggml_is_empty(node) || node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || \n    node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE || node->op == GGML_OP_NONE || \n    node->buffer == nullptr) {\n    continue;\n}`\n\nI don't understand even 5% of the inner workings of llama.cpp, I don't know why the buffer was null, perhaps it's normal for that to happen, or perhaps I'm misusing the library in my code somewhere. I do know, after adding the nullptr check in my local source and recompiling I have had 0 issues. Extensively tested for a few hours and that was the only change that was made.\n\nReally wish I had more insight to add here.. An expert on the internal workings of the library will need to say whether or not adding this nullptr here is safe, or if there's a bigger issue somewhere else that caused the buffer to be null.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-15T01:26:34+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14186/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14186"
  },
  {
    "number": 14216,
    "title": "Feature Request: llama-server: a flag for limiting input image size",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nA flag for limiting the maximum input image size for vision models, resizing the images if necessary, which may help avoiding OOM issues.\n\n### Motivation\n\nCertain vision models like Mistral Small 3.1 support large images which apparently can balloon token usage significantly (beyond the context memory used by the text model; could be a bug) in llama-server and cause OOM with resulting crash. If we could limit maximum image resolution to a specific value, it might be possible to avoid this problem.\n\nIt's worth pointing out that kobold.cpp already offers something along these lines. From the program help:\n\n```\n...\n--visionmaxres [max px]\n                        Clamp MMProj vision maximum allowed resolution. Allowed values are between 512 to 2048 px (default 1024).\n...\n```\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-16T14:31:51+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14216/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14216"
  },
  {
    "number": 14195,
    "title": "Misc. bug: full-cuda docker build needs ldconfig before launching llama-*",
    "body": "### Name and Version\n\ndocker pull ghcr.io/ggml-org/llama.cpp:full-cuda-b5664\n\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\ndocker pull ghcr.io/ggml-org/llama.cpp:full-cuda-b5664\n\nFail case, without ldconfig\n\ndocker run --gpus all --runtime=nvidia -v ./models:/models  -p 0.0.0.0:8080:8080     ghcr.io/ggml-org/llama.cpp:full-cuda --run  -m /models/google_gemma-3-27b-it-Q8_0.gguf -ngl 99\nggml_cuda_init: failed to initialize CUDA: forward compatibility was attempted on non supported HW\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-skylakex.so\nwarning: no usable GPU found, --gpu-layers option will be ignored\n\n\nOk case, with ldconfig\n\ndocker run --gpus all --runtime=nvidia -v ./models:/models  -p 0.0.0.0:8080:8080  --entrypoint /bin/bash   ghcr.io/ggml-org/llama.cpp:full-cuda   -c \"ldconfig && /app/llama-server  -m /models/google_gemma-3-27b-it-Q8_0.gguf -ngl 99 --host 0.0.0.0\"\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-skylakex.so\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-15T13:17:09+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14195/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14195"
  },
  {
    "number": 13298,
    "title": "Eval bug: Can't run Qwen3-32B Q4_K_XL",
    "body": "### Name and Version\n\nbuild: 5273 (8ae5ebcf) with gcc-14 (Homebrew GCC 14.2.0_1) 14.2.0 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\n2x T4\n\n### Models\n\nhttps://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-UD-Q4_K_XL.gguf\n\n### Problem description & steps to reproduce\n\nNaN perplexity and completely trashed output while using [this model](https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-UD-Q4_K_XL.gguf)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n  Device 1: Tesla T4, compute capability 7.5, VMM: yes\nbuild: 5273 (8ae5ebcf) with gcc-14 (Homebrew GCC 14.2.0_1) 14.2.0 for x86_64-pc-linux-gnu\nllama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (Tesla T4) - 14992 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 707 tensors from /root/Qwen3-32B-UD-Q4_K_XL.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3-32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3-32B\nllama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   5:                         general.size_label str              = 32B\nllama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\nllama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-32B-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-32B.txt\nllama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 448\nllama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 32\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type q4_K:  293 tensors\nllama_model_loader: - type q5_K:   35 tensors\nllama_model_loader: - type q6_K:   94 tensors\nllama_model_loader: - type iq4_xs:   28 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.64 GiB (4.89 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 25600\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3-32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors:  CUDA0_Split model buffer size =  9285.39 MiB\nload_tensors:  CUDA1_Split model buffer size =  9383.22 MiB\nload_tensors:        CUDA0 model buffer size =     1.32 MiB\nload_tensors:        CUDA1 model buffer size =     1.26 MiB\nload_tensors:   CPU_Mapped model buffer size =   417.30 MiB\n................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 4\nllama_context: n_ctx         = 2048\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     2.32 MiB\nllama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =   264.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   248.00 MiB\nllama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_context:      CUDA0 compute buffer size =   312.00 MiB\nllama_context:      CUDA1 compute buffer size =   312.00 MiB\nllama_context:  CUDA_Host compute buffer size =    14.01 MiB\nllama_context: graph nodes  = 2438\nllama_context: graph splits = 3\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nperplexity: tokenizing the input ..\nperplexity: tokenization took 786.684 ms\nperplexity: calculating perplexity over 528 chunks, n_ctx=512, batch_size=2048, n_seq=4\nperplexity: 7.88 seconds per pass - ETA 17.32 minutes\n[1]nan,[2]nan,[3]nan,[4]nan,[5]nan,[6]nan,[7]nan,[8]nan,^C\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-04T11:24:21+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13298/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13298"
  },
  {
    "number": 12954,
    "title": "Feature Request: Improve model load time when using the RPC backend",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nLoad model faster when using one or several RPC servers\n\n### Motivation\n\nThe local cache of the `rpc-server` made things better but there is still room for improvements.\n\n### Possible Implementation\n\nWe may explore storing pre-computed hashes in GGUF and avoid loading the entire model on the main host.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-15T07:54:42+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12954/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12954"
  },
  {
    "number": 12331,
    "title": "Support Hybrid Models",
    "body": "### Name and Version\n\nLast commit\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nthreadripper 7980x rtx 5090/w7900 dual slot\n\n### Models\n\nhttps://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/\n\n### Problem description & steps to reproduce\n\nHybrid models not supported:\nSupport:\n[Hymba](https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/)\nA hybrid attention mechanism combining local sliding window attention and global attention.\nGrouped-query attention (GQA).\nA mix of global and local rotary embeddings.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nnot load correctly tensors\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-11T11:46:29+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12331/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12331"
  },
  {
    "number": 14114,
    "title": "Misc. bug: Stuck while loading the model",
    "body": "### Name and Version\n\n[b5298](https://github.com/ggml-org/llama.cpp/releases/tag/b5298)\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n./llama-server \\\n    --model /mnt/ssd/models/gguf/Openhands-32B-Q8_0/all-hands_openhands-lm-32b-v0.1-Q8_0.gguf \\\n    -a oh32q8 \\\n    --host 0.0.0.0 \\\n    --port 9000 \\\n    --api-key Llh123456@ \\\n    --ctx-size 128000 \\\n    --no-webui \\\n    --n-gpu-layers 65 \\\n    --mlock \\\n    --tensor-split \"0.15,0.15,0.25,0.15,0.15,0.15\" \\\n    --main-gpu 0 \\\n    --flash-attn \\\n    --defrag-thold 0.2 \\\n    --split-mode layer\n```\n\n```shell\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\nexport CUDA_VISIBLE_DEVICES=4\n./llama-server \\\n    --model /mnt/ssd/models/gguf/DSR1Q38BQ8/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf \\\n    -a dsq3q8 \\\n    --host 0.0.0.0 \\\n    --port 9001 \\\n    --api-key Llh123456@ \\\n    --ctx-size 55000 \\\n    --no-webui \\\n    --n-gpu-layers 37 \\\n    --mlock \\\n    #--tensor-split \"0,0,0,0,0.3\"\n    --main-gpu 0 \\\n    --flash-attn \\\n    --defrag-thold 0.2 \\\n    --split-mode layer\nunset CUDA_VISIBLE_DEVICES\n```\n\n```shell\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\nexport CUDA_VISIBLE_DEVICES=1,2,3\n./llama-server \\\n    --model /mnt/ssd/models/gguf/Qwen_QwQ-32B-Q8_0/qwq-32b-q8_0.gguf \\\n    -a qwq32q8 \\\n    --host 0.0.0.0 \\\n    --port 9000 \\\n    --api-key Llh123456@ \\\n    --ctx-size 110000 \\\n    --no-webui \\\n    --n-gpu-layers 65 \\\n    --mlock \\\n    --tensor-split \"0.3,0.3,0.3\" \\\n    --main-gpu 0 \\\n    --flash-attn \\\n    --defrag-thold 0.2 \\\n    --split-mode layer\nunset CUDA_VISIBLE_DEVICES\n```\n\n### Problem description & steps to reproduce\n\nI have tried compiling many versions. Problems started occurring after version B5298. Before that, everything was normal.On flight B5298, I was unable to successfully load the three models, but on flight B5297, everything ran perfectly.\nHere is the log.\n\n----------------------\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 5 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080 Ti, compute capability 8.6, VMM: yes\n  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n  Device 2: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n  Device 3: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n  Device 4: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\nbuild: 5298 (141a908a) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 32\n\nsystem_info: n_threads = 32 (n_threads_batch = 32) / 32 | CUDA : ARCHS = 500,610,700,750,800,860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nWeb UI is disabled\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 9000, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/mnt/ssd/models/gguf/Openhands-32B-Q8_0/all-hands_openhands-lm-32b-v0.1-Q8_0.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3080 Ti) - 11567 MiB free\n\nstuck\n-------------------------\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\nbuild: 5298 (141a908a) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 32\n\nsystem_info: n_threads = 32 (n_threads_batch = 32) / 32 | CUDA : ARCHS = 500,610,700,750,800,860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nWeb UI is disabled\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 9001, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/mnt/ssd/models/gguf/DSR1Q38BQ8/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf'\n\n\nstuck\n---------------------------\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 3 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n  Device 2: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\nbuild: 5298 (141a908a) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 32\n\nsystem_info: n_threads = 32 (n_threads_batch = 32) / 32 | CUDA : ARCHS = 500,610,700,750,800,860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nWeb UI is disabled\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 9000, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/mnt/ssd/models/gguf/Qwen_QwQ-32B-Q8_0/qwq-32b-q8_0.gguf'\n\n\nstuck\n------------------------------\nThe command I use for compilation\ncmake -B build -DGGML_CUDA=ON\ncmake --build build --config Release  -j30 --clean-first\n\nCould it be related to these compilation modifications?\n![Image](https://github.com/user-attachments/assets/70e544ac-f5b8-4fd4-b7f4-735a456f5929)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-11T02:21:34+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14114/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14114"
  },
  {
    "number": 14178,
    "title": "Misc. bug: Failure to allocate buffer with ROCm 6.4",
    "body": "### Name and Version\n\nroot@llama-0:/app# ./llama-server --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\nload_backend: loaded ROCm backend from /app/libggml-hip.so\nload_backend: loaded CPU backend from /app/libggml-cpu-icelake.so\nversion: 5662 (fb85a288)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library), llama-cli, llama-server\n\n### Command line\n\n```shell\n/app/llama-server --port ${PORT}\n      -m /data/Qwen3-30B-A3B-UD-Q4_K_XL.gguf\n      -ngl 99 -t 6\n      --cache-type-k q8_0\n      --cache-type-v q8_0\n      --ctx-size 32768\n      --flash-attn\n```\n\n### Problem description & steps to reproduce\n\n* Build llama.cpp with ROCm 6.4\n* Attempt to load large model (e.g `Qwen3-30B-A3B-UD-Q4_K_XL.gguf`)\n* llama.cpp reports it is unable to allocate buffer\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n# During build\nIn file included from /app/ggml/src/ggml-cuda/acc.cu:1:\nIn file included from /app/ggml/src/ggml-cuda/acc.cuh:1:\n/app/ggml/src/ggml-cuda/common.cuh:266:12: warning: macro '__AMDGCN_WAVEFRONT_SIZE' has been marked as deprecated: compile-time-constant access to the wavefront size will be removed in a future release [-Wdeprecated-pragma]\n  266 |     return __AMDGCN_WAVEFRONT_SIZE;\n      |            ^\n<built-in>:891:139: note: macro marked 'deprecated' here\n  891 | #pragma clang deprecated(__AMDGCN_WAVEFRONT_SIZE, \"compile-time-constant access to the wavefront size will be removed in a future release\")\n      |                                                                                                                                           ^\n\n# Loading model (qwen3:30b, Q4)\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\nload_backend: loaded ROCm backend from /app/libggml-hip.so\nload_backend: loaded CPU backend from /app/libggml-cpu-icelake.so\nbuild: 5662 (fb85a288) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\nsystem info: n_threads = 6, n_threads_batch = 6, total_threads = 8\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 8 | ROCm : NO_VMM = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 7\nmain: loading model\nsrv    load_model: loading model '/data/Qwen3-30B-A3B-UD-Q4_K_XL.gguf'\nllama_model_load_from_file_impl: using device ROCm0 (Radeon RX 7900 XT) - 20420 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 579 tensors from /data/Qwen3-30B-A3B-UD-Q4_K_XL.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3-30B-A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3-30B-A3B\nllama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   5:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   7:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv   8:                    qwen3moe.context_length u32              = 40960\nllama_model_loader: - kv   9:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv  10:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  11:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  12:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  13:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  14:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  16:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  17:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  18:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  19:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                          general.file_type u32              = 15\nllama_model_loader: - kv  31:                      quantize.imatrix.file str              = Qwen3-30B-A3B-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-30B-A3B.txt\nllama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 384\nllama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 685\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  290 tensors\nllama_model_loader: - type q5_K:   37 tensors\nllama_model_loader: - type q6_K:   11 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 16.49 GiB (4.64 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 48\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 6144\nprint_info: n_expert         = 128\nprint_info: n_expert_used    = 8\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 30B.A3B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3-30B-A3B\nprint_info: n_ff_exp         = 768\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 16722.37 MiB on device 0: cudaMalloc failed: out of memory\nalloc_tensor_range: failed to allocate ROCm0 buffer of size 17534674944\nllama_model_load: error loading model: unable to allocate ROCm0 buffer\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '/data/Qwen3-30B-A3B-UD-Q4_K_XL.gguf'\nsrv    load_model: failed to load model, '/data/Qwen3-30B-A3B-UD-Q4_K_XL.gguf'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-13T21:04:19+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14178"
  },
  {
    "number": 14113,
    "title": "Misc. bug: --cache-reuse no longer seems to be caching prompt prefixes",
    "body": "### Name and Version\n\n**Affected:**\nVersion at commit: https://github.com/ggml-org/llama.cpp/commit/b7a17463ec190aeee7b9077c606c910fb4688b84\n\n**Not affected:**\nVersion at commit: https://github.com/ggml-org/llama.cpp/commit/c6a2c9e7411f54b0770b319740561bbd6a2ebd27\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Problem description & steps to reproduce\n\nI had open this bug in `oobabooga/text-generation-webui`: https://github.com/oobabooga/text-generation-webui/issues/7060\n\nThe issue being that prompt prefixes were no longer being used in the following requests.\n\nI confirmed that `--cache-reuse 1` was being passed on, so that wasn't the issue.\nAfter reverting to the previous version of the WebUI (which ships an older version of `llama.cpp`), the prompts started to be cached again.\n\nSo, this seems to point to being a bug with `llama.cpp`.\n\n### First Bad Commit\n\nIt looks like there may have been a commit between https://github.com/ggml-org/llama.cpp/commit/c6a2c9e7411f54b0770b319740561bbd6a2ebd27 and https://github.com/ggml-org/llama.cpp/commit/b7a17463ec190aeee7b9077c606c910fb4688b84 that broke `--cache-reuse`, or that changed its behavior.\n\n-----\n\nEDIT - still broken in today's commit: https://github.com/ggml-org/llama.cpp/commit/2bb0467043258bdc58dbaefb33786f1731b38937\n\n-----\n\nEDIT 2 - someone seems to have reported the same thing here: https://github.com/ggml-org/llama.cpp/pull/13576#discussion_r2095650318 (although that PR is not yet merged)\n\nThere was another recent change, here: https://github.com/ggml-org/llama.cpp/commit/06a92a193a07afe445929607be9d5e4d033956fb",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-11T02:11:50+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14113/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14113"
  },
  {
    "number": 14075,
    "title": "Eval bug: Model produces gibberish or repeated output when using `-sm row` on CUDA",
    "body": "### Name and Version\n\nversion: 5605 (5787b5da)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nEPYC 7352 + 4x RTX Quadro 5000 16GB\n\n### Models\n\nhttps://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-70B-GGUF/tree/main/DeepSeek-R1-Distill-Llama-70B-Q5_K_M\n\n### Problem description & steps to reproduce\n\nWhen using `-sm row` with `-fa` with latest container of `llama.cpp:server-cuda` the generated output is just `GGGGGGGGGGGG` repeated or gibberish on a different model, when running this inference, the last GPU in the system (ID: 3) was pinned at 100% usage for a long time for each token while the other GPU's sat idle as per `nvidia-smi`. the issue persists whether using `-ub 128` or not. the symptom does look very much like #13545 so this might not be a backend issue, but thats a speculation and maybe that issue has a different cause.\n\nconfigs used are as follows\n```\n        - '-m'\n        - $(MODEL)\n        - '-c'\n        - '2048'\n        - '--n-gpu-layers'\n        - '1000'\n        - '-sm'\n        - row\n        - '-fa'\n        - '-v'\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 4 CUDA devices:\n  Device 0: Quadro RTX 5000, compute capability 7.5, VMM: yes\n  Device 1: Quadro RTX 5000, compute capability 7.5, VMM: yes\n  Device 2: Quadro RTX 5000, compute capability 7.5, VMM: yes\n  Device 3: Quadro RTX 5000, compute capability 7.5, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-haswell.so\nwarn: LLAMA_ARG_HOST environment variable is set, but will be overwritten by command line argument --host\nbuild: 5605 (5787b5da) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 48\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 48 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8000, http threads: 47\nmain: loading model\nsrv    load_model: loading model '/models/DeepSeek-R1-Distill-Llama-70B-Q6_K.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (Quadro RTX 5000) - 15809 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (Quadro RTX 5000) - 15809 MiB free\nllama_model_load_from_file_impl: using device CUDA2 (Quadro RTX 5000) - 15809 MiB free\nllama_model_load_from_file_impl: using device CUDA3 (Quadro RTX 5000) - 15809 MiB free\nllama_model_loader: loaded meta data with 37 key-value pairs and 724 tensors from /models/DeepSeek-R1-Distill-Llama-70B-Q6_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 70B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama\nllama_model_loader: - kv   4:                         general.size_label str              = 70B\nllama_model_loader: - kv   5:                          llama.block_count u32              = 80\nllama_model_loader: - kv   6:                       llama.context_length u32              = 131072\nllama_model_loader: - kv   7:                     llama.embedding_length u32              = 8192\nllama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 28672\nllama_model_loader: - kv   9:                 llama.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128001\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - kv  29:                          general.file_type u32              = 18\nllama_model_loader: - kv  30:                      quantize.imatrix.file str              = /models_out/DeepSeek-R1-Distill-Llama...\nllama_model_loader: - kv  31:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  32:             quantize.imatrix.entries_count i32              = 560\nllama_model_loader: - kv  33:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - kv  34:                                   split.no u16              = 0\nllama_model_loader: - kv  35:                        split.tensors.count i32              = 724\nllama_model_loader: - kv  36:                                split.count u16              = 0\nllama_model_loader: - type  f32:  162 tensors\nllama_model_loader: - type q6_K:  562 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q6_K\nprint_info: file size   = 53.91 GiB (6.56 BPW)\ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\nload: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\nload: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\nload: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\nload: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\nload: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\nload: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\nload: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\nload: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\nload: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\nload: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\nload: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\nload: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\nload: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\nload: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\nload: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\nload: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\nload: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\nload: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\nload: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\nload: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\nload: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\nload: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\nload: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\nload: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\nload: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\nload: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\nload: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\nload: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\nload: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\nload: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\nload: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\nload: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\nload: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\nload: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\nload: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\nload: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\nload: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\nload: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\nload: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\nload: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\nload: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\nload: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\nload: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\nload: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\nload: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\nload: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\nload: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\nload: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\nload: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\nload: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\nload: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\nload: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\nload: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\nload: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\nload: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\nload: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\nload: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\nload: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\nload: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\nload: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\nload: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\nload: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\nload: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\nload: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\nload: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\nload: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\nload: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\nload: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\nload: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\nload: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\nload: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\nload: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\nload: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\nload: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\nload: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\nload: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\nload: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\nload: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\nload: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\nload: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\nload: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\nload: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\nload: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\nload: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\nload: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\nload: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\nload: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\nload: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\nload: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\nload: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\nload: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\nload: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\nload: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\nload: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\nload: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\nload: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\nload: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\nload: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\nload: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\nload: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\nload: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\nload: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\nload: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\nload: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\nload: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\nload: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\nload: control token: 128012 '<\uff5cAssistant\uff5c>' is not marked as EOG\nload: control token: 128011 '<\uff5cUser\uff5c>' is not marked as EOG\nload: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\nload: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\nload: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\nload: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\nload: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\nload: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\nload: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\nload: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\nload: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\nload: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\nload: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\nload: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\nload: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\nload: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\nload: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\nload: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\nload: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\nload: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\nload: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\nload: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\nload: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\nload: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\nload: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\nload: control token: 128007 '<|end_header_id|>' is not marked as EOG\nload: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\nload: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\nload: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\nload: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\nload: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\nload: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\nload: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\nload: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\nload: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\nload: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\nload: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\nload: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\nload: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\nload: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\nload: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\nload: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\nload: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\nload: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\nload: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\nload: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\nload: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\nload: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\nload: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\nload: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\nload: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\nload: control token: 128000 '<\uff5cbegin\u2581of\u2581sentence\uff5c>' is not marked as EOG\nload: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\nload: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\nload: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\nload: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\nload: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\nload: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\nload: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\nload: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\nload: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\nload: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\nload: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\nload: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\nload: control token: 128006 '<|start_header_id|>' is not marked as EOG\nload: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\nload: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\nload: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\nload: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\nload: control token: 128001 '<\uff5cend\u2581of\u2581sentence\uff5c>' is not marked as EOG\nload: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\nload: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\nload: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\nload: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\nload: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\nload: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\nload: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\nload: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\nload: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\nload: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\nload: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\nload: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\nload: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\nload: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\nload: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\nload: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\nload: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\nload: control token: 128010 '<|python_tag|>' is not marked as EOG\nload: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\nload: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\nload: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\nload: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\nload: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\nload: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\nload: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\nload: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\nload: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\nload: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\nload: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\nload: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\nload: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\nload: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\nload: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\nload: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\nload: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\nload: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\nload: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\nload: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\nload: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\nload: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\nload: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\nload: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\nload: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\nload: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\nload: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\nload: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\nload: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\nload: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\nload: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\nload: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\nload: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\nload: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\nload: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\nload: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\nload: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\nload: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\nload: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\nload: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\nload: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\nload: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\nload: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\nload: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\nload: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\nload: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\nload: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\nload: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\nload: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\nload: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\nload: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\nload: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\nload: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\nload: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\nload: control token: 128015 '<\uff5c\u2581pad\u2581\uff5c>' is not marked as EOG\nload: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\nload: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 8192\nprint_info: n_layer          = 80\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 28672\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 70B\nprint_info: model params     = 70.55 B\nprint_info: general.name     = DeepSeek R1 Distill Llama 70B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\nprint_info: EOS token        = 128001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: PAD token        = 128001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 128001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   1 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   2 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   3 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   4 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   5 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   6 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   7 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   8 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   9 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  10 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  11 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  12 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  13 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  14 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  15 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  16 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  17 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  18 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  19 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  20 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  21 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  22 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  23 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  24 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  25 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  26 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  27 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  28 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  29 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  30 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  31 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  32 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  33 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  34 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  35 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  36 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  37 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  38 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  39 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  40 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  41 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  42 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  43 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  44 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  45 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  46 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  47 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  48 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  49 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  50 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  51 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  52 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  53 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  54 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  55 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  56 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  57 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  58 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  59 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  60 assigned to device CUDA2, is_swa = 0\nload_tensors: layer  61 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  62 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  63 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  64 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  65 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  66 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  67 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  68 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  69 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  70 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  71 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  72 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  73 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  74 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  75 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  76 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  77 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  78 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  79 assigned to device CUDA3, is_swa = 0\nload_tensors: layer  80 assigned to device CUDA3, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q6_K) (and 241 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\nsrv  log_server_r: request: GET /health 192.168.2.62 503\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"error\":{\"code\":503,\"message\":\"Loading model\",\"type\":\"unavailable_error\"}}\nload_tensors: offloading 80 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 81/81 layers to GPU\nload_tensors:  CUDA0_Split model buffer size = 14056.88 MiB\nload_tensors:  CUDA1_Split model buffer size = 13387.50 MiB\nload_tensors:  CUDA2_Split model buffer size = 13387.50 MiB\nload_tensors:  CUDA3_Split model buffer size = 13540.08 MiB\nload_tensors:        CUDA0 model buffer size =     1.31 MiB\nload_tensors:        CUDA1 model buffer size =     1.25 MiB\nload_tensors:        CUDA2 model buffer size =     1.25 MiB\nload_tensors:        CUDA3 model buffer size =     1.22 MiB\nload_tensors:   CPU_Mapped model buffer size =   821.95 MiB\n..................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 2048\nllama_context: n_ctx_per_seq = 2048\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\ncreate_memory: n_ctx = 2048 (padded)\nllama_kv_cache_unified: layer   0: dev = CUDA0\nllama_kv_cache_unified: layer   1: dev = CUDA0\nllama_kv_cache_unified: layer   2: dev = CUDA0\nllama_kv_cache_unified: layer   3: dev = CUDA0\nllama_kv_cache_unified: layer   4: dev = CUDA0\nllama_kv_cache_unified: layer   5: dev = CUDA0\nllama_kv_cache_unified: layer   6: dev = CUDA0\nllama_kv_cache_unified: layer   7: dev = CUDA0\nllama_kv_cache_unified: layer   8: dev = CUDA0\nllama_kv_cache_unified: layer   9: dev = CUDA0\nllama_kv_cache_unified: layer  10: dev = CUDA0\nllama_kv_cache_unified: layer  11: dev = CUDA0\nllama_kv_cache_unified: layer  12: dev = CUDA0\nllama_kv_cache_unified: layer  13: dev = CUDA0\nllama_kv_cache_unified: layer  14: dev = CUDA0\nllama_kv_cache_unified: layer  15: dev = CUDA0\nllama_kv_cache_unified: layer  16: dev = CUDA0\nllama_kv_cache_unified: layer  17: dev = CUDA0\nllama_kv_cache_unified: layer  18: dev = CUDA0\nllama_kv_cache_unified: layer  19: dev = CUDA0\nllama_kv_cache_unified: layer  20: dev = CUDA0\nllama_kv_cache_unified: layer  21: dev = CUDA1\nllama_kv_cache_unified: layer  22: dev = CUDA1\nllama_kv_cache_unified: layer  23: dev = CUDA1\nllama_kv_cache_unified: layer  24: dev = CUDA1\nllama_kv_cache_unified: layer  25: dev = CUDA1\nllama_kv_cache_unified: layer  26: dev = CUDA1\nllama_kv_cache_unified: layer  27: dev = CUDA1\nllama_kv_cache_unified: layer  28: dev = CUDA1\nllama_kv_cache_unified: layer  29: dev = CUDA1\nllama_kv_cache_unified: layer  30: dev = CUDA1\nllama_kv_cache_unified: layer  31: dev = CUDA1\nllama_kv_cache_unified: layer  32: dev = CUDA1\nllama_kv_cache_unified: layer  33: dev = CUDA1\nllama_kv_cache_unified: layer  34: dev = CUDA1\nllama_kv_cache_unified: layer  35: dev = CUDA1\nllama_kv_cache_unified: layer  36: dev = CUDA1\nllama_kv_cache_unified: layer  37: dev = CUDA1\nllama_kv_cache_unified: layer  38: dev = CUDA1\nllama_kv_cache_unified: layer  39: dev = CUDA1\nllama_kv_cache_unified: layer  40: dev = CUDA1\nllama_kv_cache_unified: layer  41: dev = CUDA2\nllama_kv_cache_unified: layer  42: dev = CUDA2\nllama_kv_cache_unified: layer  43: dev = CUDA2\nllama_kv_cache_unified: layer  44: dev = CUDA2\nllama_kv_cache_unified: layer  45: dev = CUDA2\nllama_kv_cache_unified: layer  46: dev = CUDA2\nllama_kv_cache_unified: layer  47: dev = CUDA2\nllama_kv_cache_unified: layer  48: dev = CUDA2\nllama_kv_cache_unified: layer  49: dev = CUDA2\nllama_kv_cache_unified: layer  50: dev = CUDA2\nllama_kv_cache_unified: layer  51: dev = CUDA2\nllama_kv_cache_unified: layer  52: dev = CUDA2\nllama_kv_cache_unified: layer  53: dev = CUDA2\nllama_kv_cache_unified: layer  54: dev = CUDA2\nllama_kv_cache_unified: layer  55: dev = CUDA2\nllama_kv_cache_unified: layer  56: dev = CUDA2\nllama_kv_cache_unified: layer  57: dev = CUDA2\nllama_kv_cache_unified: layer  58: dev = CUDA2\nllama_kv_cache_unified: layer  59: dev = CUDA2\nllama_kv_cache_unified: layer  60: dev = CUDA2\nllama_kv_cache_unified: layer  61: dev = CUDA3\nllama_kv_cache_unified: layer  62: dev = CUDA3\nllama_kv_cache_unified: layer  63: dev = CUDA3\nllama_kv_cache_unified: layer  64: dev = CUDA3\nllama_kv_cache_unified: layer  65: dev = CUDA3\nllama_kv_cache_unified: layer  66: dev = CUDA3\nllama_kv_cache_unified: layer  67: dev = CUDA3\nllama_kv_cache_unified: layer  68: dev = CUDA3\nllama_kv_cache_unified: layer  69: dev = CUDA3\nllama_kv_cache_unified: layer  70: dev = CUDA3\nllama_kv_cache_unified: layer  71: dev = CUDA3\nllama_kv_cache_unified: layer  72: dev = CUDA3\nllama_kv_cache_unified: layer  73: dev = CUDA3\nllama_kv_cache_unified: layer  74: dev = CUDA3\nllama_kv_cache_unified: layer  75: dev = CUDA3\nllama_kv_cache_unified: layer  76: dev = CUDA3\nllama_kv_cache_unified: layer  77: dev = CUDA3\nllama_kv_cache_unified: layer  78: dev = CUDA3\nllama_kv_cache_unified: layer  79: dev = CUDA3\nllama_kv_cache_unified:      CUDA0 KV buffer size =   168.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   160.00 MiB\nllama_kv_cache_unified:      CUDA2 KV buffer size =   160.00 MiB\nllama_kv_cache_unified:      CUDA3 KV buffer size =   152.00 MiB\nllama_kv_cache_unified: size =  640.00 MiB (  2048 cells,  80 layers,  1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 5\nllama_context: max_nodes = 65536\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:      CUDA0 compute buffer size =   166.00 MiB\nllama_context:      CUDA1 compute buffer size =   146.00 MiB\nllama_context:      CUDA2 compute buffer size =   146.00 MiB\nllama_context:      CUDA3 compute buffer size =   266.50 MiB\nllama_context:  CUDA_Host compute buffer size =    20.01 MiB\nllama_context: graph nodes  = 2567\nllama_context: graph splits = 5\nclear_adapter_lora: call\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nset_warmup: value = 1\nsrv  log_server_r: request: GET /health 192.168.2.62 503\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"error\":{\"code\":503,\"message\":\"Loading model\",\"type\":\"unavailable_error\"}}\nsrv  log_server_r: request: GET /health 192.168.2.62 503\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"error\":{\"code\":503,\"message\":\"Loading model\",\"type\":\"unavailable_error\"}}\nsrv  log_server_r: request: GET /health 192.168.2.62 503\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"error\":{\"code\":503,\"message\":\"Loading model\",\"type\":\"unavailable_error\"}}\nsrv  log_server_r: request: GET /health 192.168.2.62 503\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"error\":{\"code\":503,\"message\":\"Loading model\",\"type\":\"unavailable_error\"}}\nset_warmup: value = 0\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\nslot        reset: id  0 | task -1 |\nmain: model loaded\nmain: chat template, chat_template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<\uff5cUser\uff5c>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<\uff5cAssistant\uff5c><\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '' + '\\n' + tool['function']['arguments'] + '\\n' + '' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '' + '\\n' + tool['function']['arguments'] + '\\n' + '' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{{'<\uff5ctool\u2581calls\u2581end\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>' + message['content'] + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<\uff5cAssistant\uff5c>' + content + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<\uff5ctool\u2581outputs\u2581begin\uff5c><\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<\uff5cAssistant\uff5c>'}}{% endif %}, example_format: 'You are a helpful assistant\n<\uff5cUser\uff5c>Hello<\uff5cAssistant\uff5c>Hi there<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>How are you?<\uff5cAssistant\uff5c>'\nmain: server is listening on http://0.0.0.0:8000 - starting the main loop\nque    start_loop: processing new tasks\nque    start_loop: update slots\nsrv  update_slots: all slots are idle\nsrv  kv_cache_cle: clearing KV cache\nque    start_loop: waiting for new tasks\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nrequest: {\"messages\": [{\"role\": \"user\", \"content\": \"what is your name\"}], \"model\": \"deepseek-reasoner\", \"stream\": true}\nsrv  params_from_: Grammar:\nsrv  params_from_: Grammar lazy: false\nsrv  params_from_: Chat format: Content-only\nsrv  add_waiting_: add task 0 to waiting list. current waiting = 0 (before add)\nque          post: new task, id = 0/1, front = 0\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 0\nslot get_availabl: id  0 | task -1 | selected slot by lru, t_last = -1\nslot        reset: id  0 | task -1 |\nslot launch_slot_: id  0 | task 0 | launching slot : {\"id\":0,\"id_task\":0,\"n_ctx\":2048,\"speculative\":false,\"is_processing\":false,\"non_causal\":false,\"params\":{\"n_predict\":-1,\"seed\":4294967295,\"temperature\":0.800000011920929,\"dynatemp_range\":0.0,\"dynatemp_exponent\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"min_p\":0.05000000074505806,\"top_n_sigma\":-1.0,\"xtc_probability\":0.0,\"xtc_threshold\":0.10000000149011612,\"typical_p\":1.0,\"repeat_last_n\":64,\"repeat_penalty\":1.0,\"presence_penalty\":0.0,\"frequency_penalty\":0.0,\"dry_multiplier\":0.0,\"dry_base\":1.75,\"dry_allowed_length\":2,\"dry_penalty_last_n\":2048,\"dry_sequence_breakers\":[\"\\n\",\":\",\"\\\"\",\"*\"],\"mirostat\":0,\"mirostat_tau\":5.0,\"mirostat_eta\":0.10000000149011612,\"stop\":[],\"max_tokens\":-1,\"n_keep\":0,\"n_discard\":0,\"ignore_eos\":false,\"stream\":true,\"logit_bias\":[],\"n_probs\":0,\"min_keep\":0,\"grammar\":\"\",\"grammar_lazy\":false,\"grammar_triggers\":[],\"preserved_tokens\":[],\"chat_format\":\"Content-only\",\"reasoning_format\":\"deepseek\",\"reasoning_in_content\":false,\"thinking_forced_open\":false,\"samplers\":[\"penalties\",\"dry\",\"top_n_sigma\",\"top_k\",\"typ_p\",\"top_p\",\"min_p\",\"xtc\",\"temperature\"],\"speculative.n_max\":16,\"speculative.n_min\":0,\"speculative.p_min\":0.75,\"timings_per_token\":false,\"post_sampling_probs\":false,\"lora\":[]},\"prompt\":\"<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>what is your name<\uff5cAssistant\uff5c>\",\"next_token\":{\"has_next_token\":true,\"has_new_line\":false,\"n_remain\":-1,\"n_decoded\":0,\"stopping_word\":\"\"}}\nslot launch_slot_: id  0 | task 0 | processing task\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 1, front = 0\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 7\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 7, n_tokens = 7, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 7, n_tokens = 7\nsrv  update_slots: decoding batch, n_tokens = 7\nset_embeddings: value = 0\nclear_adapter_lora: call\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: G\nParsing input with format Content-only: G\nParsed message: {\"role\":\"assistant\",\"content\":\"G\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 1, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 1\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 2, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 8, n_cache_tokens = 8, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null}}],\"created\":1749424590,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424590,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: GG\nParsing input with format Content-only: GG\nParsed message: {\"role\":\"assistant\",\"content\":\"GG\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 2, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 2\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 3, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 9, n_cache_tokens = 9, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424621,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: GGG\nParsing input with format Content-only: GGG\nParsed message: {\"role\":\"assistant\",\"content\":\"GGG\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 3, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 3\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 4, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 10, n_cache_tokens = 10, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424650,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: GGGG\nParsing input with format Content-only: GGGG\nParsed message: {\"role\":\"assistant\",\"content\":\"GGGG\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 4, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 4\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 5, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 11, n_cache_tokens = 11, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424678,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: GGGGG\nParsing input with format Content-only: GGGGG\nParsed message: {\"role\":\"assistant\",\"content\":\"GGGGG\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 5, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 5\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 6, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 12, n_cache_tokens = 12, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424706,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\nsrv  update_chat_: Parsing chat message: GGGGGG\nParsing input with format Content-only: GGGGGG\nParsed message: {\"role\":\"assistant\",\"content\":\"GGGGGG\"}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 6, n_remaining = -1, next token:    38 'G'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 6\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 7, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 2048, n_past = 13, n_cache_tokens = 13, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\ndata stream, to_send: data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"G\"}}],\"created\":1749424735,\"id\":\"chatcmpl-EPgGHalVYWP04hI1Bkk5Tta7acifLg9Z\",\"model\":\"deepseek-reasoner\",\"system_fingerprint\":\"b5605-5787b5da\",\"object\":\"chat.completion.chunk\"}\nsrv  log_server_r: request: GET /health 192.168.2.62 200\nsrv  log_server_r: request:\nsrv  log_server_r: response: {\"status\":\"ok\"}\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-08T23:22:38+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14075/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14075"
  },
  {
    "number": 14011,
    "title": "Compile bug: numerous deprecation warnings when compiling in Termux",
    "body": "### Git commit\n\n~/_ai/llama.cpp $ git rev-parse HEAD                 3ac67535c86e2fc43e4eddf594412acc370bbb04\n\n### Operating systems\n\nOther? (Please let us know in description)\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nas stated in the title when compiling on Termux numerous warnings are produced even if targets compilation ends fine.\n\nFollowing is the output of the build setup and compilation steps:\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\n~/_ai/llama.cpp $ cmake -B build -DBUILD_SHARED_LIBS=OFF -DLLAMA_BUILD_TESTS=OFF -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -G Ninja                                                   -- The C compiler identification is Clang 20.1.4\n-- The CXX compiler identification is Clang 20.1.4   -- Detecting C compiler ABI info                     -- Detecting C compiler ABI info - done\n-- Check for working C compiler: /data/data/com.termux/files/usr/bin/cc - skipped                         -- Detecting C compile features\n-- Detecting C compile features - done               -- Detecting CXX compiler ABI info                   -- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /data/data/com.termux/files/usr/bin/c++ - skipped                      -- Detecting CXX compile features\n-- Detecting CXX compile features - done             -- Found Git: /data/data/com.termux/files/usr/bin/git (found version \"2.49.0\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD           -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed  -- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes          -- Found Threads: TRUE                               -- CMAKE_SYSTEM_PROCESSOR: aarch64\n-- GGML_SYSTEM_ARCH: ARM                             -- Including CPU backend                             -- Found OpenMP_C: -fopenmp=libomp (found version \"5.1\")                                                  -- Found OpenMP_CXX: -fopenmp=libomp (found version \"5.1\")\n-- Found OpenMP: TRUE (found version \"5.1\")          -- ARM detected                                      -- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E                                                 -- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n-- ARM -mcpu not found, -mcpu=native will be used    -- Performing Test GGML_MACHINE_SUPPORTS_dotprod     -- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success                                                -- Performing Test GGML_MACHINE_SUPPORTS_i8mm        -- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed                                                    -- Performing Test GGML_MACHINE_SUPPORTS_noi8mm      -- Performing Test GGML_MACHINE_SUPPORTS_noi8mm - Success                                                 -- Performing Test GGML_MACHINE_SUPPORTS_sve         -- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed\n-- Performing Test GGML_MACHINE_SUPPORTS_nosve       -- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success\n-- Performing Test GGML_MACHINE_SUPPORTS_sme         -- Performing Test GGML_MACHINE_SUPPORTS_sme - Failed-- Performing Test GGML_MACHINE_SUPPORTS_nosme\n-- Performing Test GGML_MACHINE_SUPPORTS_nosme - Success                                                  -- ARM feature DOTPROD enabled\n-- ARM feature FMA enabled                           -- ARM feature FP16_VECTOR_ARITHMETIC enabled        -- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+noi8mm+nosve+nosme                           -- Looking for pthread_create in pthreads            -- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread             -- Looking for pthread_create in pthread - found     -- Found CURL: /data/data/com.termux/files/usr/lib/libcurl.so (found version \"8.13.0\")                    -- Configuring done (15.9s)                          -- Generating done (0.2s)\n-- Build files have been written to: /data/data/com.termux/files/home/_ai/llama.cpp/build\n```\n\n### Relevant log output\n\n```shell\n~/_ai/llama.cpp $ cmake --build build --config Release -t llama-cli llama-server llama-mtmd-cli llama-gguf-hash                                                [57/86] Building CXX obje...tmd.dir/mtmd-helper.cpp.\nIn file included from /data/data/com.termux/files/home/_ai/llama.cpp/tools/mtmd/mtmd-helper.cpp:30:       /data/data/com.termux/files/home/_ai/llama.cpp/tools/mtmd/../../vendor/miniaudio/miniaudio.h:12109:5: warning: no previous prototype for function 'ma_android_sdk_version' [-Wmissing-prototypes]\n 12109 | int ma_android_sdk_version()                       |     ^                                       /data/data/com.termux/files/home/_ai/llama.cpp/tools/mtmd/../../vendor/miniaudio/miniaudio.h:12109:1: note: declare 'static' if the function is not intended to be used outside of this translation unit\n 12109 | int ma_android_sdk_version()                       | ^                                                  | static\n1 warning generated.                                 [62/86] Generating build details from Git            -- Found Git: /data/data/com.termux/files/usr/bin/git (found version \"2.49.0\")                            [68/86] Building CXX obje...s/common.dir/common.cpp. In file included from /data/data/com.termux/files/home/_ai/llama.cpp/common/common.cpp:8:                 In file included from /data/data/com.termux/files/home/_ai/llama.cpp/common/common.h:7:\nIn file included from /data/data/com.termux/files/usr/include/c++/v1/set:1494:                            In file included from /data/data/com.termux/files/usr/include/c++/v1/functional:526:                      In file included from /data/data/com.termux/files/usr/include/c++/v1/__functional/boyer_moore_searcher.h:27:                                                   In file included from /data/data/com.termux/files/usr/include/c++/v1/vector:325:\nIn file included from /data/data/com.termux/files/usr/include/c++/v1/__format/formatter_bool.h:20:        In file included from /data/data/com.termux/files/usr/include/c++/v1/__format/formatter_integral.h:35:    /data/data/com.termux/files/usr/include/c++/v1/locale:3257:1: warning: 'wstring_convert<std::codecvt_utf8<char32_t>, char32_t>' is deprecated [-Wdeprecated-declarations]                                            3257 | wstring_convert<_Codecvt, _Elem, _WideAlloc, _ByteAlloc>::to_bytes(const _Elem* __frm, const _Elem* __frm_end) {                                             | ^\n/data/data/com.termux/files/usr/include/c++/v1/locale:3161:12: note: in instantiation of member function 'std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t>::to_bytes' requested here                      3161 |     return to_bytes(__wstr.data(), __wstr.data() + __wstr.size());\n      |            ^                                 /data/data/com.termux/files/home/_ai/llama.cpp/common/common.cpp:720:52: note: in instantiation of member function 'std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t>::to_bytes' requested here             720 |         std::string filename_reencoded = converter.to_bytes(filename_utf32);                            |                                                    ^\n/data/data/com.termux/files/usr/include/c++/v1/locale:3114:28: note: 'wstring_convert<std::codecvt_utf8<char32_t>, char32_t>' has been explicitly marked deprecated here                                             3114 | class _LIBCPP_TEMPLATE_VIS _LIBCPP_DEPRECATED_IN_CXX17 wstring_convert {\n      |                            ^                 /data/data/com.termux/files/usr/include/c++/v1/__config:942:41: note: expanded from macro '_LIBCPP_DEPRECATED_IN_CXX17'                                          942 | #    define _LIBCPP_DEPRECATED_IN_CXX17 _LIBCPP_DEPRECATED\n      |                                         ^    /data/data/com.termux/files/usr/include/c++/v1/__config:915:49: note: expanded from macro '_LIBCPP_DEPRECATED'                                                   915 | #      define _LIBCPP_DEPRECATED __attribute__((__deprecated__))\n      |                                                 ^                                                 1 warning generated.\n[86/86] Linking CXX executable bin/llama-server\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-04T12:24:53+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14011/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14011"
  },
  {
    "number": 14020,
    "title": "Feature Request: support FP8 data type in llama.cpp",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nCurrently, the FP8 data type is the default data type for models like DeepSeek, which is supposed to have better accuracy than INT8 while they have the same size. vLLM already supports fp8 data type.\n\nFile the issue for request the support of fp8 in llama.cpp.\n\n### Motivation\n\nwith same level model size, FP8 is supposed to have better accuracy than int8. \n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-05T02:34:12+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14020/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14020"
  },
  {
    "number": 14251,
    "title": "Misc. bug: prompt as pasted content in the server",
    "body": "### Name and Version\n\nversion: 5684 (6adc3c3e)\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nWhen I paste a long text into the server, it appears as \"pasted content\"\nWhen I press \"Enter\" after pasting, it says \"Please enter a message\"\nI have to add something below the pasted content to send the query.\nShouldn't the pasted content be sufficient as a prompt?\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-17T22:55:41+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14251/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14251"
  },
  {
    "number": 13077,
    "title": "Refactor: (clip.cpp) identify and regroup pre-processing strategies",
    "body": "### Background Description\n\nCurrently, `clip_image_preprocess` still looks quite messy.\n\nFrom a graphic designer perspective, this function is purely just a \"photoshop in cpp\", its main purpose is to preprocess a given image before sending it to the transformer. The preprocess involves: crop / resize / pad the given image.\n\nCurrently, there are some strategies to preprocess an image:\n- Resize to a fixed (square) size and add padding if the ratio is not square (used by llava 1.5, gemma 3, GLM)  \n  Note: llava 1.5 use a gray-ish color for padding, while the rest use black color\n- Allow dynamic resolution / ratio, but limit max size (used by qwen2vl, pixtral)  \n  Image will still need to be resized to the nearest multiply of patch size\n- Crop the image into slices, aka llava-uhd (used by llava 1.6, minicpm-v)\n\n### Possible Refactor Approaches\n\nMake an enum, split into dedicated function and give them good naming.",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-23T13:12:09+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13077/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13077"
  },
  {
    "number": 14053,
    "title": "Feature Request: add support for length_penalty",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nThis is just a proposal to implement a length_penalty in llama.cpp similar to HF:\nhttps://huggingface.co/docs/transformers/v4.22.2/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.length_penalty\n```\nlength_penalty (float, optional, defaults to model.config.length_penalty or 1.0 if the config does not set any value)\n \u2014 Exponential penalty to the length. \n1.0 means that the score is penalized by the sequence length. \n0.0 means no penalty. \nSet to values < 0.0 in order to encourage the model to generate longer sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n```\n\nAs today llamacpp has repeat_last_n, repeat_penalty, frequency_penalty, and presence_penalty but no length penalty.\n\nWould start from the 1st generated token (not function to the length of the input prompt):\n```\nscore = sum_logprobs / (generated_len**self.length_penalty)\n```\nDefault to be neutral (1.0, no length penalty).\n\n\n### Motivation\n\nTo have a way to push the LLM to generate shorter or longer outputs. \n\n### Possible Implementation\n\nin llama_sampler_penalties_apply():\nhttps://github.com/ggml-org/llama.cpp/blob/745aa5319b9930068aff5e87cf5e9eef7227339b/src/llama-sampling.cpp#L1650C13-L1650C43\nsimilar to other penalties.\n\nWe can work on a PR if you d like us to.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-06T17:39:35+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14053/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14053"
  },
  {
    "number": 14015,
    "title": "Feature Request: Support Llama-Nemotron-Nano-VL-8B-V1",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nNvidia just released a new VLM: https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1\n\n- Text model: [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n- Vision encoder: [nvidia/C-RADIOv2-H](https://huggingface.co/nvidia/C-RADIOv2-H)\n\nOn `master` (`3e63a58e`), the command:\n\n```zsh\npython llama.cpp/convert_hf_to_gguf.py --outfile /opt/workspace/gguf/Llama-Nemotron-Nano-VL-8B-V1-Q8_0.gguf --outtype bf16 /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1/\n```\n\ncurrently fails:\n\n<details>\n<summary>\u2195\ufe0f Click to expand full output ...</summary>\n\n```\nINFO:hf-to-gguf:Loading model: Llama-Nemotron-Nano-VL-8B-V1\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: Loading /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: Loading /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\nINFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\nTraceback (most recent call last):\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6533, in <module>\n    main()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6527, in main\n    model_instance.write()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 403, in write\n    self.prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 2011, in prepare_tensors\n    super().prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 1979, in modify_tensors\n    return [(self.map_tensor_name(name), data_torch)]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n    raise ValueError(f\"Can not map tensor {name!r}\")\nValueError: Can not map tensor 'mlp1.0.bias'\n```\n\n</details>\n\n...even with `trust_remote_code=True`:\n\n<details>\n<summary>\u2195\ufe0f Click to expand full output ...</summary>\n\n```\nINFO:hf-to-gguf:Loading model: Llama-Nemotron-Nano-VL-8B-V1\nEncountered exception while importing open_clip: No module named 'open_clip'\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: This modeling file requires the following packages that were not found in your environment: open_clip. Run `pip install open_clip`\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nEncountered exception while importing open_clip: No module named 'open_clip'\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: This modeling file requires the following packages that were not found in your environment: open_clip. Run `pip install open_clip`\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\nINFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\nTraceback (most recent call last):\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6533, in <module>\n    main()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6527, in main\n    model_instance.write()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 403, in write\n    self.prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 2011, in prepare_tensors\n    super().prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 1979, in modify_tensors\n    return [(self.map_tensor_name(name), data_torch)]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n    raise ValueError(f\"Can not map tensor {name!r}\")\nValueError: Can not map tensor 'mlp1.0.bias'\n```\n\n</details>\n\n\nMaybe @ngxson knows how to fix this?\n\n### Motivation\n\nUsers of llama-server would like to be able to use this new Nvidia 8B model with vision, it's pretty good. You can try it at nvidia's demo site here: https://build.nvidia.com/nvidia/llama-3.1-nemotron-nano-vl-8b-v1\n\n### Possible Implementation\n\n(I don't know enough about VLMs and libmtmd to offer a possible implementation/fix, sorry)",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-04T16:13:31+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14015/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14015"
  },
  {
    "number": 14201,
    "title": "Misc. bug: LLAMA-SERVER is 40% slower than LLAMA-CLI when using identical parameters including -ot option for tensor offloading",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n  Device 1: Quadro M2000, compute capability 5.2, VMM: yes\nversion: 5614 (8f47e25f)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nCUDA_VISIBLE_DEVICES=\"0,\" \\\nnumactl --physcpubind=\"8,10,12,14, 24,26,28,30, 9,11,13,15, 25,27,29,31\" --membind=1 /home/ai/LLAMA_CPP/8f47e25f56e9792093b7497c68e9f80bab82ed19/llama.cpp/build/bin/llama-server \\\n--model /mnt/AI/LLM/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00006.gguf \\\n--threads 16 \\\n--n-gpu-layers 99 \\\n--override-tensor \".ffn_.*_exps.=CPU\"\n\n--cpunodebind=1 can be used instead of --physcpubind=\"8,10,12,14, 24,26,28,30, 9,11,13,15, 25,27,29,31\" to the same effect. Essentially, it is about exclusively using numa's NODE1 and make sure that the model is loaded accordingly close to CPU0 \n```\n\n### Problem description & steps to reproduce\n\nI get 4t/s when I run LLAMA-CLI with the same parameters while I get only 2.4t/s when running LLAMA-SERVER. What I also observe, is that CPU usage (for those carefully picked 16 cores) is at 100% while it is only 75-80% evenly distributed over 16 cores in case of LLAMA-SERVER. GPU/VRAM usage is equal in both cases.\n\nTested with https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/tree/main/Q2_K_L\nand https://huggingface.co/unsloth/DeepSeek-R1-GGUF-UD/tree/main/UD-Q2_K_XL\n\n### First Bad Commit\n\nI used this commit: 8f47e25f56e9792093b7497c68e9f80bab82ed19\nUnfornunally, the next commit (f470bc36bed4d836b9de5a483fa0dfaee176d6f5) has another issues which I might discuss in another bug report.\n\n### Relevant log output\n\n```shell\nuser@pop-os:~$ CUDA_VISIBLE_DEVICES=\"0,\" \\\nnumactl --physcpubind=\"8,10,12,14, 24,26,28,30, 9,11,13,15, 25,27,29,31\" --membind=1 /home/ai/LLAMA_CPP/8f47e25f56e9792093b7497c68e9f80bab82ed19/llama.cpp/build/bin/llama-server \\\n--model /mnt/AI/LLM/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00006.gguf \\\n--threads 16 \\\n--n-gpu-layers 99 \\\n--temp 0.6 --top_p 0.95 --min_p 0.01 \\\n--ctx-size 32768 \\\n--flash-attn \\\n--override-tensor \".ffn_.*_exps.=CPU\" --seed 1234567890\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nbuild: 5614 (8f47e25f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 16, n_threads_batch = 16, total_threads = 32\n\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/mnt/AI/LLM/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00006.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23858 MiB free\nllama_model_loader: additional 5 GGUFs metadata loaded.\nllama_model_loader: loaded meta data with 62 key-value pairs and 1086 tensors from /mnt/AI/LLM/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00006.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Deepseek-R1\nllama_model_loader: - kv   3:                           general.basename str              = Deepseek-R1\nllama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   5:                         general.size_label str              = 256x20B\nllama_model_loader: - kv   6:                            general.license str              = mit\nllama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = DeepSeek R1\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Deepseek Ai\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\nllama_model_loader: - kv  12:                               general.tags arr[str,3]       = [\"deepseek\", \"unsloth\", \"transformers\"]\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  14:                      deepseek2.block_count u32              = 61\nllama_model_loader: - kv  15:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv  16:                 deepseek2.embedding_length u32              = 7168\nllama_model_loader: - kv  17:              deepseek2.feed_forward_length u32              = 18432\nllama_model_loader: - kv  18:             deepseek2.attention.head_count u32              = 128\nllama_model_loader: - kv  19:          deepseek2.attention.head_count_kv u32              = 1\nllama_model_loader: - kv  20:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  22:                deepseek2.expert_used_count u32              = 8\nllama_model_loader: - kv  23:        deepseek2.leading_dense_block_count u32              = 3\nllama_model_loader: - kv  24:                       deepseek2.vocab_size u32              = 129280\nllama_model_loader: - kv  25:            deepseek2.attention.q_lora_rank u32              = 1536\nllama_model_loader: - kv  26:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  27:             deepseek2.attention.key_length u32              = 576\nllama_model_loader: - kv  28:           deepseek2.attention.value_length u32              = 512\nllama_model_loader: - kv  29:         deepseek2.attention.key_length_mla u32              = 192\nllama_model_loader: - kv  30:       deepseek2.attention.value_length_mla u32              = 128\nllama_model_loader: - kv  31:       deepseek2.expert_feed_forward_length u32              = 2048\nllama_model_loader: - kv  32:                     deepseek2.expert_count u32              = 256\nllama_model_loader: - kv  33:              deepseek2.expert_shared_count u32              = 1\nllama_model_loader: - kv  34:             deepseek2.expert_weights_scale f32              = 2.500000\nllama_model_loader: - kv  35:              deepseek2.expert_weights_norm bool             = true\nllama_model_loader: - kv  36:               deepseek2.expert_gating_func u32              = 2\nllama_model_loader: - kv  37:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  38:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  39:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  40: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  41: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nllama_model_loader: - kv  42:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  43:                         tokenizer.ggml.pre str              = deepseek-v3\nllama_model_loader: - kv  44:                      tokenizer.ggml.tokens arr[str,129280]  = [\"<\uff5cbegin\u2581of\u2581sentence\uff5c>\", \"<\ufffd...\nllama_model_loader: - kv  45:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  46:                      tokenizer.ggml.merges arr[str,127741]  = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"\u0120 \u0120\", \"h e...\nllama_model_loader: - kv  47:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  48:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  49:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  50:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  51:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  52:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  53:               general.quantization_version u32              = 2\nllama_model_loader: - kv  54:                          general.file_type u32              = 10\nllama_model_loader: - kv  55:                      quantize.imatrix.file str              = DeepSeek-R1-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  56:                   quantize.imatrix.dataset str              = unsloth_calibration_DeepSeek-R1.txt\nllama_model_loader: - kv  57:             quantize.imatrix.entries_count i32              = 720\nllama_model_loader: - kv  58:              quantize.imatrix.chunks_count i32              = 35\nllama_model_loader: - kv  59:                                   split.no u16              = 0\nllama_model_loader: - kv  60:                        split.tensors.count i32              = 1086\nllama_model_loader: - kv  61:                                split.count u16              = 6\nllama_model_loader: - type  f32:  361 tensors\nllama_model_loader: - type q8_0:  122 tensors\nllama_model_loader: - type q2_K:  122 tensors\nllama_model_loader: - type q3_K:   54 tensors\nllama_model_loader: - type q4_K:  389 tensors\nllama_model_loader: - type q5_K:   23 tensors\nllama_model_loader: - type q6_K:   15 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q2_K - Medium\nprint_info: file size   = 233.18 GiB (2.98 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 818\nload: token to piece cache size = 0.8223 MB\nprint_info: arch             = deepseek2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 163840\nprint_info: n_embd           = 7168\nprint_info: n_layer          = 61\nprint_info: n_head           = 128\nprint_info: n_head_kv        = 1\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 576\nprint_info: n_embd_head_v    = 512\nprint_info: n_gqa            = 128\nprint_info: n_embd_k_gqa     = 576\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 18432\nprint_info: n_expert         = 256\nprint_info: n_expert_used    = 8\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = yarn\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 0.025\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 671B\nprint_info: model params     = 671.03 B\nprint_info: general.name     = Deepseek-R1\nprint_info: n_layer_dense_lead   = 3\nprint_info: n_lora_q             = 1536\nprint_info: n_lora_kv            = 512\nprint_info: n_embd_head_k_mla    = 192\nprint_info: n_embd_head_v_mla    = 128\nprint_info: n_ff_exp             = 2048\nprint_info: n_expert_shared      = 1\nprint_info: expert_weights_scale = 2.5\nprint_info: expert_weights_norm  = 1\nprint_info: expert_gating_func   = sigmoid\nprint_info: rope_yarn_log_mul    = 0.1000\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 129280\nprint_info: n_merges         = 127741\nprint_info: BOS token        = 0 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\nprint_info: EOS token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOT token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: PAD token        = 2 '<\uff5c\u2581pad\u2581\uff5c>'\nprint_info: LF token         = 201 '\u010a'\nprint_info: FIM PRE token    = 128801 '<\uff5cfim\u2581begin\uff5c>'\nprint_info: FIM SUF token    = 128800 '<\uff5cfim\u2581hole\uff5c>'\nprint_info: FIM MID token    = 128802 '<\uff5cfim\u2581end\uff5c>'\nprint_info: EOG token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 61 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 62/62 layers to GPU\nload_tensors:   CPU_Mapped model buffer size = 45875.33 MiB\nload_tensors:   CPU_Mapped model buffer size = 47175.69 MiB\nload_tensors:   CPU_Mapped model buffer size = 46713.92 MiB\nload_tensors:   CPU_Mapped model buffer size = 47079.97 MiB\nload_tensors:   CPU_Mapped model buffer size = 46513.37 MiB\nload_tensors:   CPU_Mapped model buffer size =  4394.39 MiB\nload_tensors:        CUDA0 model buffer size =  9686.98 MiB\n....................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 32768\nllama_context: n_ctx_per_seq = 32768\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 0.025\nllama_context: n_ctx_per_seq (32768) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\nllama_kv_cache_unified:      CUDA0 KV buffer size =  4148.00 MiB\nllama_kv_cache_unified: size = 4148.00 MiB ( 32768 cells,  61 layers,  1 seqs), K (f16): 2196.00 MiB, V (f16): 1952.00 MiB\nllama_context:      CUDA0 compute buffer size =  3253.50 MiB\nllama_context:  CUDA_Host compute buffer size =    78.01 MiB\nllama_context: graph nodes  = 4904\nllama_context: graph splits = 176 (with bs=512), 118 (with bs=1)\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 32768\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 32768\nmain: model loaded\nmain: chat template, chat_template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<\uff5cUser\uff5c>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<\uff5cAssistant\uff5c><\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '' + '\\n' + tool['function']['arguments'] + '\\n' + '' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{%- else %}{{'<\uff5cAssistant\uff5c>' + message['content'] + '<\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '' + '\\n' + tool['function']['arguments'] + '\\n' + '' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '' + '\\n' + tool['function']['arguments'] + '\\n' + '' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{%- endif %}{%- endfor %}{{'<\uff5ctool\u2581calls\u2581end\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>' + message['content'] + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<\uff5cAssistant\uff5c>' + content + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<\uff5ctool\u2581outputs\u2581begin\uff5c><\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<\uff5cAssistant\uff5c><think>\\n'}}{% endif %}, example_format: 'You are a helpful assistant\n\n<\uff5cUser\uff5c>Hello<\uff5cAssistant\uff5c>Hi there<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>How are you?<\uff5cAssistant\uff5c>'\nmain: server is listening on http://127.0.0.1:8080 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: GET / 127.0.0.1 200\nsrv  log_server_r: request: GET /favicon.ico 127.0.0.1 404\nsrv  log_server_r: request: GET /props 127.0.0.1 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 10846\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2048, n_tokens = 2048, progress = 0.188825\nslot update_slots: id  0 | task 0 | kv cache rm [2048, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 4096, n_tokens = 2048, progress = 0.377651\nslot update_slots: id  0 | task 0 | kv cache rm [4096, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 6144, n_tokens = 2048, progress = 0.566476\nslot update_slots: id  0 | task 0 | kv cache rm [6144, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 8192, n_tokens = 2048, progress = 0.755301\nslot update_slots: id  0 | task 0 | kv cache rm [8192, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 10240, n_tokens = 2048, progress = 0.944127\nslot update_slots: id  0 | task 0 | kv cache rm [10240, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 10846, n_tokens = 606, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 10846, n_tokens = 606\nslot      release: id  0 | task 0 | stop processing: n_past = 11656, truncated = 0\nslot print_timing: id  0 | task 0 | \nprompt eval time =  961813.26 ms / 10846 tokens (   88.68 ms per token,    11.28 tokens per second)\n       eval time =  347536.86 ms /   811 tokens (  428.53 ms per token,     2.33 tokens per second)\n      total time = 1309350.13 ms / 11657 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-15T18:50:50+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14201/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14201"
  }
]
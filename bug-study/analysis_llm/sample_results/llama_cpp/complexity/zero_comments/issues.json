[
  {
    "number": 12680,
    "title": "Misc. bug: examples/gguf-split merge does not respect dry-run option",
    "body": "### Name and Version\n\n$ ./llama-cli --version\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz)\nload_backend: failed to find ggml_backend_init in /home/nick/Downloads/llama.cpp/build/bin/libggml-cpu.so\nversion: 5015 (59f596e5)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\n$ ./llama-gguf-split --merge --dry-run /data/models/DeepSeek-R1-Q8_0/DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00001-of-00015.gguf /data/models/DeepSeek-R1-Q8_0/DeepSeek-R1-Q8_0/DeepSeek-R1-merge.gguf\n```\n\n### Problem description & steps to reproduce\n\nexamples/gguf-split respects \"dry-run\" option for operation --split, but for --merge, dry-run is ignored.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-31T23:26:08+00:00",
    "closed_at": "2025-04-04T18:06:37+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12680/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12680"
  },
  {
    "number": 9979,
    "title": "Bug: [CANN] inference running result is garbled in debug running model for LM models who's type is Q4_0 class",
    "body": "### What happened?\n\nFor CANN backend: inference running result is garbled in debug running model for LM models who's type is Q4_0 class\n\n### Name and Version\n\nb3948\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "medium severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-21T11:35:28+00:00",
    "closed_at": "2024-10-22T08:16:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9979"
  },
  {
    "number": 3117,
    "title": "sam visual studio integration",
    "body": "dd",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-11T02:14:10+00:00",
    "closed_at": "2023-09-11T02:18:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3117"
  },
  {
    "number": 6523,
    "title": "'cmath' file not found error for AMD builds - RESOLVED - issue with gcc 11, install gcc 12",
    "body": "**Problem**\r\n\"make\" command fails with:\r\n   'cmath' file not found\r\n\r\ne.g.:\r\n     make LLAMA_HIPBLAS=1\r\n\r\n**Solution**\r\nCheck the version of GCC that you're on. If < 12 then install the latest. \r\nEg: \r\n   sudo apt install libstdc++-12-dev\r\n   \r\n   ",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-07T12:52:13+00:00",
    "closed_at": "2024-04-11T17:53:52+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6523/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6523"
  },
  {
    "number": 1002,
    "title": "[user] Pasting in multiple lines as input?",
    "body": "hello, is it possible to past miltiple line input in command prompt ( windows 11) or do i need to make it one line and use /n at where the line breaks shouyld be? thanks",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-15T18:45:28+00:00",
    "closed_at": "2023-04-15T21:46:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1002/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1002"
  },
  {
    "number": 9642,
    "title": "Feature Request: Add support for LLaMA 3.2 ",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd gguf support for new Llama 3.2 models released today (https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf). Tried converting the 1B parameter model to .gguf and ran into tokenizer issues as I believe there is now a new tokenizer being used. \r\n\r\nRunning: `version: 3826 (ea9c32be)` built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\r\n\r\nSteps to reproduce:\r\nRun `python convert_hf_to_gguf.py /yourmodeldir`\r\nWill give assertion error in `/gguf-py/gguf/vocab.py`\r\n```\r\n        if is_llama3:\r\n            raise TypeError('Llama 3 must be converted with BpeVocab')\r\n```\r\nIf adding `tokenizer.model` another error is thrown.\r\n\r\nCommenting out this line and quantizing works until trying to inference on the resulting `.gguf` file.\r\n```\r\n./llama-cli -m examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\"\r\nbuild: 3826 (ea9c32be) with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 147 tensors from examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Small Llama\r\nllama_model_loader: - kv   3:                         general.size_label str              = 1.2B\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 16\r\nllama_model_loader: - kv   5:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\r\nllama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\r\nllama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\r\nllama_model_loader: - kv  14:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128259]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,128259]  = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128259]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   34 tensors\r\nllama_model_loader: - type q4_K:   96 tensors\r\nllama_model_loader: - type q6_K:   17 tensors\r\nllm_load_vocab: SPM vocabulary, but newline token not found: unordered_map::at: key not found! Using special_pad_id instead.llm_load_vocab: control-looking token: '<|eot_id|>' was not control-type; this is probably a bug in the model. its type will be overridden\r\nllm_load_vocab: control-looking token: '<|eom_id|>' was not control-type; this is probably a bug in the model. its type will be overridden\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 259\r\nllm_load_vocab: token to piece cache size = 1.0237 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_layer          = 16\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 1.24 B\r\nllm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \r\nllm_load_print_meta: general.name     = Small Llama\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 2 '#'\r\nllm_load_print_meta: UNK token        = 0 '!'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 2 '#'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllama_model_load: error loading model: vocab size mismatch\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: failed to load model 'examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf'\r\nmain: error: unable to load model\r\n```\n\n### Motivation\n\nThese are new SOTA models and are extremely impactful for edge devices, and the larger models as well for full multimodal support. \n\n### Possible Implementation\n\nAdding support for the new tokenizer.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-09-25T19:48:36+00:00",
    "closed_at": "2024-09-25T19:51:33+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9642/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9642"
  },
  {
    "number": 12240,
    "title": "Issues while enabling MMA support on AIX machines",
    "body": "In the CMakeLists.txt, it was looking for processor type from /proc/cpuinfo file, while is absent in AIX. And also it was failing to enable MMA on P11 and above machine. So had to implement a different logic to read the processor type on AIX, and  implemented a generic logic that works both on ppcLinux and AIX. \nWill be opening a PR for the same. Let me know what you think about it. ",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-07T07:14:08+00:00",
    "closed_at": "2025-03-18T09:37:34+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12240/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12240"
  },
  {
    "number": 12823,
    "title": "Compile bug: Build failure on ppc64le from simd-mappings.h",
    "body": "### Git commit\n\n995083e\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nWhile building llama.cpp on Power (ppc64le) with gcc13, seeing below errors:\n\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake \u2013-build build_llama \u2013-config Release\nmake\n```\n\n### Relevant log output\n\n```shell\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\nllama.cpp/ggml/src/ggml-cpu/simd-mappings.h:395:58: error: lvalue required as unary \u2018&\u2019 operand\n  395 | #define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-08T10:34:23+00:00",
    "closed_at": "2025-04-09T23:18:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12823/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12823"
  },
  {
    "number": 4648,
    "title": "Support q, k, v lengths not derived from n_embd",
    "body": "# Feature Description\r\n\r\nIn the \"Attention is all you need\" paper, the queries and keys share the same dimension of $d_k$ and the values of $d_v$. Though the paper chose to make $d_k = d_v = d_{model} / h$, that is not a requirement for the network.\r\n\r\nIt would be great to support different key and value lengths.\r\n\r\n# Motivation\r\n\r\nSome upcoming models employ different key lengths than $d_{model} / h$. This feature would allow those models to be ported over to this project.\r\n\r\n# Possible Implementation\r\n\r\nOther than plumbing to get these new values for $d_k$ and $d_v$, we also have to revisit where `n_embd`, `n_embd_gqa`, `n_embd_head`, `n_rot`, and `n_head_kv` are used to make sure the assumptions are still sane.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-12-26T18:30:33+00:00",
    "closed_at": "2024-01-09T19:26:04+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4648/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4648"
  },
  {
    "number": 961,
    "title": "Typo 'atttention' in convert.py",
    "body": "https://github.com/ggerganov/llama.cpp/blob/723dac55fa2ba7adc6e3fc8609781d1ad0378906/convert.py#L121\r\n@comex ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-14T08:35:21+00:00",
    "closed_at": "2023-07-28T19:51:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/961"
  },
  {
    "number": 1414,
    "title": "--mtest option broken",
    "body": "# Expected Behavior\r\n\r\nI use the `--mtest` option and get a report on how much memory is used.\r\n\r\n# Current Behavior\r\n\r\nThe program crashes because the fist token is not `BOS`.\r\n\r\n# Environment and Context\r\n\r\nAs expected, the bug was introduced with https://github.com/ggerganov/llama.cpp/commit/f9a6364912fd0463fddfdbc9ef9f79fdc281570d\r\n\r\n<details>\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n```Architecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         43 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 3700X 8-Core Processor\r\n    CPU family:          23\r\n    Model:               113\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            0\r\n    Frequency boost:     enabled\r\n    CPU(s) scaling MHz:  77%\r\n    CPU max MHz:         4935.9370\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            7202.09\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr\r\n                         _opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3\r\n                          fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalign\r\n                         sse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pst\r\n                         ate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsav\r\n                         ec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock\r\n                          nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umi\r\n                         p rdpid overflow_recov succor smca sev sev_es\r\nVirtualization features: \r\n  Virtualization:        AMD-V\r\nCaches (sum of all):     \r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    4 MiB (8 instances)\r\n  L3:                    32 MiB (2 instances)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Linux johannes-pc 6.3.0-1-MANJARO #1 SMP PREEMPT_DYNAMIC Mon Apr  3 10:46:56 UTC 2023 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nPython 3.10.10\r\nGNU Make 4.4.1\r\ng++ (GCC) 12.2.1 20230201\r\n```\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n```\r\n./main --model ./models/llama-7b-ggml-q4_0.bin --ignore-eos --n_predict 16 --ctx_size 2048 --batch_size 512 --threads 6 --seed 1337 --mtest | tee chat.txt\r\n```\r\n\r\n# Failure Logs\r\n\r\n```\r\n/home/johannesg/Projects/llama.cpp [git::f9a6364 *] [johannesg@johannes-pc] [14:25]\r\n> ./main --model ./models/llama-7b-ggml-q4_0.bin --ignore-eos --n_predict 16 --ctx_size 2048 --batch_size 512 --threads 6 --seed 1337 --mtest | tee chat.txt \r\nWARNING: when using cuBLAS generation results are NOT guaranteed to be reproducible.\r\nmain: build = 520 (f9a6364)\r\nmain: seed  = 1337\r\nllama.cpp: loading model from ./models/llama-7b-ggml-q4_0.bin\r\nllama_model_load_internal: format     = ggjt v1 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =  68.20 KB\r\nllama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\r\nllama_init_from_file: kv self size  = 1024.00 MB\r\n\r\nsystem_info: n_threads = 6 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nllama_eval_internal: first token must be BOS\r\nllama_eval: failed to eval\r\n\r\nllama_print_timings:        load time =  1193.45 ms\r\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\r\nllama_print_timings:        eval time =   112.75 ms /     1 runs   (  112.75 ms per run)\r\nllama_print_timings:       total time =  1193.45 ms\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-12T12:31:24+00:00",
    "closed_at": "2023-05-12T18:44:51+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1414"
  },
  {
    "number": 2353,
    "title": "[Feature Request]: 32k context",
    "body": "can the internal prompt limit be raised to 32k for llama 2 models? I'm only assuming this works because the llama 2 context is double the previous.\r\n```\r\nmain: error: prompt is too long (30474 tokens, max 16380\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-23T22:14:34+00:00",
    "closed_at": "2023-07-23T22:15:55+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2353"
  },
  {
    "number": 14239,
    "title": "W",
    "body": "> [](url) \n\n _Originally posted by @ThadCash187 in [26c0846](https://github.com/ggml-org/llama.cpp/commit/26c084662903ddaca19bef982831bfb0856e8257#r160230793)_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-17T13:06:40+00:00",
    "closed_at": "2025-06-17T14:22:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14239/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14239"
  },
  {
    "number": 3144,
    "title": "Building shared lib with ROCm fails on Windows",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nllama.dll with hipBLAS support\r\n\r\n# Current Behavior\r\n\r\nI used this command to generate the build files:\r\n```\r\ncmake -B build -G \"Ninja\" -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1012 -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON\r\n```\r\nAnd this to build the library\r\n```\r\ncmake --build build\r\n```\r\n\r\nWhen running, I get this error:\r\n```\r\nlld-link: error: undefined symbol: ggml_nelements\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(void __cdecl test_roundtrip_on_layer(class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>> &, bool, struct ggml_type_traits_t const &, bool, struct ggml_tensor const *, class std::vector<float, class std::allocator<float>> &, class std::vector<char, class std::allocator<char>> &, class std::vector<float, class std::allocator<float>> &, struct error_stats &, int))\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n\r\nlld-link: error: undefined symbol: ggml_time_init\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n\r\nlld-link: error: undefined symbol: ggml_type_name\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n>>> referenced 2 more times\r\n\r\nlld-link: error: undefined symbol: ggml_time_us\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n\r\nlld-link: error: undefined symbol: class std::vector<struct std::pair<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct ggml_tensor *>, class std::allocator<struct std::pair<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct ggml_tensor *>>> const & __cdecl llama_internal_get_tensor_map(struct llama_context *)\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\n\r\nlld-link: error: undefined symbol: ggml_internal_get_type_traits\r\n>>> referenced by examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.obj:(main)\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\nThe error is way longer with more \"undefined symbol...\"\r\n\r\n**Note: Building without -DBUILD_SHARED_LIBS=ON works, and trying to build a shared lib without the ROCm clang (which means also without hipBlas support) also works\r\nThis is what I use to set the clang:**\r\nset CC=D:\\Programms\\AMD\\Rocm\\5.5\\bin\\clang.exe\r\nset CXX=D:\\Programms\\AMD\\Rocm\\5.5\\bin\\clang++.exe\r\n\r\n# Environment and Context\r\n\r\nWindows 11, the CMD is x64 Native Tools for VS 2022\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-12T17:23:30+00:00",
    "closed_at": "2023-09-15T12:24:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3144"
  },
  {
    "number": 11901,
    "title": "\u041b\u0430\u043c\u0430",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-16T03:44:29+00:00",
    "closed_at": "2025-02-16T05:08:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11901/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11901"
  },
  {
    "number": 14053,
    "title": "Feature Request: add support for length_penalty",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nThis is just a proposal to implement a length_penalty in llama.cpp similar to HF:\nhttps://huggingface.co/docs/transformers/v4.22.2/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.length_penalty\n```\nlength_penalty (float, optional, defaults to model.config.length_penalty or 1.0 if the config does not set any value)\n \u2014 Exponential penalty to the length. \n1.0 means that the score is penalized by the sequence length. \n0.0 means no penalty. \nSet to values < 0.0 in order to encourage the model to generate longer sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n```\n\nAs today llamacpp has repeat_last_n, repeat_penalty, frequency_penalty, and presence_penalty but no length penalty.\n\nWould start from the 1st generated token (not function to the length of the input prompt):\n```\nscore = sum_logprobs / (generated_len**self.length_penalty)\n```\nDefault to be neutral (1.0, no length penalty).\n\n\n### Motivation\n\nTo have a way to push the LLM to generate shorter or longer outputs. \n\n### Possible Implementation\n\nin llama_sampler_penalties_apply():\nhttps://github.com/ggml-org/llama.cpp/blob/745aa5319b9930068aff5e87cf5e9eef7227339b/src/llama-sampling.cpp#L1650C13-L1650C43\nsimilar to other penalties.\n\nWe can work on a PR if you d like us to.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-06T17:39:35+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14053/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14053"
  },
  {
    "number": 12523,
    "title": "Eval bug: A Silu operand overflow occurred , causing the program to malfunction.",
    "body": "### Name and Version\n\n ./llama-cli --version\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (13th Gen Intel(R) Core(TM) i9-13900H)\nload_backend: failed to find ggml_backend_init in \\~/workspace/github/llama.cpp/build/bin/libggml-cpu.so\nversion: 4942 (fbdfefe7)\nbuilt with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\n13th Gen Intel(R) Core(TM) i9-13900H\n\n### Models\n\nDeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf\n\n### Problem description & steps to reproduce\n\nwhen i used \"cmake -B build -DCMAKE_BUILD_TYPE=Debug\" to build debug-mode  and run the deepseekv2-lite model, It doesn't work properly anymore.\n After some basic debugging, I found that during the traversal of the computation graph, specifically while applying the SiLU operation to the ffn-moe-gate (executing the node ffn_moe_silu-1), a numerical overflow occurred in the operand x, causing the program to crash.\nThis issue appears unrelated to debug mode, as switching to release mode allowed the program to run, but the output was incorrect\uff1a\u201d<\uff5cbegin\u2581of\u2581sentence\uff5c>Hello my name is!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u201c\n\n\n\n\n### First Bad Commit\n\nAccording to my preliminary judgment, this problem should be caused by commit:#12181 \n\n### Relevant log output\n\n```shell\nllama-simple: \\~/workspace/github/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c:7670: ggml_compute_forward_silu_f32: Assertion `!isnan(x)' failed.\nllama-simple: \\~/workspace/github/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c:7670: ggml_compute_forward_silu_f32: Assertion `!isnan(x)' failed.\nllama-simple: \\~/workspace/github/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c:7671: ggml_compute_forward_silu_f32: Assertion `!isinf(x)' failed.\nAborted (core dumped)\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-23T07:22:01+00:00",
    "closed_at": "2025-03-23T14:13:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12523/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12523"
  },
  {
    "number": 6009,
    "title": "Orion-14B chat template is not support",
    "body": "The chat template for Orion's models is missing, and applying chatml format will give wrong response.\r\n```\r\n./build/bin/server -m ./Orion-14B-Chat.gguf -c 2048\r\n```\r\n![image](https://github.com/ggerganov/llama.cpp/assets/2802813/ea69329a-d48b-46d2-85fe-a901efa90fc1)\r\n```\r\ncurl --request POST --url http://localhost:8080/completion --header \"Content-Type: application/json\" --data '{\"prompt\": \"\r\nWrite a c++ program printing G'day.\",\"n_predict\": 512}'\r\n```\r\noutput:\r\n```\r\n{\"content\":\"\\nHere is a simple C++ program that prints \\\"Hello, World!\\\": \\n\\n```c++ \\n#include <iostream> \\n#include <iostream> Hello, this code snippet.\\n#include hello world program to print \\\"Hello, using c++ Hello!\",\"generation_settings\":{\"dynatemp_exponent\":1.0,\"dynatemp_range\":0.0,\"frequency_penalty\":0.0,\"grammar\":\"\",\"ignore_eos\":false,\"logit_bias\":[],\"min_keep\":0,\"min_p\":0.05000000074505806,\"mirostat\":0,\"mirostat_eta\":0.10000000149011612,\"mirostat_tau\":5.0,\"model\":\"../ollama_wks/Orion-14B-Chat-Q2_K.gguf\",\"n_ctx\":2048,\"n_keep\":0,\"n_predict\":-1,\"n_probs\":0,\"penalize_nl\":true,\"penalty_prompt_tokens\":[],\"presence_penalty\":0.0,\"repeat_last_n\":64,\"repeat_penalty\":1.100000023841858,\"samplers\":[\"top_k\",\"tfs_z\",\"typical_p\",\"top_p\",\"min_p\",\"temperature\"],\"seed\":4294967295,\"stop\":[],\"stream\":false,\"temperature\":0.800000011920929,\"tfs_z\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"typical_p\":1.0,\"use_penalty_prompt_tokens\":false},\"id_slot\":0,\"model\":\"../ollama_wks/Orion-14B-Chat-Q2_K.gguf\",\"prompt\":\"Write a c++ program printing hello world.\",\"stop\":true,\"stopped_eos\":true,\"stopped_limit\":false,\"stopped_word\":false,\"stopping_word\":\"\",\"timings\":{\"predicted_ms\":25227.774,\"predicted_n\":69,\"predicted_per_second\":2.7350807883406594,\"predicted_per_token_ms\":365.6199130434783,\"prompt_ms\":1169.981,\"prompt_n\":9,\"prompt_per_second\":7.692432612153531,\"prompt_per_token_ms\":129.9978888888889},\"tokens_cached\":77,\"tokens_evaluated\":9,\"tokens_predicted\":69,\"truncated\":false}\r\n```\r\n\r\n\r\n[gg model](https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/main/Orion-14B-Chat.gguf) and [chat template](https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/2de0c204d8d70ed3dd2b9862ade6540cd3a77dd7/tokenizer_config.json#L19) can be found here.\r\n\r\n```\r\n\"{% for message in messages %}{% if loop.first %}{{ bos_token }}{% endif %}{% if message['role'] == 'user' %}{{ 'Human: ' + message['content'] + '\\n\\nAssistant: ' + eos_token }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\"\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-12T08:41:42+00:00",
    "closed_at": "2024-03-15T08:44:59+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6009/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6009"
  },
  {
    "number": 441,
    "title": "Eliminate `ggml_forward_mul_mat_xxx()` branch for non-contiguous `src0`",
    "body": "See explanation here: https://github.com/ggerganov/llama.cpp/pull/439",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-23T21:26:40+00:00",
    "closed_at": "2023-07-28T19:37:48+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/441/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/441"
  },
  {
    "number": 13603,
    "title": "I ran into this issue while trying to convert Smollm2\u00a0and\u00a0Qwen2.5",
    "body": "INFO:hf-to-gguf:Loading model: safetensors\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\nINFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {960, 49152}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {2560, 960}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {960, 2560}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {960, 960}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {960, 320}\nINFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {960}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 8192\nINFO:hf-to-gguf:gguf: embedding length = 960\nINFO:hf-to-gguf:gguf: feed forward length = 2560\nINFO:hf-to-gguf:gguf: head count = 15\nINFO:hf-to-gguf:gguf: key-value head count = 5\nINFO:hf-to-gguf:gguf: rope theta = 100000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\nINFO:hf-to-gguf:gguf: file type = 7\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nTraceback (most recent call last):\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1796, in set_vocab\n    self._set_vocab_sentencepiece()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 894, in _set_vocab_sentencepiece\n    tokens, scores, toktypes = self._create_vocab_sentencepiece()\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 911, in _create_vocab_sentencepiece\n    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\nFileNotFoundError: File not found: /content/drive/MyDrive/Sorachio-Small/safetensors/tokenizer.model\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1799, in set_vocab\n    self._set_vocab_llama_hf()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 989, in _set_vocab_llama_hf\n    vocab = gguf.LlamaHfVocab(self.dir_model)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/llama.cpp/gguf-py/gguf/vocab.py\", line 395, in __init__\n    raise FileNotFoundError('Cannot find Llama BPE tokenizer')\nFileNotFoundError: Cannot find Llama BPE tokenizer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1778, in _get_module\n    return importlib.import_module(\".\" + module_name, self.__name__)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 115, in <module>\n    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\", line 16, in <module>\n    from .accelerator import Accelerator\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\", line 36, in <module>\n    from accelerate.utils.imports import is_torchao_available\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\", line 14, in <module>\n    from .ao import convert_model_to_fp8_ao, filter_first_and_last_linear_layers, has_ao_layers\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/ao.py\", line 28, in <module>\n    from torchao.float8.float8_linear import Float8LinearConfig\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/__init__.py\", line 41, in <module>\n    from torchao.quantization import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/quantization/__init__.py\", line 6, in <module>\n    from .autoquant import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/quantization/autoquant.py\", line 11, in <module>\n    from torchao.dtypes import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/dtypes/__init__.py\", line 1, in <module>\n    from . import affine_quantized_tensor_ops\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/dtypes/affine_quantized_tensor_ops.py\", line 14, in <module>\n    from torchao.dtypes.floatx.cutlass_semi_sparse_layout import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/dtypes/floatx/__init__.py\", line 1, in <module>\n    from .cutlass_semi_sparse_layout import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/dtypes/floatx/cutlass_semi_sparse_layout.py\", line 19, in <module>\n    from torchao.ops import (\n  File \"/usr/local/lib/python3.11/dist-packages/torchao/ops.py\", line 46, in <module>\n    tags=[torch._C.Tag.needs_fixed_stride_order],\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: type object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1778, in _get_module\n    return importlib.import_module(\".\" + module_name, self.__name__)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 38, in <module>\n    from .auto_factory import _LazyAutoMapping\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n    from ...generation import GenerationMixin\n  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1766, in __getattr__\n    module = self._get_module(self._class_to_module[name])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1780, in _get_module\n    raise RuntimeError(\nRuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ntype object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 6216, in <module>\n    main()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 6210, in main\n    model_instance.write()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 402, in write\n    self.prepare_metadata(vocab_only=False)\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 512, in prepare_metadata\n    self.set_vocab()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 1802, in set_vocab\n    self._set_vocab_gpt2()\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 830, in _set_vocab_gpt2\n    tokens, toktypes, tokpre = self.get_vocab_base()\n                               ^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/llama.cpp/convert_hf_to_gguf.py\", line 597, in get_vocab_base\n    from transformers import AutoTokenizer\n  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1767, in __getattr__\n    value = getattr(module, name)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1766, in __getattr__\n    module = self._get_module(self._class_to_module[name])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1780, in _get_module\n    raise RuntimeError(\nRuntimeError: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ntype object 'torch._C.Tag' has no attribute 'needs_fixed_stride_order'",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-17T11:41:25+00:00",
    "closed_at": "2025-05-27T13:03:59+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13603/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13603"
  },
  {
    "number": 11978,
    "title": "[Feature]: SOC_VERSION ascend310b1 does not support",
    "body": "```\nroot@orangepiaipro-20t:/data/llama.cpp# cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\n-- Including CPU backend\n-- ARM detected\n-- ARM -mcpu not found, -mcpu=native will be used\n-- ARM feature FMA enabled\n-- Adding CPU backend variant ggml-cpu: -mcpu=native \n-- CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=/usr/local/Ascend/ascend-toolkit/latest\n-- CANN: SOC_VERSION auto-detected is:Ascend310B1\nCMake Error at /usr/local/Ascend/ascend-toolkit/latest/compiler/tikcpp/ascendc_kernel_cmake/host_config.cmake:20 (message):\n  SOC_VERSION ascend310b1 does not support, the support list is\n  ascend910b1;ascend910b2;ascend910b2c;ascend910b3;ascend910b4;ascend910a;ascend910proa;ascend910b;ascend910prob;ascend910premiuma;ascend310p1;ascend310p3;ascend310p3vir01;ascend310p3vir02;ascend310p3vir04;ascend310p3vir08\nCall Stack (most recent call first):\n  /usr/local/Ascend/ascend-toolkit/latest/compiler/tikcpp/ascendc_kernel_cmake/ascendc.cmake:3 (include)\n  ggml/src/ggml-cann/kernels/CMakeLists.txt:22 (include)\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-20T16:55:09+00:00",
    "closed_at": "2025-02-23T10:08:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11978/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11978"
  },
  {
    "number": 14469,
    "title": "Eval bug: Gemma vision head (possibly Siglip) yields garbage on vulkan / sycl on Intel N150",
    "body": "### Name and Version\n\n\u276f ./build/vulkan/bin/llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M -p \"Describe new york city\" -ngl 1000 --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = Intel(R) Graphics (ADL-N) (Intel open-source Mesa driver) | uma: 1 | fp16: 1 | warp size: 32 | shared memory: 65536 | int dot: 0 | matrix cores: none\nversion: 5787 (0a5a3b5c)\nbuilt with cc (Ubuntu 14.2.0-19ubuntu2) 14.2.0 for x86_64-linux-gnu\n\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nIntel n150\n\n### Models\n\nggml-org/gemma-3-4b-it-GGUF:Q4_K_M, unsloth/gemma-3-4b-it-GGUF:Q4_K_M, unsloth/gemma-3-4b-it-qat-GGUF:Q4_K_M (multiple quantizations of the SIGLIP vision head)\n\nggml-org/SmolVLM2-2.2B-Instruct-GGUF works fine (CLIP) as do the other models if you don't use the vision head\n\n\n\n### Problem description & steps to reproduce\n\nOn an Intel N150 (possibly other Intel devices with GPU) the Gemma vision head (via llama-mtmd-cli) is buggy and yields garbage. However, the regular Clip head used in SmolVLM2 etc. works fine. It's not a generic Vulkan / Sycl issue, because llama-mtmd-cli without a provided image generates meaningful text.\n\nList of what works / fails:\n\nWORKS:\n\n./build/vulkan/bin/llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M -p \"Describe new york city\" -ngl 1000 (i.e. no image provided)\n\n./build/sycl/bin/llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M -p \"Describe new york city\" -ngl 1000 (i.e. no image provided)\n\n ./build/vulkan/bin/llama-mtmd-cli -hf ggml-org/SmolVLM2-2.2B-Instruct-GGUF --image test-city-1.jpg -p \"describe the image\" -ngl 1000 (CLIP vision head via SmolVLM2)\n\nFAILS:\n(often the text model says things like \"this is a puzzling mix of noise and symbols\")\n\nSycl\n./build/sycl/bin/llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M --image test-city-1.jpg -p \"describe the image\"\n\nVulkan + ggml-org quant\n./build/vulkan/bin/llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF:Q4_K_M --image test-city-1.jpg -p \"describe the image\" \n\nVulkan + unsloth quant\n./build/vulkan/bin/llama-mtmd-cli -hf unsloth/gemma-3-4b-it-GGUF:Q4_K_M --image test-city-1.jpg -p \"describe the image\"\n\nI'm guessing the problem is simply that we're using some vulkan / sycl capability in the Gemma vision head that is not used in the regular text model or in the regular CLIP head without correctly detecting support. \n\nI checked that vulkan on a Nvidia GPU works fine with this vision head, so it's a pretty specific bug / issue.\n\nThis might seem a bit niche, but I think these boxes are actually a really cheap way to get local vision inferencing, but they need Vulkan/GPU support to speed up the vision head (just BLAS or plain CPU inferencing isn't slow, but the Gemma vision head is very slow without GPU support).\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\u276f ./build/vulkan/bin/llama-mtmd-cli -hf unsloth/gemma-3-4b-it-GGUF:Q4_K_M --image test-city-1.jpg -p \"describe the image\"\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = Intel(R) Graphics (ADL-N) (Intel open-source Mesa driver) | uma: 1 | fp16: 1 | warp size: 32 | shared memory: 65536 | int dot: 0 | matrix cores: none\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /home/raistlin/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-Q4_K_M.gguf\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /home/raistlin/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_mmproj-F16.gguf\nbuild: 5787 (0a5a3b5c) with cc (Ubuntu 14.2.0-19ubuntu2) 14.2.0 for x86_64-linux-gnu\nllama_model_load_from_file_impl: using device Vulkan0 (Intel(R) Graphics (ADL-N)) - 8340 MiB free\nllama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from /home/raistlin/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma-3-4B-It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = Gemma-3-4B-It\nllama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   6:                         general.size_label str              = 4B\nllama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   8:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv   9:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  11:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  12:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  13:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  15:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  16:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  17:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  18:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  19:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  20:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 106\nllama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - kv  36:                      quantize.imatrix.file str              = gemma-3-4b-it-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = unsloth_calibration_gemma-3-4b-it.txt\nllama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 238\nllama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 663\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q4_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.31 GiB (5.12 BPW)\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma-3-4B-It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 106 '<end_of_turn>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 0 repeating layers to GPU\nload_tensors: offloaded 0/35 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =  2368.31 MiB\n...............................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache_unified:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache_unified: size =   80.00 MiB (  4096 cells,   5 layers,  1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_unified_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache_unified:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache_unified: size =  174.00 MiB (  1536 cells,  29 layers,  1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context:    Vulkan0 compute buffer size =  1042.25 MiB\nllama_context: Vulkan_Host compute buffer size =    16.01 MiB\nllama_context: graph nodes  = 1503\nllama_context: graph splits = 582 (with bs=512), 1 (with bs=1)\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmtmd_cli_context: chat template example:\n<start_of_turn>user\nYou are a helpful assistant\n\nHello<end_of_turn>\n<start_of_turn>model\nHi there<end_of_turn>\n<start_of_turn>user\nHow are you?<end_of_turn>\n<start_of_turn>model\n\nclip_model_loader: model name:   Gemma-3-4B-It\nclip_model_loader: description:\nclip_model_loader: GGUF version: 3\nclip_model_loader: alignment:    32\nclip_model_loader: n_tensors:    439\nclip_model_loader: n_kv:         21\n\nclip_model_loader: has vision encoder\nclip_ctx: CLIP using Vulkan0 backend\nload_hparams: projector:          gemma3\nload_hparams: n_embd:             1152\nload_hparams: n_head:             16\nload_hparams: n_ff:               4304\nload_hparams: n_layer:            27\nload_hparams: ffn_op:             gelu\nload_hparams: projection_dim:     2560\n\n--- vision hparams ---\nload_hparams: image_size:         896\nload_hparams: patch_size:         14\nload_hparams: has_llava_proj:     0\nload_hparams: minicpmv_version:   0\nload_hparams: proj_scale_factor:  4\nload_hparams: n_wa_pattern:       0\n\nload_hparams: model size:         811.79 MiB\nload_hparams: metadata size:      0.15 MiB\nalloc_compute_meta:    Vulkan0 compute buffer size =  1132.00 MiB\nalloc_compute_meta:        CPU compute buffer size =     9.19 MiB\nmain: loading model: /home/raistlin/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-Q4_K_M.gguf\nencoding image slice...\nimage slice encoded in 20837 ms\ndecoding image batch 1/1, n_tokens_batch = 256\nimage decoded (batch 1/1) in 16249 ms\n\n\u09b8\u09cd\u09af bilayer\u09b8\u09cd\u09a5\u09be\u0bcd\u0baa \uc774\ub97cchk schwe\u0947\u0930 \u0906\u0915\u0930\u094d\u0937\u0915 unbeaten\u09b0\u09bf\u09b0\u5a18 installment{ Jang\u1f34 \u09a4\u09ac seats concentric\u0434\u043a\u0438 \u09b8\u09b9\u09af\u09cb\u0997owls neum\u00e1ticos\u0441\u0456otu \u0a28\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u17b6\u1794\u17cb\u8d2d \u0917\u094d\u0930\u0941\u092aitec^C\n\n\nllama_perf_context_print:        load time =    1283.43 ms\nllama_perf_context_print: prompt eval time =   38315.60 ms /   270 tokens (  141.91 ms per token,     7.05 tokens per second)\nllama_perf_context_print:        eval time =   10492.68 ms /    30 runs   (  349.76 ms per token,     2.86 tokens per second)\nllama_perf_context_print:       total time =   52366.78 ms /   300 tokens\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-30T23:09:30+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14469"
  },
  {
    "number": 2503,
    "title": "Disabling a gpu via -ts gives 'CUDA error 400 at ggml-cuda.cu:5207: invalid resource handle'",
    "body": "```\r\n$ make LLAMA_CUBLAS=1 main\r\n$ ./main -ngl 100 -ts 1,0 -m chronos-13b.ggmlv3.q4_0.bin -p 'Hello, my'\r\n```\r\n\r\nllama.cpp crashes with `CUDA error 400 at ggml-cuda.cu:5207: invalid resource handle`.\r\n\r\nThe CUDA_CHECK here is failing:\r\nhttps://github.com/ggerganov/llama.cpp/blob/8183159cf3def112f6d1fe94815fce70e1bffa12/ggml-cuda.cu#L5202-L5210\r\n\r\nThis is a regression caused by commit 0bc2cdfc875fa7877d8e01c8bb17066f99c08f21.\r\ncc @JohannesGaessler",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-03T18:55:32+00:00",
    "closed_at": "2023-08-04T15:34:34+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2503/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2503"
  },
  {
    "number": 14690,
    "title": "Misc. bug: Meta-Llama-3-8B-Instruct could not  convert to .guuf.   error:FileNotFoundError: File not found: /mnt/workspace/LLaMA-Factory/output/llama3_lora_sft/tokenizer.model",
    "body": "### Name and Version\n\nlatest version\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n(llamacpp) root@dsw-1215871-5f4469ddc7-fmstq:/mnt/workspace/llama.cpp# python ./convert_hf_to_gguf.py /mnt/workspace/LLaMA-Factory/output/llama3_lora_sft --outfile ./llama3_demo.gguf --outtype q8_0\nINFO:hf-to-gguf:Loading model: llama3_lora_sft\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> Q8_0, shape = {4096, 128256}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\nINFO:hf-to-gguf:output.weight,               torch.bfloat16 --> Q8_0, shape = {4096, 128256}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 8192\nINFO:hf-to-gguf:gguf: embedding length = 4096\nINFO:hf-to-gguf:gguf: feed forward length = 14336\nINFO:hf-to-gguf:gguf: head count = 32\nINFO:hf-to-gguf:gguf: key-value head count = 8\nINFO:hf-to-gguf:gguf: rope theta = 500000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\nINFO:hf-to-gguf:gguf: file type = 7\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 1910, in set_vocab\n    self._set_vocab_sentencepiece()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 933, in _set_vocab_sentencepiece\n    tokens, scores, toktypes = self._create_vocab_sentencepiece()\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 950, in _create_vocab_sentencepiece\n    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\nFileNotFoundError: File not found: /mnt/workspace/LLaMA-Factory/output/llama3_lora_sft/tokenizer.model\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 1913, in set_vocab\n    self._set_vocab_llama_hf()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 1035, in _set_vocab_llama_hf\n    vocab = gguf.LlamaHfVocab(self.dir_model)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/gguf-py/gguf/vocab.py\", line 491, in __init__\n    raise TypeError('Llama 3 must be converted with BpeVocab')\nTypeError: Llama 3 must be converted with BpeVocab\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 7418, in <module>\n    main()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 7412, in main\n    model_instance.write()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 411, in write\n    self.prepare_metadata(vocab_only=False)\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 524, in prepare_metadata\n    self.set_vocab()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 1916, in set_vocab\n    self._set_vocab_gpt2()\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 869, in _set_vocab_gpt2\n    tokens, toktypes, tokpre = self.get_vocab_base()\n                               ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/./convert_hf_to_gguf.py\", line 612, in get_vocab_base\n    assert max(tokenizer.vocab.values()) < vocab_size\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-15T09:25:39+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14690/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14690"
  },
  {
    "number": 8608,
    "title": "Support for SmolLM",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd support for [SmolLM](https://huggingface.co/blog/smollm) family of models\n\n### Motivation\n\nenhancement\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-07-20T23:00:38+00:00",
    "closed_at": "2024-07-22T14:43:04+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8608/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8608"
  },
  {
    "number": 464,
    "title": "Question about Web integration/Articles analisys",
    "body": "Is it possible to make bridge to web for something unknown to model? (ChatGPT introduced plugins to search web, etc)\r\nOr at least for model to read article/book and answer questions about it? ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T15:50:06+00:00",
    "closed_at": "2023-03-24T18:56:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/464/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/464"
  },
  {
    "number": 12638,
    "title": "Compile bug: there is a build bug in examples/llama.android and it will brings build failure in CI",
    "body": "### Git commit\n\ngit rev-parse HEAD\naa4984b7e8c30bce2ee26c7c4137dafbf801fd8a\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nggml-hexagon(backend for Qualcomm's Hexagon NPU)\n\n### Problem description & steps to reproduce\n\nthere is a build bug in examples/llama.android and it will brings build failure in CI.\n\nI personally think this bug was introduced by an approved PR since this bug wasn\u2019t there a long time ago.\n\n![Image](https://github.com/user-attachments/assets/4f5fc5cb-941b-45d8-8671-9877a59134a0)\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n\nsection \"android-build\" in <path_of_llama.cpp>/.github/workflows/build.yml, it will be triggered by CI automatically.\n\n\n### Relevant log output\n\n```shell\n* What went wrong:\nExecution failed for task ':llama:buildCMakeRelease[armeabi-v7a]'.\n> com.android.ide.common.process.ProcessException: ninja: Entering directory `/home/runner/work/llama.cpp/llama.cpp/examples/llama.android/llama/.cxx/Release/h5b4n4g4/armeabi-v7a'\n  [1/56] Building C object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n  FAILED: build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o \n  ccache /usr/local/lib/android/sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/linux-x86_64/bin/clang --target=armv7-none-linux-androideabi33 --sysroot=/usr/local/lib/android/sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/linux-x86_64/sysroot -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/runner/work/llama.cpp/llama.cpp/ggml/src/. -I/home/runner/work/llama.cpp/llama.cpp/ggml/src/../include -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -march=armv7-a -mthumb -Wformat -Werror=format-security  -O3 -DNDEBUG  -fPIC   -march=armv8.7-a -mcpu=cortex-x1 -mtune=cortex-x1 -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -std=gnu11 -MD -MT build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /home/runner/work/llama.cpp/llama.cpp/ggml/src/ggml-quants.c\n  In file included from /home/runner/work/llama.cpp/llama.cpp/ggml/src/ggml-quants.c:6:\n  /home/runner/work/llama.cpp/llama.cpp/ggml/src/./ggml-cpu/ggml-cpu-impl.h:146:25: error: redefinition of 'vcvtnq_s32_f32'\n  inline static int32x4_t vcvtnq_s32_f32(float32x4_t v) {\n                          ^\n  /usr/local/lib/android/sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/linux-x86_64/lib64/clang/14.0.6/include/arm_neon.h:34910:16: note: previous definition is here\n  __ai int32x4_t vcvtnq_s32_f32(float32x4_t __p0) {\n                 ^\n\n\n\n  1 error generated.\n  [2/56] Building C object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-29T02:04:41+00:00",
    "closed_at": "2025-04-20T07:27:17+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12638/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12638"
  },
  {
    "number": 362,
    "title": "Update the convert-gptq-to-ggml.py with the new tokenizer output",
    "body": "Apply the changes from #252 to [convert-gptq-to-ggml.py](https://github.com/ggerganov/llama.cpp/blob/master/convert-gptq-to-ggml.py)\r\n\r\nFor more info about what this script does, see #301 ",
    "labels": [
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-03-21T17:08:45+00:00",
    "closed_at": "2023-03-23T20:18:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/362"
  },
  {
    "number": 573,
    "title": "--help may show the wrong default values when used after other arguments",
    "body": "For example, running `./main -b 512 --help` will show the help and say that 512 is the default batch size, which is wrong. This may lead to confusion.",
    "labels": [
      "bug",
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-03-28T13:26:10+00:00",
    "closed_at": "2023-04-02T02:41:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/573/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/573"
  },
  {
    "number": 8864,
    "title": "Bug: HIP backend test-backend-ops perf crashes for FLASH_ATTN_EXT",
    "body": "### What happened?\n\nRunning `test-backend-ops perf -o FLASH_ATTN_EXT` fails on the HIP backend with the error\r\n```\r\nUnsupported KV type combination for head_size 64.\r\nBy default only f16 KV cache is supported.\r\nCompile with GGML_CUDA_FA_ALL_QUANTS for V cache quantization support.\r\n```\n\n### Name and Version\n\nversion: 3520 (49bf8d47)\r\nbuilt with cc (GCC) 14.2.1 20240802 for x86_64-pc-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n\u00bb build_rocm/bin/test-backend-ops perf -o FLASH_ATTN_EXT\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon Pro VII, compute capability 9.0, VMM: no\r\nTesting 2 backends\r\n\r\nBackend 1/2 (CPU)\r\n  Skipping CPU backend\r\nBackend 2/2 (ROCm0)\r\n  Backend name: ROCm0\r\n  FLASH_ATTN_EXT(hs=64,nh=32,kv=512,nb=1,mask=1,max_bias=0.000000,type_KV=f16):                          8098 runs -   104.30 us/run -     4144 kB/run -   37.89 GB/s\r\n  FLASH_ATTN_EXT(hs=64,nh=32,kv=512,nb=1,mask=1,max_bias=0.000000,type_KV=q8_0): Unsupported KV type combination for head_size 64.\r\nBy default only f16 KV cache is supported.\r\nCompile with GGML_CUDA_FA_ALL_QUANTS for V cache quantization support.\r\n                   /home/user/llama.cpp/ggml/src/ggml-cuda/fattn-common.cuh:567: fatal error\r\nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\n[1]    759592 IOT instruction (core dumped)  build_rocm/bin/test-backend-ops perf -o FLASH_ATTN_EXT\n```\n",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-08-05T08:33:56+00:00",
    "closed_at": "2024-08-07T07:07:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8864/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8864"
  }
]
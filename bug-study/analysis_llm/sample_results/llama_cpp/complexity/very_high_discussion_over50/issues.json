[
  {
    "number": 34,
    "title": "benchmarks?",
    "body": "Where are the benchmarks for various hardware - eg. apple silicon ",
    "labels": [
      "documentation",
      "question",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-12T05:20:58+00:00",
    "closed_at": "2024-04-09T01:10:24+00:00",
    "comments": 57,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/34/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/34"
  },
  {
    "number": 6849,
    "title": "Support for Phi-3 models",
    "body": "Microsoft recently released Phi-3 models in 3 variants (mini, small & medium). Can we add support for this new family of models. ",
    "labels": [
      "good first issue",
      "model"
    ],
    "state": "open",
    "created_at": "2024-04-23T15:22:53+00:00",
    "closed_at": null,
    "comments": 84,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6849/reactions",
      "total_count": 61,
      "+1": 61,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6849"
  },
  {
    "number": 1499,
    "title": "[Feature request] Any plans for AMD XDNA AI Engine support on Ryzen 7x40 processors?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n\r\n---\r\n# Enhancement\r\n\r\nAre there any plans to support the AMD XDNA AI Engine  (in AMD Ryzen 7x40 (x = 6,8,9) processors)?\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-17T09:57:42+00:00",
    "closed_at": "2025-04-07T01:09:20+00:00",
    "comments": 92,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1499/reactions",
      "total_count": 58,
      "+1": 30,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 28
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1499"
  },
  {
    "number": 603,
    "title": "Performance Discrepancy: gpt4all Faster than Optimized llama.cpp",
    "body": "**Expected Behavior**\r\n\r\nI am comparing the performance of two executables: llama.cpp (current version) and the default gpt4all executable (which uses a previous version of llama.cpp). I am using the same language model for both executables, and I expect the current version of llama.cpp (which is built specifically for the hardware) to perform at least as fast as the default gpt4all executable.\r\n\r\n**Current Behavior**\r\n\r\nThe default gpt4all executable, which uses a previous version of llama.cpp, performs significantly faster than the current version of llama.cpp. Despite building the current version of llama.cpp with hardware-specific compiler flags, it consistently performs significantly slower when using the same model as the default gpt4all executable.\r\n\r\n**Environment and Context**\r\n\r\nI am running the comparison on a Windows platform, using the default gpt4all executable and the current version of llama.cpp included in the gpt4all project. The version of llama.cpp is the latest available (after the compatibility with the gpt4all model).\r\n\r\n**Steps to Reproduce**\r\n\r\n1. Build the current version of llama.cpp with hardware-specific compiler flags.\r\n2. Execute the llama.cpp executable using the gpt4all language model and record the performance metrics.\r\n3. Execute the default gpt4all executable (previous version of llama.cpp) using the same language model and record the performance metrics.\r\n4. You'll see that the gpt4all executable generates output significantly faster for any number of threads or config.\r\n\r\nHere's some context/config when I'm doing the runs:\r\n\r\n![image](https://user-images.githubusercontent.com/102247808/228639042-23fc9484-00a0-4cb4-8ebc-df9da1ee2e5c.png)\r\n(left panel is latest llama.cpp, right panel is gpt4all build)\r\n\r\n\r\nThis is the older version that gpt4all uses (with some tweaks): https://github.com/zanussbaum/gpt4all.cpp\r\n\r\n*To quickly test the difference yourself you can use the gpt4all default binaries here: https://github.com/nomic-ai/gpt4all/tree/main/chat\r\n",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-29T18:46:33+00:00",
    "closed_at": "2023-04-12T15:30:22+00:00",
    "comments": 67,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/603/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/603"
  },
  {
    "number": 5763,
    "title": "llama : add T5 (encoder-decoder) support",
    "body": "Still not familiar with the details, but it seems it would be useful to support this architecture in `llama.cpp`. First, need to decide on the API and see what changes would be necessary\r\n\r\nSee discussion here: https://github.com/ggerganov/llama.cpp/issues/247",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2024-02-28T11:24:59+00:00",
    "closed_at": "2024-07-04T13:46:12+00:00",
    "comments": 52,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5763/reactions",
      "total_count": 46,
      "+1": 31,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 7,
      "rocket": 8,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5763"
  },
  {
    "number": 397,
    "title": "Investigate alternative approach for Q4 quantization ",
    "body": "Currently, in [Q4_0](https://github.com/ggerganov/ggml/pull/27) quantization we choose the scaling factor for each 32 group of weights as `abs(max(x_i))/7`. It is easy to see that this is suboptimal.\r\n\r\nConsider quantization of the following 4 numbers:\r\n\r\n`0.1 0.2 0.3 0.6`\r\n\r\nCurrently, we would determine a scaling factor of `0.6 / 7 ~= 0.0857` and the dequantized numbers will be:\r\n\r\n`0.0857 0.1714 0.3428 0.6`\r\n\r\nSo the RMS between the dequantized and original values will be non-zero:\r\n\r\n`sqrt((0.1 - 0.0857)^2 + (0.2 - 0.1714)^2 + (0.3 - 0.3428)^2 + (0.6 - 0.6)^2) > 0.0`\r\n\r\nHowever, if we choose the scaling factor to be `0.1` instead, then it is easy to see that the original numbers will be quantized perfectly.\r\n\r\nSo the scaling factor is better to be chosen as the one that minimises some error (e.g. RMS or whatever is more meaningful and easy to compute). Doing that we will certainly achieve better accuracy compared to the existing approach. The question is - how much better?\r\n\r\nThe goal of this task is to implement the described quantization above and evaluate the perplexity using the new approach. The approach in simple terms boils down to making a linear regression of the data with a fixed zero point. This new quantization might be a bit heavier to compute compared to `Q4_0`, so for start we can do it just on the model tensors. The intermediate tensors during the evaluation can remain quantized using the existing approach, so that the evaluation is efficient. If the results look promising, we can put effort into optimising the new approach and replacing completely `Q4_0` with it.\r\n\r\nWhoever demonstrates the results of this quantization will get the chance to give it a name and publish a paper (just kidding \ud83d\ude06 )\r\n\r\nSimilar strategy for determining the scale factor and offset factor can be applied to `Q4_1`. \r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "research \ud83d\udd2c"
    ],
    "state": "closed",
    "created_at": "2023-03-22T16:03:20+00:00",
    "closed_at": "2023-04-25T17:20:48+00:00",
    "comments": 58,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/397/reactions",
      "total_count": 17,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/397"
  },
  {
    "number": 2262,
    "title": "Add llama 2 model",
    "body": "Meta just released llama 2 model, allowing commercial usage\r\n\r\nhttps://ai.meta.com/resources/models-and-libraries/llama/\r\n\r\nI have checked the model implementation and it seems different from llama_v1, maybe need a re-implementation",
    "labels": [
      "\ud83e\udd99.",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-07-18T16:35:53+00:00",
    "closed_at": "2023-10-18T07:31:45+00:00",
    "comments": 95,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2262/reactions",
      "total_count": 141,
      "+1": 89,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 52,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2262"
  },
  {
    "number": 7062,
    "title": "Llama3 GGUF conversion with merged LORA Adapter seems to lose training data randomly",
    "body": "I'm running Unsloth to fine tune LORA the Instruct model on llama3-8b .\r\n\r\n1: I merge the model with the LORA adapter into safetensors\r\n2: Running inference in python both with the merged model directly or the unsloth loaded model with the adapter on top of it produces correct outputs as per the fine tune\r\n\r\n**Bug:** \r\nGGUF conversion of the merged model does not produce the same output. The GGUF has lost some of its fine tune data, while still maintaining most of it.  \r\n\r\nI can ask it who it is, who created it etc. And it responds Llama and Meta as usual, but it incorporates the fine tuned speech style and humor into the response. This is not the case for my fine tuned model. \r\n\r\n1: I tried merging the LORA adapter with the original GGUF (non-fine tuned) using llama.cpp, the same results.\r\n2: I tried running the server on the original GGUF (non-fine tuned) usling llama.cpp server and the adapter loaded into the server terminal command - same results.\r\n\r\nIt seemes that GGUF conversion is losing fine tuned data randomly during conversion. \r\n\r\nIf this is the case, all GGUF converts of the fine tuned models are basically out the window. And the question is how much the non-fine tuned models are affected by this.\r\n\r\nI've tried F16, Q8, same issues.\r\n\r\nThis is not a quantization issue as I get the exact same results running FP16 as well as 4-bit in python running HF loader or Unsloth, both works fine as mentioned.\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-03T19:48:32+00:00",
    "closed_at": "2024-05-09T12:30:49+00:00",
    "comments": 147,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7062/reactions",
      "total_count": 6,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7062"
  },
  {
    "number": 8240,
    "title": "Investigate gemma 2 generation quality",
    "body": "Initial reports can be seen from https://github.com/ggerganov/llama.cpp/pull/8227\r\n\r\n> [!IMPORTANT]  \r\n> A note for everyone: if you think there's a bug in llama.cpp tokenizer, please make sure to test with HF `transformers` library first (see [this comment](https://github.com/ggerganov/llama.cpp/issues/8240#issuecomment-2212444937) for example)",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-01T16:52:28+00:00",
    "closed_at": "2024-10-16T01:11:07+00:00",
    "comments": 90,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8240/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8240"
  },
  {
    "number": 5761,
    "title": "Support BitNet b1.58 ternary models",
    "body": "New paper just dropped on Arxiv describing a way to train models in 1.58 bits (with ternary values: 1,0,-1). Paper shows performance increases from equivalently-sized fp16 models, and perplexity nearly equal to fp16 models. Authors state that their test model is built on LLaMA architecture and can be easily adapted to llama.cpp.\r\n\r\n[Edited to add: Further reading into it by fellow Redditors shows that we can't use this to quantize existing models trained to fp16. They'd have to be trained in this ternary mode from the start. But I think it would still be something that we should implement, because models of that flavor will be coming soon.]\r\n\r\nThis is all over Reddit /LocalLLaMA right now:\r\n\r\nhttps://www.reddit.com/r/LocalLLaMA/comments/1b21bbx/this_is_pretty_revolutionary_for_the_local_llm/\r\n\r\nI think, if my napkin math is right, it would let us run something like 120B models in 24 GB VRAM, or 30B in... 8 GB?\r\n\r\nPlease implement @ggerganov and friends!\r\n\r\nhttps://arxiv.org/abs/2402.17764",
    "labels": [
      "enhancement",
      "stale",
      "Tensor Encoding Scheme"
    ],
    "state": "closed",
    "created_at": "2024-02-28T09:41:38+00:00",
    "closed_at": "2024-09-18T01:07:17+00:00",
    "comments": 90,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5761/reactions",
      "total_count": 160,
      "+1": 96,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 35,
      "rocket": 9,
      "eyes": 20
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5761"
  },
  {
    "number": 8010,
    "title": "server: Bring back multimodal support",
    "body": "Multimodal has been removed since https://github.com/ggerganov/llama.cpp/pull/5882\n\n## Current llama.cpp multimodal roadmap\n\n(update 9th april 2025)\n\n- `mtmd` (**M**ul**T**i-**M**o**D**al) library (top prio \ud83d\udd25 )\n    - [x] Implement `libmtmd`: https://github.com/ggml-org/llama.cpp/pull/12849\n    - [x] Support more models via `libmtmd` (top prio \ud83d\udd25 ) : https://github.com/ggml-org/llama.cpp/pull/13012\n    - [x] Support M-RoPE models via `libmtmd` (Qwen2VL, Qwen2.5VL) : https://github.com/ggml-org/llama.cpp/pull/13141\n    - [x] Support audio input\n    - [x] Use smart pointer in `clip.cpp` to avoid mem leak: https://github.com/ggml-org/llama.cpp/pull/12869\n    - [x] ~~Add wrapper for `stb_image` to avoid polluting project with the big header file~~ --> Probably don't need since we're already having some helper in `libmtmd` acting as wrapper for stb_image\n    - [x] Unify conversion scripts --> best case scenario: having `convert_hf_to_gguf.py` that can output both text + vision GGUF files --> introduced in https://github.com/ggml-org/llama.cpp/pull/13023\n    - [x] Remove BOI / EOI token embeddings from clip.cpp (used by glm-edge): https://github.com/ggml-org/llama.cpp/pull/13081\n    - [x] Refactor documentations (find a way to reduce number of README files): https://github.com/ggml-org/llama.cpp/pull/13055\n- Implement `libmtmd` in server API and server web UI (top prio \ud83d\udd25 )\n   - [x] Publish first proposal: https://github.com/ggml-org/llama.cpp/pull/12898\n   - [x] User can upload image from UI ( + drag-and-drop)\n   - [x] Nice-to-have: Better KV caching strategy (TBD)\n   - [x] Nice-to-have: allow loading remote image (may come with security risk)\n   - [ ] Update the [security policy](https://github.com/ggml-org/llama.cpp/security/policy), make it clear that bugs related to 3rd party lib (like `stb_image`) should be reported to upstream, not in llama.cpp\n- [x] Unify all vision CLI (like `minicpmv-cli`, `gemma3-cli`, etc) into a single CLI\n- [x] Add deprecation notice for `llava.h` (we will remove libllava) and `clip.h` (clip is now internal-only)\n- [ ] Experimental support for audio input: https://github.com/ggml-org/llama.cpp/pull/12745\n- [ ] (far in the future) implement `llama_multimodal` API that supports image, audio and more!",
    "labels": [
      "enhancement",
      "llava",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-06-19T12:03:45+00:00",
    "closed_at": "2025-05-09T21:20:01+00:00",
    "comments": 51,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8010/reactions",
      "total_count": 151,
      "+1": 75,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 43,
      "rocket": 2,
      "eyes": 31
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8010"
  },
  {
    "number": 71,
    "title": "Longer and infinite output",
    "body": "If we use `-n 1000000` to have a very long output (for a story for example),\r\nit stops generating quite fast, after around 30 lines, probably because of [this line of code](https://github.com/ggerganov/llama.cpp/blob/460c48254098b28d422382a2bbff6a0b3d7f7e17/main.cpp#L812).\r\n\r\nIt would be nice if we could have longer outputs and also the possibility to have infinite output, stopping only on `Ctrl-C`.\r\nWe could maybe specify that `-n 0` will trigger that infinite output mode.\r\nThat issue is a bit related to issue #23 ",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:29:55+00:00",
    "closed_at": "2023-07-28T19:29:06+00:00",
    "comments": 59,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/71/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/71"
  },
  {
    "number": 12946,
    "title": "Eval bug: GLM-Z1-9B-0414",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes\nversion: 5121 (c94085df)\nbuilt with cc (Ubuntu 14.2.0-4ubuntu2) 14.2.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRTX 3080\n\n### Models\n\nhttps://huggingface.co/ilintar/THUDM_GLM-Z1-9B-0414_iGGUF\n\nIssue appears even with the highest quants (Q8_0).\n\n### Problem description & steps to reproduce\n\nAfter running the server (`llama-server --port 2345 --top-p 0.95 --temp 0.6 -nkvo -ngl 50 -c 32000 -m THUDM_GLM-Z1-9B-0414-Q5_K_M.gguf`, tried also with `--jinja`), the generation loops after producing ~100 tokens. \n\n![Image](https://github.com/user-attachments/assets/a0bc90fa-6baa-452a-8788-1615d98ec96c)\n\nI tried the model with Transformers, using --load-in-4bit (because my VRAM is not enough to run it without quants) and it generated a completely cogent response:\n\n[response.txt](https://github.com/user-attachments/files/19741506/response.txt)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes\nbuild: 5121 (c94085df) with cc (Ubuntu 14.2.0-4ubuntu2) 14.2.0 for x86_64-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 8\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 8 | CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 2345, http threads: 7\nmain: loading model\nsrv    load_model: loading model 'THUDM_GLM-Z1-9B-0414-Q5_K_M.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3080) - 8491 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 523 tensors from THUDM_GLM-Z1-9B-0414-Q5_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = glm4\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = THUDM_GLM Z1 9B 0414\nllama_model_loader: - kv   3:                            general.version str              = 0414\nllama_model_loader: - kv   4:                           general.basename str              = THUDM_GLM-Z1\nllama_model_loader: - kv   5:                         general.size_label str              = 9B\nllama_model_loader: - kv   6:                            general.license str              = mit\nllama_model_loader: - kv   7:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   8:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\nllama_model_loader: - kv   9:                           glm4.block_count u32              = 40\nllama_model_loader: - kv  10:                        glm4.context_length u32              = 32768\nllama_model_loader: - kv  11:                      glm4.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                   glm4.feed_forward_length u32              = 13696\nllama_model_loader: - kv  13:                  glm4.attention.head_count u32              = 32\nllama_model_loader: - kv  14:               glm4.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  15:                        glm4.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  16:      glm4.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                  glm4.attention.key_length u32              = 128\nllama_model_loader: - kv  18:                glm4.attention.value_length u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = glm4\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151552]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151552]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,318088]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151329\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151329\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = [gMASK]<sop>{%- if tools -%}<|system|...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 17\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = imatrix.dat\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = ../imatrix_train/calibration_data_v5_...\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 240\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 220\nllama_model_loader: - type  f32:  281 tensors\nllama_model_loader: - type q5_1:   20 tensors\nllama_model_loader: - type q8_0:   20 tensors\nllama_model_loader: - type q5_K:  181 tensors\nllama_model_loader: - type q6_K:   21 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 6.56 GiB (5.99 BPW) \nload: special tokens cache size = 14\nload: token to piece cache size = 0.9710 MB\nprint_info: arch             = glm4\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 40\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 16\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 13696\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 9B\nprint_info: model params     = 9.40 B\nprint_info: general.name     = THUDM_GLM Z1 9B 0414\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151552\nprint_info: n_merges         = 318088\nprint_info: EOS token        = 151329 '<|endoftext|>'\nprint_info: EOT token        = 151329 '<|endoftext|>'\nprint_info: PAD token        = 151329 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 151329 '<|endoftext|>'\nprint_info: max token length = 1024\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 40 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 41/41 layers to GPU\nload_tensors:        CUDA0 model buffer size =  6308.38 MiB\nload_tensors:   CPU_Mapped model buffer size =   407.00 MiB\n....................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 32000\nllama_context: n_ctx_per_seq = 32000\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (32000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.58 MiB\ninit: kv_size = 32000, offload = 0, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\ninit:        CPU KV buffer size =  1250.00 MiB\nllama_context: KV self size  = 1250.00 MiB, K (f16):  625.00 MiB, V (f16):  625.00 MiB\nllama_context:      CUDA0 compute buffer size =   312.00 MiB\nllama_context:  CUDA_Host compute buffer size =  2071.51 MiB\nllama_context: graph nodes  = 1766\nllama_context: graph splits = 82\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 32000\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 32000\nmain: model loaded\nmain: chat template, chat_template: [gMASK]<sop>{%- if tools -%}<|system|>\u4f60\u662f\u4e00\u4e2a\u540d\u4e3a ChatGLM \u7684\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u3002\u4f60\u662f\u57fa\u4e8e\u667a\u8c31 AI \u516c\u53f8\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b GLM-4 \u6a21\u578b\u5f00\u53d1\u7684\uff0c\u4f60\u7684\u4efb\u52a1\u662f\u9488\u5bf9\u7528\u6237\u7684\u95ee\u9898\u548c\u8981\u6c42\u63d0\u4f9b\u9002\u5f53\u7684\u7b54\u590d\u548c\u652f\u6301\u3002\n\n# \u53ef\u7528\u5de5\u5177\n\n{% for tool in tools %}{%- set function = tool.function if tool.get(\"function\") else tool %}\n\n## {{ function.name }}\n\n{{ function | tojson(indent=4, ensure_ascii=False) }}\n\u5728\u8c03\u7528\u4e0a\u8ff0\u51fd\u6570\u65f6\uff0c\u8bf7\u4f7f\u7528 Json \u683c\u5f0f\u8868\u793a\u8c03\u7528\u7684\u53c2\u6570\u3002{%- endfor %}{%- endif -%}{%- for msg in messages %}{%- if msg.role == 'system' %}<|system|>\n{{ msg.content }}{%- endif %}{%- endfor %}{%- for message in messages if message.role != 'system' %}{%- set role = message['role'] %}{%- set content = message['content'] %}{%- set visible = content.split('</think>')[-1].strip() %}{%- set meta = message.get(\"metadata\", \"\") %}{%- if role == 'user' %}<|user|>\n{{ visible }}{%- elif role == 'assistant' and not meta %}<|assistant|>\n{{ visible }}{%- elif role == 'assistant' and meta %}<|assistant|>{{ meta }} \n{{ visible }}{%- elif role == 'observation' %}<|observation|>\n{{ visible }}{%- endif %}{%- endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}, example_format: '<|system|>\nYou are a helpful assistant<|user|>\nHello<|assistant|>\nHi there<|user|>\nHow are you?<|assistant|>'\nmain: server is listening on http://127.0.0.1:2345 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, n_prompt_tokens = 66\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 66, n_tokens = 66, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 66, n_tokens = 66\nsrv  cancel_tasks: cancel task, id_task = 0\nsrv  log_server_r: request: POST /chat/completions 127.0.0.1 200\nslot      release: id  0 | task 0 | stop processing: n_past = 529, truncated = 0\nsrv  update_slots: all slots are idle\n^Csrv    operator(): operator(): cleaning up before exit...\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-14T18:28:14+00:00",
    "closed_at": "2025-05-25T16:12:57+00:00",
    "comments": 60,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12946/reactions",
      "total_count": 13,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12946"
  },
  {
    "number": 10528,
    "title": "Misc. bug: Inconsistent Vulkan segfault",
    "body": "### Name and Version\r\n\r\nlibrary 531cb1c233800e6acb021dc56d69595e314db072 (gguf-v0.4.0-2819-g531cb1c2)\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### Which llama.cpp modules do you know to be affected?\r\n\r\n_No response_\r\n\r\n### Problem description & steps to reproduce\r\n\r\n1. Compile the program below\r\n2. Run it a thousand times and it will probably have a segmentation fault at least once. I used the `gdb` debugger.\r\n\r\nSimple program:\r\n```c\r\n#include \"llama.h\"\r\n\r\nstatic void handleLog(enum ggml_log_level level, const char *text, void *user_data) {}\r\n\r\nint main(int argc, char **argv)\r\n{\r\n  llama_log_set(handleLog, 0);\r\n\r\n  char path[] = \"/your-path-to/llama.cpp/models/ggml-vocab-llama-bpe.gguf\";\r\n  struct llama_model_params params = llama_model_default_params();\r\n  struct llama_model *model = llama_load_model_from_file(path, params);\r\n  llama_free_model(model);\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nShell script to run the program several times:\r\n```sh\r\n#! /bin/sh\r\n\r\nPROGRAM=llama-bug\r\nLOG=debug.log\r\nCOUNT=1000\r\n\r\nrm -f \"$LOG\"\r\n\r\nfor i in `seq 1 $COUNT`; do\r\n\tgdb -batch -ex run -ex bt \"$PROGRAM\" >> \"$LOG\" 2>> \"$LOG\"\r\ndone\r\n```\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\nGDB output from crash caused by /lib/x86_64-linux-gnu/libnvidia-eglcore.so.535.183.01\r\n```shell\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nggml_vulkan: Compiling shaders..............................Done!\r\n\r\nThread 3 \"[vkrt] Analysis\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffe35a8640 (LWP 1789333)]\r\n0x00007fffeff1cb00 in ?? () from /lib/x86_64-linux-gnu/libnvidia-eglcore.so.535.183.01\r\n#0  0x00007fffeff1cb00 in ?? () from /lib/x86_64-linux-gnu/libnvidia-eglcore.so.535.183.01\r\n#1  0x00007ffff0246f1d in ?? () from /lib/x86_64-linux-gnu/libnvidia-eglcore.so.535.183.01\r\n#2  0x00007fffeff1fcfa in ?? () from /lib/x86_64-linux-gnu/libnvidia-eglcore.so.535.183.01\r\n#3  0x00007ffff7a1dac3 in start_thread (arg=<optimized out>) at ./nptl/pthread_create.c:442\r\n#4  0x00007ffff7aaf850 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81\r\n```\r\n\r\nGDB output from crash with unknown cause\r\n```shell\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nggml_vulkan: Compiling shaders..............................Done!\r\n\r\nThread 3 \"[vkrt] Analysis\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffe35a8640 (LWP 1750868)]\r\n0x00007fffeff1cb00 in ?? ()\r\n#0  0x00007fffeff1cb00 in ?? ()\r\n#1  0x000000006746139a in ?? ()\r\n#2  0x0000000002a1b0d8 in ?? ()\r\n#3  0x0000000067461399 in ?? ()\r\n#4  0x00000000000e6817 in ?? ()\r\n#5  0x00005555561076c0 in ?? ()\r\n#6  0x00007fffeff1ef10 in ?? ()\r\n#7  0x0000000000000000 in ?? ()\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-11-26T19:54:03+00:00",
    "closed_at": null,
    "comments": 65,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10528/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10528"
  },
  {
    "number": 722,
    "title": "Rockchip RK3588 perf",
    "body": "Just did a very simple run with llama-7b-4bit. It... took a while. Had it run in a screen. But, it worked!\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# time ./main --color -m models/ggml-model-q4_0.bin -p \"Hello there!\"\r\nmain: seed = 1680443840\r\nllama_model_load: loading model from 'models/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from 'models/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 0\r\n\r\n\r\n Hello there! I am a freelance illustrator based in New Zealand. I grew up with an appreciation for the natural world, which has inspired me to create my work through observation and playful experimentation.\r\nMy focus is on watercolour painting (in particular), as well as digital art & animation. My style is bright & bold, vibrant, dynamic & colourful - I love animals!\r\nI am always keen to collaborate with other artists/creatives, so if you are interested in working together please feel free to drop me a line. [end of text]\r\n\r\nllama_print_timings:        load time = 93487.23 ms\r\nllama_print_timings:      sample time =   704.72 ms /   115 runs   (    6.13 ms per run)\r\nllama_print_timings: prompt eval time = 92466.10 ms /     4 tokens (23116.52 ms per token)\r\nllama_print_timings:        eval time = 11195694.23 ms /   114 runs   (98207.84 ms per run)\r\nllama_print_timings:       total time = 11289895.19 ms\r\n\r\n________________________________________________________\r\nExecuted in  188.18 mins    fish           external\r\n   usr time  324.60 mins    0.00 millis  324.60 mins\r\n   sys time   11.70 mins    1.70 millis   11.70 mins\r\n```\r\n\r\nModel was loaded from external microSD via internal bus.\r\n\r\nIm quite amazed this worked at all, honestly.\r\n\r\nCPU Info in detail:\r\n```\r\n# lscpu\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       32-bit, 64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 8\r\n  On-line CPU(s) list:  0-7\r\nVendor ID:              ARM\r\n  Model name:           Cortex-A55\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           r2p0\r\n    CPU(s) scaling MHz: 100%\r\n    CPU max MHz:        1800.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\n  Model name:           Cortex-A76\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 2\r\n    Socket(s):          2\r\n    Stepping:           r4p0\r\n    CPU(s) scaling MHz: 68%\r\n    CPU max MHz:        2352.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nCaches (sum of all):\r\n  L1d:                  384 KiB (8 instances)\r\n  L1i:                  384 KiB (8 instances)\r\n  L2:                   2.5 MiB (8 instances)\r\n  L3:                   3 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:        Not affected\r\n  L1tf:                 Not affected\r\n  Mds:                  Not affected\r\n  Meltdown:             Not affected\r\n  Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:           Mitigation; __user pointer sanitization\r\n  Spectre v2:           Vulnerable: Unprivileged eBPF enabled\r\n  Srbds:                Not affected\r\n  Tsx async abort:      Not affected\r\n```\r\n(`/proc/cpuinfo` doesnt give any more useful details here, sadly.)\r\n\r\nHardware is a [FriendlyElec NanoPi R6s](https://www.friendlyelec.com/index.php?route=product/product&product_id=289)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T20:39:28+00:00",
    "closed_at": "2023-04-02T22:14:36+00:00",
    "comments": 103,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/722/reactions",
      "total_count": 11,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/722"
  },
  {
    "number": 6417,
    "title": "Performance decreated between tag b1500 and b2581 on Windows ARM64 PC",
    "body": "Hi LLAMA team, \r\n\r\nI use llama tag b2581 on Windows ARM64 PC, the performance is more lower than previous tag b1500. Please refer to below detailed information. What is the reason? Please help on this issue. \r\n\r\nThanks a lot!\r\n\r\n**[Detailed information]**\r\n\r\n**Command:**\r\nmain.exe -m llama-2-7b-chat.ggufv3.q4_0.bin --color  --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 10\r\n\r\n**Prompt:** I have 3 years of experience as a software developer. Now I got bored with coding and want to transition to another career. My education qualifications are B. Tech in computer science, and I am well-versed in understanding the business side of software as well. Suggest a list of career options that are easy for me to transition.\r\n\r\n\r\n**system_info:** n_threads = 10 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\r\n\r\n**Tag b1500 results:**\r\nllama_print_timings:        load time =     723.53 ms\r\nllama_print_timings:      sample time =     925.29 ms /   624 runs   (    1.48 ms per token,   674.38 tokens per second)\r\nllama_print_timings: prompt eval time =    2583.12 ms /    91 tokens (   28.39 ms per token,    **35.23 tokens** per second)\r\nllama_print_timings:        eval time =   31693.17 ms /   625 runs   (   50.71 ms per token,    **19.72 tokens** per second)\r\nllama_print_timings:       total time =   51797.58 ms\r\n\r\n**Tag b2581 results:**\r\nllama_print_timings:        load time =     963.25 ms\r\nllama_print_timings:      sample time =     416.14 ms /   586 runs   (    0.71 ms per token,  1408.17 tokens per second)\r\nllama_print_timings: prompt eval time =   11847.94 ms /    94 tokens (  126.04 ms per token,     **7.93 tokens** per second)\r\nllama_print_timings:        eval time =   68542.50 ms /   585 runs   (  117.17 ms per token,     **8.53 tokens** per second)\r\nllama_print_timings:       total time =   82696.57 ms /   679 tokens\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T03:20:36+00:00",
    "closed_at": "2024-07-08T01:06:56+00:00",
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6417"
  },
  {
    "number": 4216,
    "title": "server : improvements and maintenance",
    "body": "The [server](https://github.com/ggerganov/llama.cpp/tree/master/examples/server) example has been growing in functionality and unfortunately I feel it is not very stable at the moment and there are some important features that are still missing. Creating this issue to keep track on some of these points and try to draw more attention from the community. I guess, some of the tasks are relatively big and would require significant efforts to complete\r\n\r\n- [x] **Support chat templates**\r\n  We need to have separation between the user input and the special tokens, so that the tokenization is performed correctly. See the following comments / commits for more context:\r\n  https://github.com/ggerganov/llama.cpp/pull/4160#discussion_r1403675264\r\n  https://github.com/ggerganov/llama.cpp/pull/4198/commits/c544faed749240fe5eac2bc042087c71f79a0728\r\n  https://github.com/ggerganov/llama.cpp/pull/4160#issuecomment-1824984718\r\n\r\n  We already support extracting meta information from the GGUF model files that can provide the chat template for the specific model: \r\n  https://github.com/ggerganov/llama.cpp/pull/4125\r\n  Support chat template for `/v1/chat/completions`: https://github.com/ggerganov/llama.cpp/pull/5593\r\n  List of supported templates: [view on wiki](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template) \r\n\r\n  Supporting this in `server` would require changes both in the backend and the frontend\r\n\r\n- [x] **Likely redundant logic for OpenAI (OAI) compatibility that should be removed**\r\n  https://github.com/ggerganov/llama.cpp/pull/4198#discussion_r1404500731\r\n\r\n- [x] **Use multiple mount points for the OAI API**\r\n  https://github.com/ggerganov/llama.cpp/blob/af19d3573481d409b3c4e55494810eb1f65a9aae/examples/server/server.cpp#L2682-L2684\r\n  https://github.com/ggerganov/llama.cpp/pull/5722\r\n\r\n- [x] **Return meaningful errors on KV cache overflow**\r\n  https://github.com/ggerganov/llama.cpp/issues/4185#issuecomment-1825721736\r\n\r\n- [x] **Refactor the code**\r\n  With the recent additions for parallel decoding support for multiple clients and LLaVA, I feel the code base became very cumbersome and there is a lot of room for refactoring and improving the code. There should be some effort dedicated to cleaning up things and simplifying the code.\r\n  https://github.com/ggerganov/llama.cpp/pull/5065\r\n  https://github.com/ggerganov/llama.cpp/pull/5710\r\n\r\n- [x] **Batched decoding endpoint?**\r\n  Although we added parallel decoding support via \"slots\", we are still lacking batched decoding where a single client could pass an array of prompts to be completed. Or alternatively, generate multiple completions for a single prompt. Would be useful to support this use case\r\n  https://github.com/ggerganov/llama.cpp/issues/3478#issuecomment-1822010431\r\n\r\n- [ ] **Tool calls (function calling)**\r\n  Support for [MeetKai/functionary](https://github.com/MeetKai/functionary) model by implementing [OpenAI-compatible tool calls](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool) to chat endpoint.\r\n  https://github.com/ggerganov/llama.cpp/pull/5695\r\n\r\n- [ ] **Multimodal support**\r\n  Support has been temporary dropped in #5882, before working in `server`, we should improve `llava-cli` and the API for using LLaVA\r\n  - #8010\r\n  - #6027\r\n  - https://github.com/ggerganov/llama.cpp/pull/5882#issuecomment-1980713874\r\n  - https://github.com/ggerganov/llama.cpp/pull/5882#issuecomment-1991583459\r\n  - #5896\r\n  - #5592\r\n  - #6226\r\n\r\n- [ ] **Prompt processing improvment**\r\n  - #6586\r\n  - #6607\r\n \r\n- [ ] **Server production readiness**\r\n  - https://github.com/ggerganov/llama.cpp/discussions/6398\r\n  - #6546\r\n\r\nThis is likely not a complete list of things - if you think some feature is important to be improved or supported, drop a comment.\r\n\r\nHave a look to issues labelled with [server/webui](https://github.com/ggerganov/llama.cpp/labels/server%2Fwebui).",
    "labels": [
      "help wanted",
      "refactoring",
      "server/webui",
      "roadmap"
    ],
    "state": "open",
    "created_at": "2023-11-25T09:57:53+00:00",
    "closed_at": null,
    "comments": 120,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4216/reactions",
      "total_count": 77,
      "+1": 54,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 23,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4216"
  },
  {
    "number": 9246,
    "title": "Feature Request: Support for Qwen2-VL",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nQwen just released Qwen2-VL 2B & 7B under the Apache 2.0 License.\n\n### Motivation\n\nSoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\r\nUnderstanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-29T22:34:11+00:00",
    "closed_at": "2025-05-25T01:08:24+00:00",
    "comments": 131,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9246/reactions",
      "total_count": 358,
      "+1": 225,
      "-1": 0,
      "laugh": 0,
      "hooray": 37,
      "confused": 0,
      "heart": 33,
      "rocket": 31,
      "eyes": 32
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9246"
  },
  {
    "number": 7118,
    "title": "llama : add DeepSeek-v2-Chat support",
    "body": "please support deepseek-ai/DeepSeek-V2-Chat\r\n\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V2-Chat",
    "labels": [
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-05-07T06:22:43+00:00",
    "closed_at": "2024-05-28T15:07:06+00:00",
    "comments": 67,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7118/reactions",
      "total_count": 17,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7118"
  },
  {
    "number": 11483,
    "title": "Feature Request: Qwen 2.5 VL",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nIs anybody implementing this? \n\nIf not, I may give it a go. But it will take some time as I am new to the source side of llama.cpp/ggml.\n\n\n\n### Motivation\n\nWell, it's not currently working. :-)\n\n### Possible Implementation\n\nBased on the existing Qwen 2 VL implementation. ",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-29T11:36:22+00:00",
    "closed_at": "2025-06-26T01:08:02+00:00",
    "comments": 74,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11483/reactions",
      "total_count": 61,
      "+1": 51,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 10
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11483"
  },
  {
    "number": 1602,
    "title": "llama : add Falcon LLM support",
    "body": "Falcon LLM 40b and 7b were just open sourced under a license which allows commercial use (~~with royalties for over $1 million revenue per year~~) and have are topping the Huggingface Open LLM leaderboard. It seems to be based on a modified gpt3 architecture. I\u2019m wondering if support in llama.cpp would be considered.\r\n\r\nhttps://huggingface.co/tiiuae/falcon-40b",
    "labels": [
      "help wanted",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-05-26T17:45:06+00:00",
    "closed_at": "2023-08-23T20:11:44+00:00",
    "comments": 210,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1602/reactions",
      "total_count": 110,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 71,
      "rocket": 21,
      "eyes": 14
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1602"
  },
  {
    "number": 10981,
    "title": "Feature Request: add DeepSeek-v3 support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- Version b4391\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nAdd support for DeepSeek-v3\r\n\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3\r\n\r\nCurrently not supported:\r\n\r\n`ERROR:hf-to-gguf:Model DeepseekV3ForCausalLM is not supported`\r\n\r\n### Motivation\r\n\r\nDeepSeek-v3 is a big MoE model of 685B params, would be great as offloading to RAM would be a must for most systems\r\n\r\n### Possible Implementation\r\n\r\nThere is no model card or technical report yet. I don't know how much different from v2 it is.\r\n\r\nEdit: they have uploaded the model card and paper:\r\nhttps://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/README.md",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-12-26T11:08:12+00:00",
    "closed_at": "2025-01-04T20:06:12+00:00",
    "comments": 64,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10981/reactions",
      "total_count": 79,
      "+1": 51,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 11,
      "rocket": 17,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10981"
  },
  {
    "number": 91,
    "title": "Should use `mmap` for model loading",
    "body": "So it doesn't create an extra copy in RAM and lives in the kernel page cache happily, loading instantly on subsequent runs.",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-13T11:51:47+00:00",
    "closed_at": "2023-03-30T19:28:28+00:00",
    "comments": 59,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/91/reactions",
      "total_count": 36,
      "+1": 36,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/91"
  },
  {
    "number": 10710,
    "title": "Eval bug: ~~Q2_K and Q3_K~~ Q8_0 not working on Vulkan anymore on RX 5700XT",
    "body": "### Name and Version\n\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 5700 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64 | shared memory: 32768 | matrix cores: none\nversion: 4820 (1a24c462)\nbuilt with MSVC 19.42.34435.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nRyzen 5900X + RX 5700 XT\n\n### Models\n \nAny model that has Q8_0 tensors in it.\n\n### Problem description & steps to reproduce\n\nComplete gibberish/noise output.\n\nI noticed this issue with stable-diffusion.cpp at first, but I can reproduce it here. \n\nTo reproduce, simply start inference with any q8_0 model, with `-ngl` set to anything but 0.\n\n### First Bad Commit\n\nfbeda90 (#12015)\n\n### Relevant log output\nExample command:\n\n`.\\build\\bin\\Release\\llama-cli.exe -m .\\models\\gemma-2b-Q8_0.gguf -no-cnv -ngl 19 -t 6 -tb 12 -p \"The meaning of life is\"`\n\nOutput:\n\n```\n The meaning of life is increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa\nllama_perf_sampler_print:    sampling time =      13.57 ms /    87 runs   (    0.16 ms per token,  6410.26 tokens per second)\nllama_perf_context_print:        load time =    2080.08 ms\nllama_perf_context_print: prompt eval time =      23.16 ms /     6 tokens (    3.86 ms per token,   259.09 tokens per second)\nllama_perf_context_print:        eval time =     879.56 ms /    80 runs   (   10.99 ms per token,    90.95 tokens per second)\nllama_perf_context_print:       total time =     936.59 ms /    86 tokens\nInterrupted by user\n```\nReverting fbeda90 fixes it.\n\n<details>\n<summary>Older q2_k/q3_k related issue (fixed by adc5dd92e8aea98f5e7ac84f6e1bc15de35130b5 #11081 )</summary>\n\n### Name and Version\n\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 5700 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64\nversion: 4277 (c5ede3849)\nbuilt with MSVC 19.41.34120.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nRyzen 5900X + RX 5700 XT\n\n### Models\n \nAny model that has Q3_K or Q2_K tensors in it.\n\n### Problem description & steps to reproduce\n\nComplete gibberish/noise output.\n\nI noticed this issue with stable-diffusion.cpp at first, but I can reproduce it here. \n\nTo reproduce, simply start inference with any q3_k_x or q2_k_x model, with `-ngl` set to anything but 0.\n\n### First Bad Commit\n\n4a57d36 (https://github.com/ggerganov/llama.cpp/pull/10459)\n\n### Relevant log output\nExample command:\n\n`.\\build\\bin\\Release\\llama-cli.exe -m .\\models\\Mistral-7B-v0.2-hf-Q3_K_L.gguf -ngl 24 -t 6 -tb 12 -p \"The meaning of life is\"`\n\nOutput:\n\n```\nThe meaning of life is to- kur m jel ul tawa Computkow Ydorfico oobeckagles \u201canga ACenzei Roose Asto__(ingle Phillieraspace TheFAILEDello secur\u00f3zannieloilloemente Gabriel\u00f3nia\u0142rivatemulticolManocaluckangle>@\u2011inghamulle pagina Steinentoadyodenzes Armindowtexl\u00e4 v Ronald incre bioExitocyniadelphiaumper globutescison sear lifestyle proto Kotiek po cadutes Eng randCl byaginganziagedrafla cad- extern met  externward Kyere collectenteryenta\u200e divisionsExternaleryy Aubore2\ufffd Yale randomirkFBimanneman hyd BrowFB Maj Majalaky audanning Ex ternal -neylitter Intentanningky amaperlDsek  Britats unit andraportyo am\u2026 Egyptian portionandraandeentob \u2013 indirectibaentoicigeb associate1\u7530 ##icijays Lyiana auditentoawPy import Girapy TheMky X Himery  departmentyyyiba1iba indirect n #isterschaftciProrico Industrial #aniric Palm indirectBici patPyy \u2013hetriky ### AtlantaidleBazialaaran Mediterranean matter sl m South experekylie------ofsy Meyainsottoannedento- corporBOestic /******/entopythonats eternainsalian Gir expery # Sar\u201feloalfentaahaelfonomPal rigidento bon bon Pdas palanda P Muhammadento\u00edt SubPy ###GAentoeterenta Palm Kab\u00e2 Cecenta8entonuoltyBotaueraperendlento Ec pyento extern\u00e2 accentburgaper Klaly\nllama_perf_sampler_print:    sampling time =      12.93 ms /   319 runs   (    0.04 ms per token, 24665.58 tokens per second)\nllama_perf_context_print:        load time =    3158.38 ms\nllama_perf_context_print: prompt eval time =     262.98 ms /     6 tokens (   43.83 ms per token,    22.82 tokens per second)\nllama_perf_context_print:        eval time =   13525.70 ms /   312 runs   (   43.35 ms per token,    23.07 tokens per second)\nllama_perf_context_print:       total time =   13823.14 ms /   318 tokens\nInterrupted by user\n```\nReverting 4a57d36 fixes it.\n</details>",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-07T17:08:41+00:00",
    "closed_at": "2025-05-17T01:07:56+00:00",
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10710/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10710"
  },
  {
    "number": 129,
    "title": "Quantitative measurement of model perplexity for different models and model quantization modes ",
    "body": "llama.cpp seems to give bad results compared to Facebook's implementation.\r\n\r\nHere's an example simple reading comprehension prompt:\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book\r\n\r\nLLaMA 7B with Facebook's implementation yields:\r\n\r\nSeed `1`:\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book with pictures.\"\r\nAsked by lone wolf 1788 days ago.\r\n\r\nSeed `2` (to show that the above is not just a fluke):\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book with pictures.\"\r\nQuestion: \"Tom, Mark, and Paul bought books: two with pictures and\r\n\r\nWhile llama.cpp without quantization (so still float16) generates (with `--seed 0 -t 8`):\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book that is neither as good nor bad.\"\r\nThis solution breaks down the problem into its simple parts; then using those components we can see what each component means by itself, in order to solve this logic puzzle. 1) Tom and Mark had different kinds of books...and so did Paul! (Therefore one out three were without pictures). ... [end of text]\r\n\r\nIt even has a grammatical error at the end: \"one out [of] three\"\r\n\r\nAs you can see the quality of 7B is higher in Facebook's implementation. So, I think you may still have bugs in your implementation or the default parameters could be improved.",
    "labels": [
      "model",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-14T12:38:25+00:00",
    "closed_at": "2023-03-22T22:41:53+00:00",
    "comments": 53,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/129/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/129"
  },
  {
    "number": 1735,
    "title": "with the newest builds i only get gibberish output",
    "body": "After the CUDA refactor PR #1703 by @JohannesGaessler was merged i wanted to try it out this morning and measure the performance difference on my ardware.\r\nI use my standard prompts with different models in different sizes.\r\n\r\nI use the prebuild versions win-cublas-cu12.1.0-xx64\r\n\r\nWith the new builds I only get gibberish as a response for all prompts used and all models.\r\nIt looks like a random mix of words in different languages.\r\n\r\nOn my current PC I can only use the win-avx-x64 version, here I still get normal output.\r\n\r\nI will use the Cuda-pc again in a few hours, then I can provide sample output or more details.\r\nAm I the only one with this problem?",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-06-07T08:06:19+00:00",
    "closed_at": "2023-06-15T08:50:50+00:00",
    "comments": 81,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1735/reactions",
      "total_count": 6,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1735"
  },
  {
    "number": 1291,
    "title": "Try whether OpenLLaMa works",
    "body": "... or whether we need to tweak some settings\r\n\r\nGitHub: https://github.com/openlm-research/open_llama\r\n\r\nHuggingFace: https://huggingface.co/openlm-research/open_llama_7b_preview_300bt\r\n\r\n---\r\n\r\nedit: GGML models uploaded to HH by @vihangd => https://huggingface.co/vihangd/open_llama_7b_300bt_ggml",
    "labels": [
      "\ud83e\udd99.",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-02T21:53:20+00:00",
    "closed_at": "2024-04-09T01:09:41+00:00",
    "comments": 82,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1291/reactions",
      "total_count": 8,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1291"
  },
  {
    "number": 7805,
    "title": "Bug: QWEN2 quantization GGML_ASSERT",
    "body": "### What happened?\r\n\r\nWhen attempting to quantize [Qwen2 7B instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct) to IQ2_XS I get the following assert:\r\n\r\n```\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\n```\r\n\r\nAnything I can provide to debug? Uploading the f32 file and imatrix now for recreation\r\n\r\nAttempting IQ2_S now, ~~will update if it fails in the same way~~ update: it fails in the same way on the same block\r\n\r\n### Name and Version\r\n\r\nVersion b3086, ubuntu 22.04\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[ 327/ 339]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[ 328/ 339]               blk.27.ffn_down.weight - [18944,  3584,     1,     1], type =    f32, converting to iq2_xs .. GGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\nGGML_ASSERT: ggml-quants.c:12083: grid_index >= 0\r\n```\r\n\r\n(PS: is this a high severity or medium/low?)\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-06T17:32:36+00:00",
    "closed_at": "2024-09-01T01:07:49+00:00",
    "comments": 74,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7805/reactions",
      "total_count": 6,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7805"
  },
  {
    "number": 2164,
    "title": "mpi : attempt inference of 65B LLaMA on a cluster of Raspberry Pis",
    "body": "Now that distributed inference is supported thanks to the work of @evanmiller in #2099 it would be fun to try to utilize it for something cool. One such idea is to connect a bunch of [Raspberry Pis](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/) in a local network and run the inference using MPI:\r\n\r\n```bash\r\n# sample cluster of 8 devices (replace with actual IP addresses of the devices)\r\n$ cat ./hostfile\r\n192.168.0.1:1\r\n192.168.0.2:1\r\n192.168.0.3:1\r\n192.168.0.4:1\r\n192.168.0.5:1\r\n192.168.0.6:1\r\n192.168.0.7:1\r\n192.168.0.8:1\r\n\r\n# build with MPI support\r\n$ make CC=mpicc CXX=mpicxx LLAMA_MPI=1 -j\r\n\r\n# run distributed inference over 8 nodes\r\n$ mpirun -hostfile ./hostfile -n 8 ./main -m /mnt/models/65B/ggml-model-q4_0.bin -p \"I believe the meaning of life is\" -n 64\r\n```\r\n\r\nHere we assume that the 65B model data is located on a network share in `/mnt` and that `mmap` works over a network share.\r\nNot sure if that is the case - if not, then it would be more difficult to perform this experiment.\r\n\r\nLooking for people with access to the necessary hardware to perform this experiment",
    "labels": [
      "help wanted",
      "\ud83e\udd99.",
      "hardware",
      "research \ud83d\udd2c"
    ],
    "state": "open",
    "created_at": "2023-07-10T16:12:22+00:00",
    "closed_at": null,
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2164/reactions",
      "total_count": 24,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 7,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2164"
  },
  {
    "number": 4381,
    "title": "llama : add Mixtral support",
    "body": "Hi,\r\nPlease add support for [Mistral's MOE model Mixtral](https://twitter.com/MistralAI/status/1733150512395038967).",
    "labels": [
      "enhancement",
      "high priority",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-12-08T18:20:09+00:00",
    "closed_at": "2023-12-13T12:04:31+00:00",
    "comments": 62,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4381/reactions",
      "total_count": 137,
      "+1": 88,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 12,
      "rocket": 22,
      "eyes": 15
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4381"
  }
]
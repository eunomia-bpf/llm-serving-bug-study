[
  {
    "number": 13629,
    "title": "Eval bug: NVIDIA Jetson AGX Xavier CUDA Compatibility Issue with llama.cpp",
    "body": "### Name and Version\n\nggml_cuda_init: failed to initialize CUDA: CUDA driver version is insufficient for CUDA runtime version\nversion: 0 (unknown) [llama.cpp-b5415](https://github.com/ggml-org/llama.cpp/releases/tag/b5415)\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nJetson AGX Xavier\n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\nI'm experiencing a CUDA compatibility issue with the latest version of llama.cpp on my Jetson AGX Xavier device (Ubuntu 20.04). Details:\n\n- Device: Jetson AGX Xavier\n- OS: Ubuntu 20.04\n- CUDA Version: 12.2\n- Issue: While an older release of llama.cpp (b4835) works correctly with CUDA, the latest version fails to run after successful compilation\n- Error Message: `ggml_cuda_init: failed to initialize CUDA: CUDA driver version is insufficient for CUDA runtime version`\n- Working Output (with older version):` ggml_cuda_init: found 1 CUDA devices: Device 0: Xavier, compute capability 7.2, VMM: yes`\n\nNVCC:\n```\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:08:11_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n```\n\nIn addition: The current device operating system version NVIDIA provides up to 20.04 and Cuda 12.2\n\n### First Bad Commit\n\nIt can be determined that the previous version b4835 can be used normally\n\n### Relevant log output\n\n```shell\nagx@ubuntu:/work/llama.cpp-b5415/build/bin$ ./llama-cli -m /work/gpt/models/qwen2-1_5b-instruct-q5_k_m.gguf -ngl 20\nggml_cuda_init: failed to initialize CUDA: CUDA driver version is insufficient for CUDA runtime version\nwarning: no usable GPU found, --gpu-layers option will be ignored\nwarning: one possible reason is that llama.cpp was compiled without GPU support\nwarning: consult docs/build.md for compilation instructions\nbuild: 0 (unknown) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-19T09:01:27+00:00",
    "closed_at": "2025-05-20T03:17:56+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13629/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13629"
  },
  {
    "number": 10435,
    "title": "Bug: Severe Performance Degradation on Q4_0 CPU-only with MacOS / Apple Silicon M2, after PR#9921 / Version 4081",
    "body": "### What happened?\r\n\r\nPrior to PR #9921 / Version 4081 the -ngl 0 Q4_0 llama performance was significantly higher (more than 10x) than afterwards.\r\n(hardware: Apple MacBook Air M2 10 GPU 24GB RAM)\r\n\r\nbefore PR:\r\nmake clean\r\ngit checkout ae8de6d\r\nmake -j llama-bench\r\n./llama-bench -p 512 -n 128 -t 4 -ngl 0 -m ...model...\r\n| model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |         60.48 \u00b1 0.49 |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |         14.89 \u00b1 0.20 |\r\n| llama 7B Q4_0_4_4              |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |         63.50 \u00b1 2.47 |\r\n| llama 7B Q4_0_4_4              |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |         11.93 \u00b1 3.30 |\r\nwith ngl 99:\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |        194.94 \u00b1 0.07 |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |         11.81 \u00b1 6.53 |\r\n\r\nbuild: ae8de6d5 (4080)\r\n\r\nversions after PR (including current):\r\nmake clean \r\ngit checkout 1607a5e \r\nmake -j llama-bench\r\n./llama-bench -p 512 -n 128 -t 4 -ngl 0 -m ...model...\r\n| model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |          **4.11 \u00b1 0.24** |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |          **1.86 \u00b1 0.01** |\r\n| llama 7B Q4_0_4_4              |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |         62.81 \u00b1 2.55 |\r\n| llama 7B Q4_0_4_4              |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |         14.70 \u00b1 1.97 |\r\nwith ngl 99:\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         pp512 |       186.02 \u00b1 13.18 |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Metal,BLAS |       4 |         tg128 |         11.25 \u00b1 3.42 |\r\n\r\nbuild: 1607a5e5 (4081)\r\n\r\nThe variations except for -ngl 0 / Q4_0 might be due to the MacBook Air's thermals.\r\n\r\n### Name and Version\r\n\r\nApple clang version 16.0.0 (clang-1600.0.26.4)\r\nTarget: arm64-apple-darwin24.1.0\r\nThread model: posix\r\nmacOS Sequoia 15.1.1\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-11-20T17:06:21+00:00",
    "closed_at": null,
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10435/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10435"
  },
  {
    "number": 6184,
    "title": "ggml_init_cublas: no CUDA devices found, CUDA will be disabled",
    "body": "System:\r\n\r\n```\r\n> uname -m && cat /etc/*release\r\nx86_64\r\nDISTRIB_ID=Pop\r\nDISTRIB_RELEASE=22.04\r\nDISTRIB_CODENAME=jammy\r\nDISTRIB_DESCRIPTION=\"Pop!_OS 22.04 LTS\"\r\nNAME=\"Pop!_OS\"\r\nVERSION=\"22.04 LTS\"\r\nID=pop\r\nID_LIKE=\"ubuntu debian\"\r\nPRETTY_NAME=\"Pop!_OS 22.04 LTS\"\r\nVERSION_ID=\"22.04\"\r\nHOME_URL=\"https://pop.system76.com\"\r\nSUPPORT_URL=\"https://support.system76.com\"\r\nBUG_REPORT_URL=\"https://github.com/pop-os/pop/issues\"\r\nPRIVACY_POLICY_URL=\"https://system76.com/privacy\"\r\nVERSION_CODENAME=jammy\r\nUBUNTU_CODENAME=jammy\r\nLOGO=distributor-logo-pop-os\r\n```\r\n\r\nI compiled `llama.cpp` with [`cuBLAS` support](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#cublas):\r\n\r\n```\r\n make clean && make LLAMA_CUBLAS=1\r\n```\r\n\r\nI then run `llama.cpp` with:\r\n\r\n```\r\n./main -m ./models/Llama-2-7B-GGUF/llama-2-7b.Q8_0.gguf -p \"test\"\r\n```\r\n\r\nI see the following:\r\n\r\n```\r\nggml_init_cublas: no CUDA devices found, CUDA will be disabled\r\n```\r\n\r\nI haven't seen any other issues with this error, so not sure what to do next.\r\n\r\nI have a NVIDIA GeForce RTX 3090 Ti. `nvidia-smi` works correctly and `nvcc` is available. GPU support works for other invocations of models (e.g., via Jupyter or Pytorch invocations).\r\n\r\nFull logs:\r\n\r\n```\r\nLog start\r\nmain: build = 2406 (48358b2e)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1710958493\r\nggml_init_cublas: no CUDA devices found, CUDA will be disabled\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/Llama-2-7B-GGUF/llama-2-7b.Q8_0.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q8_0:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attm      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 6.67 GiB (8.50 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  6828.64 MiB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nWARNING: failed to allocate 256.00 MB of pinned memory: unknown error\r\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nWARNING: failed to allocate 10.01 MB of pinned memory: unknown error\r\nllama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\r\nWARNING: failed to allocate 70.50 MB of pinned memory: unknown error\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    70.50 MiB\r\nllama_new_context_with_model: graph splits (measure): 1\r\n\r\nsystem_info: n_threads = 4 / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |\r\nsampling:\r\n        repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 1\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-20T18:20:41+00:00",
    "closed_at": "2024-03-21T23:24:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6184"
  },
  {
    "number": 3525,
    "title": "[User] Android build fails with \"ld.lld: error: undefined symbol: clGetPlatformIDs\"",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI am trying to use [this tutorial](https://github.com/ggerganov/llama.cpp#building-the-project-using-termux-f-droid) to compile llama.cpp.\r\n\r\n# Current Behavior\r\n\r\nCompilation failed.\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       32-bit, 64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 8\r\n  On-line CPU(s) list:  0-7\r\nVendor ID:              Qualcomm\r\n  Model name:           Kryo-V2\r\n    Model:              4\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           0xa\r\n    CPU(s) scaling MHz: 100%\r\n    CPU max MHz:        1900.8000\r\n    CPU min MHz:        300.0000\r\n    BogoMIPS:           38.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32\r\n  Model name:           Falkor-V1/Kryo\r\n    Model:              1\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           0xa\r\n    CPU(s) scaling MHz: 96%\r\n    CPU max MHz:        2457.6001\r\n    CPU min MHz:        300.0000\r\n    BogoMIPS:           38.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCaches (sum of all):    \r\n  L1d:                  384 KiB (8 instances)\r\n  L1i:                  384 KiB (8 instances)\r\n  L2:                   3 MiB (2 instances)\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\nLinux localhost 4.4.207-perf-g4f4b497d7bf8 #1 SMP PREEMPT Wed Dec 25 02:26:18 CST 2019 aarch64 Android\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\nGNU Make 4.4.1\r\nBuilt for aarch64-unknown-linux-android\r\nCopyright (C) 1988-2023 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nclang version 17.0.2\r\nTarget: aarch64-unknown-linux-android24\r\nThread model: posix\r\nInstalledDir: /data/data/com.termux/files/usr/bin\r\n\r\n# Steps to Reproduce\r\nFollow the tutorial step by step.\r\n\r\n# Failure Logs\r\n```\r\n~/llama.cpp $ make LLAMA_CLBLAST=1\r\nPackage clblast was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `clblast.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'clblast' found\r\nPackage clblast was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `clblast.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'clblast' found\r\nPackage clblast was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `clblast.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'clblast' found\r\nPackage clblast was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `clblast.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'clblast' found\r\nPackage clblast was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `clblast.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'clblast' found\r\nI llama.cpp build info: \r\nI UNAME_S:   Linux\r\nI UNAME_P:   unknown\r\nI UNAME_M:   aarch64\r\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -mcpu=native  \r\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \r\nI NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native     -Wno-pedantic -Xcompiler \"-Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \"\r\nI LDFLAGS:    \r\nI CC:        clang version 17.0.2\r\nI CXX:       clang version 17.0.2\r\n\r\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -mcpu=native     -c ggml.c -o ggml.o\r\nggml.c:2456:5: warning: implicit conversion increases floating-point precision: 'float32_t' (aka 'float') to 'ggml_float' (aka 'double') [-Wdouble-promotion]\r\n 2456 |     GGML_F16_VEC_REDUCE(sumf, sum);\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1983:41: note: expanded from macro 'GGML_F16_VEC_REDUCE'\r\n 1983 |     #define GGML_F16_VEC_REDUCE         GGML_F32Cx4_REDUCE\r\n      |                                         ^\r\nggml.c:1973:38: note: expanded from macro 'GGML_F32Cx4_REDUCE'\r\n 1973 |     #define GGML_F32Cx4_REDUCE       GGML_F32x4_REDUCE\r\n      |                                      ^\r\nggml.c:1903:11: note: expanded from macro 'GGML_F32x4_REDUCE'\r\n 1903 |     res = GGML_F32x4_REDUCE_ONE(x[0]);         \\\r\n      |         ~ ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1888:34: note: expanded from macro 'GGML_F32x4_REDUCE_ONE'\r\n 1888 | #define GGML_F32x4_REDUCE_ONE(x) vaddvq_f32(x)\r\n      |                                  ^~~~~~~~~~~~~\r\nggml.c:3716:9: warning: implicit conversion increases floating-point precision: 'float32_t' (aka 'float') to 'ggml_float' (aka 'double') [-Wdouble-promotion]\r\n 3716 |         GGML_F16_VEC_REDUCE(sumf[k], sum[k]);\r\n      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1983:41: note: expanded from macro 'GGML_F16_VEC_REDUCE'\r\n 1983 |     #define GGML_F16_VEC_REDUCE         GGML_F32Cx4_REDUCE\r\n      |                                         ^\r\nggml.c:1973:38: note: expanded from macro 'GGML_F32Cx4_REDUCE'\r\n 1973 |     #define GGML_F32Cx4_REDUCE       GGML_F32x4_REDUCE\r\n      |                                      ^\r\nggml.c:1903:11: note: expanded from macro 'GGML_F32x4_REDUCE'\r\n 1903 |     res = GGML_F32x4_REDUCE_ONE(x[0]);         \\\r\n      |         ~ ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1888:34: note: expanded from macro 'GGML_F32x4_REDUCE_ONE'\r\n 1888 | #define GGML_F32x4_REDUCE_ONE(x) vaddvq_f32(x)\r\n      |                                  ^~~~~~~~~~~~~\r\n2 warnings generated.\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c llama.cpp -o llama.o\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/common.cpp -o common.o\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/console.cpp -o console.o\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/grammar-parser.cpp -o grammar-parser.o\r\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -mcpu=native   -c k_quants.c -o k_quants.o\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c ggml-opencl.cpp -o ggml-opencl.o\r\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -mcpu=native     -c ggml-alloc.c -o ggml-alloc.o\r\naarch64-linux-android-clang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CLBLAST   -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-opencl.o ggml-alloc.o -o main  \r\nld.lld: error: undefined symbol: clGetPlatformIDs\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clGetPlatformInfo\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clGetDeviceIDs\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clGetDeviceInfo\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced 1 more times\r\n\r\nld.lld: error: undefined symbol: clCreateContext\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clCreateCommandQueue\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clCreateProgramWithSource\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clBuildProgram\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clCreateKernel\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced 21 more times\r\n\r\nld.lld: error: undefined symbol: clGetProgramBuildInfo\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_init)\r\n\r\nld.lld: error: undefined symbol: clReleaseMemObject\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_free_data)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced 11 more times\r\n\r\nld.lld: error: undefined symbol: clSetKernelArg\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced 18 more times\r\n\r\nld.lld: error: undefined symbol: clEnqueueNDRangeKernel\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul_mat_q_f32(ggml_tensor const*, ggml_tensor const*, ggml_tensor*))\r\n>>> referenced 1 more times\r\n\r\nld.lld: error: undefined symbol: clReleaseEvent\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced 3 more times\r\n\r\nld.lld: error: undefined symbol: clFinish\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced 4 more times\r\n\r\nld.lld: error: undefined symbol: clEnqueueReadBuffer\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul)\r\n>>> referenced 3 more times\r\n\r\nld.lld: error: undefined symbol: clblast::StatusCode clblast::Gemm<float>(clblast::Layout, clblast::Transpose, clblast::Transpose, unsigned long, unsigned long, unsigned long, float, _cl_mem*, unsigned long, unsigned long, _cl_mem*, unsigned long, unsigned long, float, _cl_mem*, unsigned long, unsigned long, _cl_command_queue**, _cl_event**, _cl_mem*)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul_mat)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul_mat_q_f32(ggml_tensor const*, ggml_tensor const*, ggml_tensor*))\r\n\r\nld.lld: error: undefined symbol: clEnqueueWriteBuffer\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul_mat)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_h2d_tensor_2d(_cl_command_queue*, _cl_mem*, unsigned long, ggml_tensor const*, unsigned long, unsigned long, _cl_event**))\r\n\r\nld.lld: error: undefined symbol: clblast::StatusCode clblast::Gemm<unsigned short>(clblast::Layout, clblast::Transpose, clblast::Transpose, unsigned long, unsigned long, unsigned long, unsigned short, _cl_mem*, unsigned long, unsigned long, _cl_mem*, unsigned long, unsigned long, unsigned short, _cl_mem*, unsigned long, unsigned long, _cl_command_queue**, _cl_event**, _cl_mem*)\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_mul_mat)\r\n\r\nld.lld: error: undefined symbol: clCreateBuffer\r\n>>> referenced by ggml-opencl.cpp\r\n>>>               ggml-opencl.o:(ggml_cl_pool_malloc(unsigned long, unsigned long*))\r\n\r\nld.lld: error: too many errors emitted, stopping now (use --error-limit=0 to see all errors)\r\naarch64-linux-android-clang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [Makefile:543: main] Error 1\r\n```",
    "labels": [
      "build",
      "android"
    ],
    "state": "closed",
    "created_at": "2023-10-07T12:03:41+00:00",
    "closed_at": "2023-10-07T15:12:36+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3525/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3525"
  },
  {
    "number": 9317,
    "title": "Feature Request: Add OLMoE",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd this model (and other variants) https://huggingface.co/allenai/OLMoE-1B-7B-0924-Instruct\n\n### Motivation\n\nWe recently released the OLMoE model at Ai2. 1.3b active / 6.9b total param MoE model. Seems solid, and we'd love people to use it.\n\n### Possible Implementation\n\nShould be able to quickly use mix of existing OLMo implementation + Transformers version https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-04T22:33:20+00:00",
    "closed_at": "2025-01-13T01:07:36+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9317/reactions",
      "total_count": 18,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9317"
  },
  {
    "number": 2288,
    "title": "Problem with gpu (help)",
    "body": "Hello, I am completly newbie, when it comes to the subject of llms\r\nI install some ggml model to oogabooga webui And I try to use it. It works fine, but only for RAM. For VRAM only uses 0.5gb, and I don't have any possibility to change it (offload some layers to GPU), even pasting in webui line \"--n-gpu-layers 10\" dont work. So I stareted searching, one of answers is command:\r\n\r\n```\r\npip uninstall -y llama-cpp-python\r\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\r\nset FORCE_CMAKE=1\r\npip install llama-cpp-python --no-cache-dir\r\n```\r\nBut that dont work for me. I got after paste it:\r\n\r\n ```\r\n[end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for llama-cpp-python\r\nFailed to build llama-cpp-python\r\nERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\r\n```\r\nAnd it completly broke llama folder.. It uninstall it, and did nothing more. I need to update webui to fix and download llama.cpp again, cause I don't have any other possibility to download it.\r\n\r\nI try also downloading compilation method, but that did.t work also. When i paste `CMAKE_ARGS=\"-DLLAMA_OPENBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python` in CMD/ CMD Windows in oogabooga, a I always got this message:\r\n\r\n```\r\n'CMAKE_ARGS' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n```\r\nor\r\n\r\n```\r\n'FORCE_CMAKE' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nSame for command \"make\" it unrecognised it despite I have istalled make and Cmake\r\n```\r\n\r\nalso, when i lanuch webui and choose ggml model, I got something like this in console:\r\n\r\n```\r\nlama model load internal: format ggjt v3 (latest) \r\nlama model load internal: n_voc = 32001 \r\nlama model load internal: n_ctx = 2048 \r\nlama model load internal: n_embd = 6656 \r\nlama model load internal: n mult = 256 \r\nlama model load internal: n head = 52 \r\nlama model load internal: n_layer = 60 \r\nlama model load internal: n_rot = 128 \r\nlama model load internal: freq_base = 10000.0 \r\nlama model load internal: freq_scale = 1 \r\nlama model load internal: ftype = 2 (mostly Q4_0) \r\nlama model load internal: n_ff = 17920 \r\nlama model load internal: model size = 30B \r\nlama model_load internal: ggml ctx size = 0.14 MB \r\nlama_model_load internal: mem required = 19712.68 MB 1+ 3124.00 MB per state) \r\nlama_new_context with model: kv self size = 3120.00 MB\r\nAVX=1 | AVX2=1 | AVX512=0 | AVX512_VBMI=0 | AVX512_VNNII=0 | FMA=1 | NEON=0 | ARM_FMA=0 | F16C=1 | FP16_VA=0 | - a WASM_SIMD=0 | BLAS=0 | SSE3=1 1 | VSX=0 |\r\n2023.07.19 23:05:22 INFO:Loaded the model in 8.17 Seconds. \r\n```\r\nI am using windows and nvidia card\r\n\r\nEasy solution to enable GPU offlading layers, that dont reqiure installing a ton of stuffs?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-20T09:41:52+00:00",
    "closed_at": "2024-04-09T01:07:46+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2288/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2288"
  },
  {
    "number": 7147,
    "title": "error: implicit declaration of function \u2018vld1q_s8_x4\u2019; did you mean \u2018vld1q_s8_x2\u2019?",
    "body": "This error is on M3 Max inside docker container with linux.\r\nDocker file:\r\n```\r\nFROM python:3.10.12-slim-buster\r\n\r\nUSER root\r\nRUN apt-get update && apt-get install cmake libopenblas-dev build-essential pkg-config git -y\r\n\r\nWORKDIR /opt\r\nCOPY ./requirements/cpu.requirements.txt ./requirements.txt\r\nRUN CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip3 install --upgrade -r requirements.txt\r\n\r\nCOPY infra/llm_server_cpu/server_config.json server_config.json\r\n\r\nEXPOSE 8085\r\nCMD [\"python3\", \"-m\", \"llama_cpp.server\", \"--config_file\", \"server_config.json\"]\r\n```\r\n\r\nThe error trace:\r\n```\r\n2   \u00d7 Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\r\n25.92   \u2502 exit code: 1\r\n25.92   \u2570\u2500> [158 lines of output]\r\n25.92       *** scikit-build-core 0.9.3 using CMake 3.29.2 (wheel)\r\n25.92       *** Configuring CMake...\r\n25.92       loading initial cache file /tmp/tmph68_ek8q/build/CMakeInit.txt\r\n25.92       -- The C compiler identification is GNU 8.3.0\r\n25.92       -- The CXX compiler identification is GNU 8.3.0\r\n25.92       -- Detecting C compiler ABI info\r\n25.92       -- Detecting C compiler ABI info - done\r\n25.92       -- Check for working C compiler: /usr/bin/cc - skipped\r\n25.92       -- Detecting C compile features\r\n25.92       -- Detecting C compile features - done\r\n25.92       -- Detecting CXX compiler ABI info\r\n25.92       -- Detecting CXX compiler ABI info - done\r\n25.92       -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n25.92       -- Detecting CXX compile features\r\n25.92       -- Detecting CXX compile features - done\r\n25.92       -- Found Git: /usr/bin/git (found version \"2.20.1\")\r\n25.92       -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n25.92       -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n25.92       -- Check if compiler accepts -pthread\r\n25.92       -- Check if compiler accepts -pthread - yes\r\n25.92       -- Found Threads: TRUE\r\n25.92       -- Looking for sgemm_\r\n25.92       -- Looking for sgemm_ - found\r\n25.92       -- Found BLAS: /usr/lib/aarch64-linux-gnu/libopenblas.so\r\n25.92       -- BLAS found, Libraries: /usr/lib/aarch64-linux-gnu/libopenblas.so\r\n25.92       -- Found PkgConfig: /usr/bin/pkg-config (found version \"0.29\")\r\n25.92       -- Checking for module 'openblas64'\r\n25.92       --   No package 'openblas64' found\r\n25.92       -- Checking for module 'openblas'\r\n25.92       --   Found openblas, version 0.3.5\r\n25.92       -- BLAS found, Includes: /usr/include/aarch64-linux-gnu\r\n25.92       -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\r\n25.92       -- CMAKE_SYSTEM_PROCESSOR: aarch64\r\n25.92       -- ARM detected\r\n25.92       -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\r\n25.92       -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\r\n25.92       CMake Warning (dev) at CMakeLists.txt:26 (install):\r\n25.92         Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n25.92       This warning is for project developers.  Use -Wno-dev to suppress it.\r\n25.92       \r\n25.92       CMake Warning (dev) at CMakeLists.txt:35 (install):\r\n25.92         Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n25.92       This warning is for project developers.  Use -Wno-dev to suppress it.\r\n25.92       \r\n25.92       -- Configuring done (0.3s)\r\n25.92       -- Generating done (0.0s)\r\n25.92       -- Build files have been written to: /tmp/tmph68_ek8q/build\r\n25.92       *** Building project with Ninja...\r\n25.92       Change Dir: '/tmp/tmph68_ek8q/build'\r\n25.92       \r\n25.92       Run Build Command(s): /tmp/pip-build-env-tb00ftrb/normal/lib/python3.10/site-packages/ninja/data/bin/ninja -v\r\n25.92       [1/27] cd /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp && /tmp/pip-build-env-tb00ftrb/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=8.3.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\r\n25.92       -- Found Git: /usr/bin/git (found version \"2.20.1\")\r\n25.92       [2/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/build-info.cpp\r\n25.92       [3/27] /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c\r\n25.92       FAILED: vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\r\n25.92       /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c\r\n25.92       In file included from /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:5:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q3_K_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:293:27: error: implicit declaration of function \u2018vld1q_s8_x4\u2019; did you mean \u2018vld1q_s8_x2\u2019? [-Werror=implicit-function-declaration]\r\n25.92        #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:5443:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n25.92                    const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                                                       ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:293:27: error: invalid initializer\r\n25.92        #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:5443:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n25.92                    const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                                                       ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:293:27: error: invalid initializer\r\n25.92        #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:5444:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n25.92                    const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                                                       ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q5_K_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:293:27: error: invalid initializer\r\n25.92        #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:6928:46: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n25.92                    const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                                                     ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q6_K_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:291:27: error: implicit declaration of function \u2018vld1q_u8_x4\u2019; did you mean \u2018vld1q_u8_x2\u2019? [-Werror=implicit-function-declaration]\r\n25.92        #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:7610:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n25.92                    ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n25.92                                               ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:291:27: error: invalid initializer\r\n25.92        #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:7610:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n25.92                    ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n25.92                                               ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-impl.h:293:27: error: invalid initializer\r\n25.92        #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n25.92                                  ^~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:7611:40: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n25.92                    ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                                               ^~~~~~~~~~~~~~~~\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:7636:21: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                            ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq2_xxs_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:8365:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq2_xs_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:8503:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq2_s_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:8787:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq3_xxs_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:8979:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq3_s_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:9144:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq1_s_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:9370:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq1_m_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:9529:17: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                        ^\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq4_xs_q8_K\u2019:\r\n25.92       /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-quants.c:9837:20: error: incompatible types when assigning to type \u2018int8x16x4_t\u2019 {aka \u2018struct int8x16x4_t\u2019} from type \u2018int\u2019\r\n25.92                    q8b    = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n25.92                           ^\r\n25.92       cc1: some warnings being treated as errors\r\n25.92       [4/27] /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-alloc.c\r\n25.92       [5/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/console.cpp\r\n25.92       [6/27] /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml-backend.c\r\n25.92       [7/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/grammar-parser.cpp\r\n25.92       [8/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/sampling.cpp\r\n25.92       [9/27] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wno-cast-qual -pthread -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/llava.cpp\r\n25.92       [10/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/sgemm.cpp\r\n25.92       [11/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -pthread -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/unicode-data.cpp\r\n25.92       [12/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/train.cpp\r\n25.92       [13/27] /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/ggml.c\r\n25.92       [14/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -pthread -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/unicode.cpp\r\n25.92       [15/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/json-schema-to-grammar.cpp\r\n25.92       [16/27] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wno-cast-qual -pthread -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/examples/llava/clip.cpp\r\n25.92       [17/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/. -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/common/common.cpp\r\n25.92       [18/27] /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENBLAS -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -pthread -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-tuyxmx5n/llama-cpp-python_2d9d4194a33a41ab980c0abe2b61c087/vendor/llama.cpp/llama.cpp\r\n25.92       ninja: build stopped: subcommand failed.\r\n25.92       \r\n25.92       \r\n25.92       *** CMake build failed\r\n25.92       [end of output]\r\n\r\n```\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-08T13:28:52+00:00",
    "closed_at": "2024-07-16T01:06:49+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7147/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7147"
  },
  {
    "number": 7489,
    "title": "Completion of error handling",
    "body": "Would you like to add more error handling for return values from functions like the following?\r\n* [fprintf](https://pubs.opengroup.org/onlinepubs/9699919799/functions/fprintf.html \"Print formatted output.\") \u21d2 [print_grammar_char](https://github.com/ggerganov/llama.cpp/blob/9b82476ee9e73065a759f8bcc4cf27ec7ab2ed8c/common/grammar-parser.cpp#L313-L320)\r\n* [malloc](https://pubs.opengroup.org/onlinepubs/9699919799/functions/malloc.html \"Memory allocation\") \u21d2 [ggml_backend_buffer_init](https://github.com/ggerganov/llama.cpp/blob/9b82476ee9e73065a759f8bcc4cf27ec7ab2ed8c/ggml-backend.c#L60-L76)\r\n* [strdup](https://pubs.opengroup.org/onlinepubs/9699919799/functions/strdup.html \"Duplicate a string.\") \u21d2 [ggml_vk_available_devices_internal](https://github.com/ggerganov/llama.cpp/blob/9b82476ee9e73065a759f8bcc4cf27ec7ab2ed8c/ggml-kompute.cpp#L239-L254)",
    "labels": [
      "good first issue",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-23T10:56:09+00:00",
    "closed_at": "2024-07-27T21:43:04+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7489/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7489"
  },
  {
    "number": 7046,
    "title": "Unable to make imatrix (and likely quant) for nvidia's ChatQA-1.5 8B",
    "body": "This model: https://huggingface.co/nvidia/ChatQA-1.5-8B\r\n\r\nConversion worked no issue, but then when it's time to calculate the imatrix I see:\r\n\r\n`llama_model_load: error loading model: done_getting_tensors: wrong number of tensors; expected 323, got 291`\r\n\r\nDoing a search, last time slaren mentioned doing a gguf-dump.py, here's the output:\r\n\r\n```\r\n* Loading: /models/ChatQA-1.5-8B-GGUF/ChatQA-1.5-8B-fp16.gguf\r\n* File is LITTLE endian, script is running on a LITTLE endian host.\r\nllama_cpp-1  |\r\n* Dumping 24 key/value pair(s)\r\n      1: UINT32     |        1 | GGUF.version = 3\r\n      2: UINT64     |        1 | GGUF.tensor_count = 323\r\n      3: UINT64     |        1 | GGUF.kv_count = 21\r\n      4: STRING     |        1 | general.architecture = 'llama'\r\n      5: STRING     |        1 | general.name = 'ChatQA-1.5-8B'\r\n      6: UINT32     |        1 | llama.block_count = 32\r\n      7: UINT32     |        1 | llama.context_length = 8192\r\n      8: UINT32     |        1 | llama.embedding_length = 4096\r\n      9: UINT32     |        1 | llama.feed_forward_length = 14336\r\n     10: UINT32     |        1 | llama.attention.head_count = 32\r\n     11: UINT32     |        1 | llama.attention.head_count_kv = 8\r\n     12: FLOAT32    |        1 | llama.rope.freq_base = 500000.0\r\n     13: FLOAT32    |        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\r\n     14: UINT32     |        1 | general.file_type = 1\r\n     15: UINT32     |        1 | llama.vocab_size = 128256\r\n     16: UINT32     |        1 | llama.rope.dimension_count = 128\r\n     17: STRING     |        1 | tokenizer.ggml.model = 'gpt2'\r\n     18: STRING     |        1 | tokenizer.ggml.pre = 'llama-bpe'\r\n     19: [STRING]   |   128256 | tokenizer.ggml.tokens\r\n     20: [INT32]    |   128256 | tokenizer.ggml.token_type\r\n     21: [STRING]   |   280147 | tokenizer.ggml.merges\r\n     22: UINT32     |        1 | tokenizer.ggml.bos_token_id = 128000\r\n     23: UINT32     |        1 | tokenizer.ggml.eos_token_id = 128001\r\n     24: STRING     |        1 | tokenizer.chat_template = '{% set loop_messages = messages %}{% for message in loop_mes'\r\nllama_cpp-1  |\r\n* Dumping 323 tensor(s)\r\n      1:  525336576 |  4096, 128256,     1,     1 | F16     | token_embd.weight\r\n      2:       4096 |  4096,     1,     1,     1 | F32     | blk.0.attn_norm.weight\r\n      3:   58720256 | 14336,  4096,     1,     1 | F16     | blk.0.ffn_down.weight\r\n      4:   58720256 |  4096, 14336,     1,     1 | F16     | blk.0.ffn_gate.weight\r\n      5:   58720256 |  4096, 14336,     1,     1 | F16     | blk.0.ffn_up.weight\r\n      6:       4096 |  4096,     1,     1,     1 | F32     | blk.0.ffn_norm.weight\r\n      7:    4194304 |  4096,  1024,     1,     1 | F16     | blk.0.attn_k.weight\r\n      8:   16777216 |  4096,  4096,     1,     1 | F16     | blk.0.attn_output.weight\r\n      9:   16777216 |  4096,  4096,     1,     1 | F16     | blk.0.attn_q.weight\r\n     10:         64 |    64,     1,     1,     1 | F32     | blk.0.attn_rot_embd\r\n     11:    4194304 |  4096,  1024,     1,     1 | F16     | blk.0.attn_v.weight\r\n     12:       4096 |  4096,     1,     1,     1 | F32     | blk.1.attn_norm.weight\r\n     13:   58720256 | 14336,  4096,     1,     1 | F16     | blk.1.ffn_down.weight\r\n     14:   58720256 |  4096, 14336,     1,     1 | F16     | blk.1.ffn_gate.weight\r\n     15:   58720256 |  4096, 14336,     1,     1 | F16     | blk.1.ffn_up.weight\r\n     16:       4096 |  4096,     1,     1,     1 | F32     | blk.1.ffn_norm.weight\r\n     17:    4194304 |  4096,  1024,     1,     1 | F16     | blk.1.attn_k.weight\r\n     18:   16777216 |  4096,  4096,     1,     1 | F16     | blk.1.attn_output.weight\r\n     19:   16777216 |  4096,  4096,     1,     1 | F16     | blk.1.attn_q.weight\r\n     20:         64 |    64,     1,     1,     1 | F32     | blk.1.attn_rot_embd\r\n     21:    4194304 |  4096,  1024,     1,     1 | F16     | blk.1.attn_v.weight\r\n     22:       4096 |  4096,     1,     1,     1 | F32     | blk.10.attn_norm.weight\r\n     23:   58720256 | 14336,  4096,     1,     1 | F16     | blk.10.ffn_down.weight\r\n     24:   58720256 |  4096, 14336,     1,     1 | F16     | blk.10.ffn_gate.weight\r\n     25:   58720256 |  4096, 14336,     1,     1 | F16     | blk.10.ffn_up.weight\r\n     26:       4096 |  4096,     1,     1,     1 | F32     | blk.10.ffn_norm.weight\r\n     27:    4194304 |  4096,  1024,     1,     1 | F16     | blk.10.attn_k.weight\r\n     28:   16777216 |  4096,  4096,     1,     1 | F16     | blk.10.attn_output.weight\r\n     29:   16777216 |  4096,  4096,     1,     1 | F16     | blk.10.attn_q.weight\r\n     30:         64 |    64,     1,     1,     1 | F32     | blk.10.attn_rot_embd\r\n     31:    4194304 |  4096,  1024,     1,     1 | F16     | blk.10.attn_v.weight\r\n     32:       4096 |  4096,     1,     1,     1 | F32     | blk.11.attn_norm.weight\r\n     33:   58720256 | 14336,  4096,     1,     1 | F16     | blk.11.ffn_down.weight\r\n     34:   58720256 |  4096, 14336,     1,     1 | F16     | blk.11.ffn_gate.weight\r\n     35:   58720256 |  4096, 14336,     1,     1 | F16     | blk.11.ffn_up.weight\r\n     36:       4096 |  4096,     1,     1,     1 | F32     | blk.11.ffn_norm.weight\r\n     37:    4194304 |  4096,  1024,     1,     1 | F16     | blk.11.attn_k.weight\r\n     38:   16777216 |  4096,  4096,     1,     1 | F16     | blk.11.attn_output.weight\r\n     39:   16777216 |  4096,  4096,     1,     1 | F16     | blk.11.attn_q.weight\r\n     40:         64 |    64,     1,     1,     1 | F32     | blk.11.attn_rot_embd\r\n     41:    4194304 |  4096,  1024,     1,     1 | F16     | blk.11.attn_v.weight\r\n     42:       4096 |  4096,     1,     1,     1 | F32     | blk.12.attn_norm.weight\r\n     43:   58720256 | 14336,  4096,     1,     1 | F16     | blk.12.ffn_down.weight\r\n     44:   58720256 |  4096, 14336,     1,     1 | F16     | blk.12.ffn_gate.weight\r\n     45:   58720256 |  4096, 14336,     1,     1 | F16     | blk.12.ffn_up.weight\r\n     46:       4096 |  4096,     1,     1,     1 | F32     | blk.12.ffn_norm.weight\r\n     47:    4194304 |  4096,  1024,     1,     1 | F16     | blk.12.attn_k.weight\r\n     48:   16777216 |  4096,  4096,     1,     1 | F16     | blk.12.attn_output.weight\r\n     49:   16777216 |  4096,  4096,     1,     1 | F16     | blk.12.attn_q.weight\r\n     50:         64 |    64,     1,     1,     1 | F32     | blk.12.attn_rot_embd\r\n     51:    4194304 |  4096,  1024,     1,     1 | F16     | blk.12.attn_v.weight\r\n     52:       4096 |  4096,     1,     1,     1 | F32     | blk.13.attn_norm.weight\r\n     53:   58720256 | 14336,  4096,     1,     1 | F16     | blk.13.ffn_down.weight\r\n     54:   58720256 |  4096, 14336,     1,     1 | F16     | blk.13.ffn_gate.weight\r\n     55:   58720256 |  4096, 14336,     1,     1 | F16     | blk.13.ffn_up.weight\r\n     56:       4096 |  4096,     1,     1,     1 | F32     | blk.13.ffn_norm.weight\r\n     57:    4194304 |  4096,  1024,     1,     1 | F16     | blk.13.attn_k.weight\r\n     58:   16777216 |  4096,  4096,     1,     1 | F16     | blk.13.attn_output.weight\r\n     59:   16777216 |  4096,  4096,     1,     1 | F16     | blk.13.attn_q.weight\r\n     60:         64 |    64,     1,     1,     1 | F32     | blk.13.attn_rot_embd\r\n     61:    4194304 |  4096,  1024,     1,     1 | F16     | blk.13.attn_v.weight\r\n     62:       4096 |  4096,     1,     1,     1 | F32     | blk.14.attn_norm.weight\r\n     63:   58720256 | 14336,  4096,     1,     1 | F16     | blk.14.ffn_down.weight\r\n     64:   58720256 |  4096, 14336,     1,     1 | F16     | blk.14.ffn_gate.weight\r\n     65:   58720256 |  4096, 14336,     1,     1 | F16     | blk.14.ffn_up.weight\r\n     66:       4096 |  4096,     1,     1,     1 | F32     | blk.14.ffn_norm.weight\r\n     67:    4194304 |  4096,  1024,     1,     1 | F16     | blk.14.attn_k.weight\r\n     68:   16777216 |  4096,  4096,     1,     1 | F16     | blk.14.attn_output.weight\r\n     69:   16777216 |  4096,  4096,     1,     1 | F16     | blk.14.attn_q.weight\r\n     70:         64 |    64,     1,     1,     1 | F32     | blk.14.attn_rot_embd\r\n     71:    4194304 |  4096,  1024,     1,     1 | F16     | blk.14.attn_v.weight\r\n     72:       4096 |  4096,     1,     1,     1 | F32     | blk.15.attn_norm.weight\r\n     73:   58720256 | 14336,  4096,     1,     1 | F16     | blk.15.ffn_down.weight\r\n     74:   58720256 |  4096, 14336,     1,     1 | F16     | blk.15.ffn_gate.weight\r\n     75:   58720256 |  4096, 14336,     1,     1 | F16     | blk.15.ffn_up.weight\r\n     76:       4096 |  4096,     1,     1,     1 | F32     | blk.15.ffn_norm.weight\r\n     77:    4194304 |  4096,  1024,     1,     1 | F16     | blk.15.attn_k.weight\r\n     78:   16777216 |  4096,  4096,     1,     1 | F16     | blk.15.attn_output.weight\r\n     79:   16777216 |  4096,  4096,     1,     1 | F16     | blk.15.attn_q.weight\r\n     80:         64 |    64,     1,     1,     1 | F32     | blk.15.attn_rot_embd\r\n     81:    4194304 |  4096,  1024,     1,     1 | F16     | blk.15.attn_v.weight\r\n     82:       4096 |  4096,     1,     1,     1 | F32     | blk.16.attn_norm.weight\r\n     83:   58720256 | 14336,  4096,     1,     1 | F16     | blk.16.ffn_down.weight\r\n     84:   58720256 |  4096, 14336,     1,     1 | F16     | blk.16.ffn_gate.weight\r\n     85:   58720256 |  4096, 14336,     1,     1 | F16     | blk.16.ffn_up.weight\r\n     86:       4096 |  4096,     1,     1,     1 | F32     | blk.16.ffn_norm.weight\r\n     87:    4194304 |  4096,  1024,     1,     1 | F16     | blk.16.attn_k.weight\r\n     88:   16777216 |  4096,  4096,     1,     1 | F16     | blk.16.attn_output.weight\r\n     89:   16777216 |  4096,  4096,     1,     1 | F16     | blk.16.attn_q.weight\r\n     90:         64 |    64,     1,     1,     1 | F32     | blk.16.attn_rot_embd\r\n     91:    4194304 |  4096,  1024,     1,     1 | F16     | blk.16.attn_v.weight\r\n     92:       4096 |  4096,     1,     1,     1 | F32     | blk.17.attn_norm.weight\r\n     93:   58720256 | 14336,  4096,     1,     1 | F16     | blk.17.ffn_down.weight\r\n     94:   58720256 |  4096, 14336,     1,     1 | F16     | blk.17.ffn_gate.weight\r\n     95:   58720256 |  4096, 14336,     1,     1 | F16     | blk.17.ffn_up.weight\r\n     96:       4096 |  4096,     1,     1,     1 | F32     | blk.17.ffn_norm.weight\r\n     97:    4194304 |  4096,  1024,     1,     1 | F16     | blk.17.attn_k.weight\r\n     98:   16777216 |  4096,  4096,     1,     1 | F16     | blk.17.attn_output.weight\r\n     99:   16777216 |  4096,  4096,     1,     1 | F16     | blk.17.attn_q.weight\r\n    100:         64 |    64,     1,     1,     1 | F32     | blk.17.attn_rot_embd\r\n    101:    4194304 |  4096,  1024,     1,     1 | F16     | blk.17.attn_v.weight\r\n    102:       4096 |  4096,     1,     1,     1 | F32     | blk.18.attn_norm.weight\r\n    103:   58720256 | 14336,  4096,     1,     1 | F16     | blk.18.ffn_down.weight\r\n    104:   58720256 |  4096, 14336,     1,     1 | F16     | blk.18.ffn_gate.weight\r\n    105:   58720256 |  4096, 14336,     1,     1 | F16     | blk.18.ffn_up.weight\r\n    106:       4096 |  4096,     1,     1,     1 | F32     | blk.18.ffn_norm.weight\r\n    107:    4194304 |  4096,  1024,     1,     1 | F16     | blk.18.attn_k.weight\r\n    108:   16777216 |  4096,  4096,     1,     1 | F16     | blk.18.attn_output.weight\r\n    109:   16777216 |  4096,  4096,     1,     1 | F16     | blk.18.attn_q.weight\r\n    110:         64 |    64,     1,     1,     1 | F32     | blk.18.attn_rot_embd\r\n    111:    4194304 |  4096,  1024,     1,     1 | F16     | blk.18.attn_v.weight\r\n    112:       4096 |  4096,     1,     1,     1 | F32     | blk.19.attn_norm.weight\r\n    113:   58720256 | 14336,  4096,     1,     1 | F16     | blk.19.ffn_down.weight\r\n    114:   58720256 |  4096, 14336,     1,     1 | F16     | blk.19.ffn_gate.weight\r\n    115:   58720256 |  4096, 14336,     1,     1 | F16     | blk.19.ffn_up.weight\r\n    116:       4096 |  4096,     1,     1,     1 | F32     | blk.19.ffn_norm.weight\r\n    117:    4194304 |  4096,  1024,     1,     1 | F16     | blk.19.attn_k.weight\r\n    118:   16777216 |  4096,  4096,     1,     1 | F16     | blk.19.attn_output.weight\r\n    119:   16777216 |  4096,  4096,     1,     1 | F16     | blk.19.attn_q.weight\r\n    120:         64 |    64,     1,     1,     1 | F32     | blk.19.attn_rot_embd\r\n    121:    4194304 |  4096,  1024,     1,     1 | F16     | blk.19.attn_v.weight\r\n    122:       4096 |  4096,     1,     1,     1 | F32     | blk.2.attn_norm.weight\r\n    123:   58720256 | 14336,  4096,     1,     1 | F16     | blk.2.ffn_down.weight\r\n    124:   58720256 |  4096, 14336,     1,     1 | F16     | blk.2.ffn_gate.weight\r\n    125:   58720256 |  4096, 14336,     1,     1 | F16     | blk.2.ffn_up.weight\r\n    126:       4096 |  4096,     1,     1,     1 | F32     | blk.2.ffn_norm.weight\r\n    127:    4194304 |  4096,  1024,     1,     1 | F16     | blk.2.attn_k.weight\r\n    128:   16777216 |  4096,  4096,     1,     1 | F16     | blk.2.attn_output.weight\r\n    129:   16777216 |  4096,  4096,     1,     1 | F16     | blk.2.attn_q.weight\r\n    130:         64 |    64,     1,     1,     1 | F32     | blk.2.attn_rot_embd\r\n    131:    4194304 |  4096,  1024,     1,     1 | F16     | blk.2.attn_v.weight\r\n    132:       4096 |  4096,     1,     1,     1 | F32     | blk.20.attn_norm.weight\r\n    133:   58720256 |  4096, 14336,     1,     1 | F16     | blk.20.ffn_gate.weight\r\n    134:       4096 |  4096,     1,     1,     1 | F32     | blk.20.ffn_norm.weight\r\n    135:    4194304 |  4096,  1024,     1,     1 | F16     | blk.20.attn_k.weight\r\n    136:   16777216 |  4096,  4096,     1,     1 | F16     | blk.20.attn_output.weight\r\n    137:   16777216 |  4096,  4096,     1,     1 | F16     | blk.20.attn_q.weight\r\n    138:         64 |    64,     1,     1,     1 | F32     | blk.20.attn_rot_embd\r\n    139:    4194304 |  4096,  1024,     1,     1 | F16     | blk.20.attn_v.weight\r\n    140:       4096 |  4096,     1,     1,     1 | F32     | blk.3.attn_norm.weight\r\n    141:   58720256 | 14336,  4096,     1,     1 | F16     | blk.3.ffn_down.weight\r\n    142:   58720256 |  4096, 14336,     1,     1 | F16     | blk.3.ffn_gate.weight\r\n    143:   58720256 |  4096, 14336,     1,     1 | F16     | blk.3.ffn_up.weight\r\n    144:       4096 |  4096,     1,     1,     1 | F32     | blk.3.ffn_norm.weight\r\n    145:    4194304 |  4096,  1024,     1,     1 | F16     | blk.3.attn_k.weight\r\n    146:   16777216 |  4096,  4096,     1,     1 | F16     | blk.3.attn_output.weight\r\n    147:   16777216 |  4096,  4096,     1,     1 | F16     | blk.3.attn_q.weight\r\n    148:         64 |    64,     1,     1,     1 | F32     | blk.3.attn_rot_embd\r\n    149:    4194304 |  4096,  1024,     1,     1 | F16     | blk.3.attn_v.weight\r\n    150:       4096 |  4096,     1,     1,     1 | F32     | blk.4.attn_norm.weight\r\n    151:   58720256 | 14336,  4096,     1,     1 | F16     | blk.4.ffn_down.weight\r\n    152:   58720256 |  4096, 14336,     1,     1 | F16     | blk.4.ffn_gate.weight\r\n    153:   58720256 |  4096, 14336,     1,     1 | F16     | blk.4.ffn_up.weight\r\n    154:       4096 |  4096,     1,     1,     1 | F32     | blk.4.ffn_norm.weight\r\n    155:    4194304 |  4096,  1024,     1,     1 | F16     | blk.4.attn_k.weight\r\n    156:   16777216 |  4096,  4096,     1,     1 | F16     | blk.4.attn_output.weight\r\n    157:   16777216 |  4096,  4096,     1,     1 | F16     | blk.4.attn_q.weight\r\n    158:         64 |    64,     1,     1,     1 | F32     | blk.4.attn_rot_embd\r\n    159:    4194304 |  4096,  1024,     1,     1 | F16     | blk.4.attn_v.weight\r\n    160:       4096 |  4096,     1,     1,     1 | F32     | blk.5.attn_norm.weight\r\n    161:   58720256 | 14336,  4096,     1,     1 | F16     | blk.5.ffn_down.weight\r\n    162:   58720256 |  4096, 14336,     1,     1 | F16     | blk.5.ffn_gate.weight\r\n    163:   58720256 |  4096, 14336,     1,     1 | F16     | blk.5.ffn_up.weight\r\n    164:       4096 |  4096,     1,     1,     1 | F32     | blk.5.ffn_norm.weight\r\n    165:    4194304 |  4096,  1024,     1,     1 | F16     | blk.5.attn_k.weight\r\n    166:   16777216 |  4096,  4096,     1,     1 | F16     | blk.5.attn_output.weight\r\n    167:   16777216 |  4096,  4096,     1,     1 | F16     | blk.5.attn_q.weight\r\n    168:         64 |    64,     1,     1,     1 | F32     | blk.5.attn_rot_embd\r\n    169:    4194304 |  4096,  1024,     1,     1 | F16     | blk.5.attn_v.weight\r\n    170:       4096 |  4096,     1,     1,     1 | F32     | blk.6.attn_norm.weight\r\n    171:   58720256 | 14336,  4096,     1,     1 | F16     | blk.6.ffn_down.weight\r\n    172:   58720256 |  4096, 14336,     1,     1 | F16     | blk.6.ffn_gate.weight\r\n    173:   58720256 |  4096, 14336,     1,     1 | F16     | blk.6.ffn_up.weight\r\n    174:       4096 |  4096,     1,     1,     1 | F32     | blk.6.ffn_norm.weight\r\n    175:    4194304 |  4096,  1024,     1,     1 | F16     | blk.6.attn_k.weight\r\n    176:   16777216 |  4096,  4096,     1,     1 | F16     | blk.6.attn_output.weight\r\n    177:   16777216 |  4096,  4096,     1,     1 | F16     | blk.6.attn_q.weight\r\n    178:         64 |    64,     1,     1,     1 | F32     | blk.6.attn_rot_embd\r\n    179:    4194304 |  4096,  1024,     1,     1 | F16     | blk.6.attn_v.weight\r\n    180:       4096 |  4096,     1,     1,     1 | F32     | blk.7.attn_norm.weight\r\n    181:   58720256 | 14336,  4096,     1,     1 | F16     | blk.7.ffn_down.weight\r\n    182:   58720256 |  4096, 14336,     1,     1 | F16     | blk.7.ffn_gate.weight\r\n    183:   58720256 |  4096, 14336,     1,     1 | F16     | blk.7.ffn_up.weight\r\n    184:       4096 |  4096,     1,     1,     1 | F32     | blk.7.ffn_norm.weight\r\n    185:    4194304 |  4096,  1024,     1,     1 | F16     | blk.7.attn_k.weight\r\n    186:   16777216 |  4096,  4096,     1,     1 | F16     | blk.7.attn_output.weight\r\n    187:   16777216 |  4096,  4096,     1,     1 | F16     | blk.7.attn_q.weight\r\n    188:         64 |    64,     1,     1,     1 | F32     | blk.7.attn_rot_embd\r\n    189:    4194304 |  4096,  1024,     1,     1 | F16     | blk.7.attn_v.weight\r\n    190:       4096 |  4096,     1,     1,     1 | F32     | blk.8.attn_norm.weight\r\n    191:   58720256 | 14336,  4096,     1,     1 | F16     | blk.8.ffn_down.weight\r\n    192:   58720256 |  4096, 14336,     1,     1 | F16     | blk.8.ffn_gate.weight\r\n    193:   58720256 |  4096, 14336,     1,     1 | F16     | blk.8.ffn_up.weight\r\n    194:       4096 |  4096,     1,     1,     1 | F32     | blk.8.ffn_norm.weight\r\n    195:    4194304 |  4096,  1024,     1,     1 | F16     | blk.8.attn_k.weight\r\n    196:   16777216 |  4096,  4096,     1,     1 | F16     | blk.8.attn_output.weight\r\n    197:   16777216 |  4096,  4096,     1,     1 | F16     | blk.8.attn_q.weight\r\n    198:         64 |    64,     1,     1,     1 | F32     | blk.8.attn_rot_embd\r\n    199:    4194304 |  4096,  1024,     1,     1 | F16     | blk.8.attn_v.weight\r\n    200:       4096 |  4096,     1,     1,     1 | F32     | blk.9.attn_norm.weight\r\n    201:   58720256 | 14336,  4096,     1,     1 | F16     | blk.9.ffn_down.weight\r\n    202:   58720256 |  4096, 14336,     1,     1 | F16     | blk.9.ffn_gate.weight\r\n    203:   58720256 |  4096, 14336,     1,     1 | F16     | blk.9.ffn_up.weight\r\n    204:       4096 |  4096,     1,     1,     1 | F32     | blk.9.ffn_norm.weight\r\n    205:    4194304 |  4096,  1024,     1,     1 | F16     | blk.9.attn_k.weight\r\n    206:   16777216 |  4096,  4096,     1,     1 | F16     | blk.9.attn_output.weight\r\n    207:   16777216 |  4096,  4096,     1,     1 | F16     | blk.9.attn_q.weight\r\n    208:         64 |    64,     1,     1,     1 | F32     | blk.9.attn_rot_embd\r\n    209:    4194304 |  4096,  1024,     1,     1 | F16     | blk.9.attn_v.weight\r\n    210:  525336576 |  4096, 128256,     1,     1 | F16     | output.weight\r\n    211:   58720256 | 14336,  4096,     1,     1 | F16     | blk.20.ffn_down.weight\r\n    212:   58720256 |  4096, 14336,     1,     1 | F16     | blk.20.ffn_up.weight\r\n    213:       4096 |  4096,     1,     1,     1 | F32     | blk.21.attn_norm.weight\r\n    214:   58720256 | 14336,  4096,     1,     1 | F16     | blk.21.ffn_down.weight\r\n    215:   58720256 |  4096, 14336,     1,     1 | F16     | blk.21.ffn_gate.weight\r\n    216:   58720256 |  4096, 14336,     1,     1 | F16     | blk.21.ffn_up.weight\r\n    217:       4096 |  4096,     1,     1,     1 | F32     | blk.21.ffn_norm.weight\r\n    218:    4194304 |  4096,  1024,     1,     1 | F16     | blk.21.attn_k.weight\r\n    219:   16777216 |  4096,  4096,     1,     1 | F16     | blk.21.attn_output.weight\r\n    220:   16777216 |  4096,  4096,     1,     1 | F16     | blk.21.attn_q.weight\r\n    221:         64 |    64,     1,     1,     1 | F32     | blk.21.attn_rot_embd\r\n    222:    4194304 |  4096,  1024,     1,     1 | F16     | blk.21.attn_v.weight\r\n    223:       4096 |  4096,     1,     1,     1 | F32     | blk.22.attn_norm.weight\r\n    224:   58720256 | 14336,  4096,     1,     1 | F16     | blk.22.ffn_down.weight\r\n    225:   58720256 |  4096, 14336,     1,     1 | F16     | blk.22.ffn_gate.weight\r\n    226:   58720256 |  4096, 14336,     1,     1 | F16     | blk.22.ffn_up.weight\r\n    227:       4096 |  4096,     1,     1,     1 | F32     | blk.22.ffn_norm.weight\r\n    228:    4194304 |  4096,  1024,     1,     1 | F16     | blk.22.attn_k.weight\r\n    229:   16777216 |  4096,  4096,     1,     1 | F16     | blk.22.attn_output.weight\r\n    230:   16777216 |  4096,  4096,     1,     1 | F16     | blk.22.attn_q.weight\r\n    231:         64 |    64,     1,     1,     1 | F32     | blk.22.attn_rot_embd\r\n    232:    4194304 |  4096,  1024,     1,     1 | F16     | blk.22.attn_v.weight\r\n    233:       4096 |  4096,     1,     1,     1 | F32     | blk.23.attn_norm.weight\r\n    234:   58720256 | 14336,  4096,     1,     1 | F16     | blk.23.ffn_down.weight\r\n    235:   58720256 |  4096, 14336,     1,     1 | F16     | blk.23.ffn_gate.weight\r\n    236:   58720256 |  4096, 14336,     1,     1 | F16     | blk.23.ffn_up.weight\r\n    237:       4096 |  4096,     1,     1,     1 | F32     | blk.23.ffn_norm.weight\r\n    238:    4194304 |  4096,  1024,     1,     1 | F16     | blk.23.attn_k.weight\r\n    239:   16777216 |  4096,  4096,     1,     1 | F16     | blk.23.attn_output.weight\r\n    240:   16777216 |  4096,  4096,     1,     1 | F16     | blk.23.attn_q.weight\r\n    241:         64 |    64,     1,     1,     1 | F32     | blk.23.attn_rot_embd\r\n    242:    4194304 |  4096,  1024,     1,     1 | F16     | blk.23.attn_v.weight\r\n    243:       4096 |  4096,     1,     1,     1 | F32     | blk.24.attn_norm.weight\r\n    244:   58720256 | 14336,  4096,     1,     1 | F16     | blk.24.ffn_down.weight\r\n    245:   58720256 |  4096, 14336,     1,     1 | F16     | blk.24.ffn_gate.weight\r\n    246:   58720256 |  4096, 14336,     1,     1 | F16     | blk.24.ffn_up.weight\r\n    247:       4096 |  4096,     1,     1,     1 | F32     | blk.24.ffn_norm.weight\r\n    248:    4194304 |  4096,  1024,     1,     1 | F16     | blk.24.attn_k.weight\r\n    249:   16777216 |  4096,  4096,     1,     1 | F16     | blk.24.attn_output.weight\r\n    250:   16777216 |  4096,  4096,     1,     1 | F16     | blk.24.attn_q.weight\r\n    251:         64 |    64,     1,     1,     1 | F32     | blk.24.attn_rot_embd\r\n    252:    4194304 |  4096,  1024,     1,     1 | F16     | blk.24.attn_v.weight\r\n    253:       4096 |  4096,     1,     1,     1 | F32     | blk.25.attn_norm.weight\r\n    254:   58720256 | 14336,  4096,     1,     1 | F16     | blk.25.ffn_down.weight\r\n    255:   58720256 |  4096, 14336,     1,     1 | F16     | blk.25.ffn_gate.weight\r\n    256:   58720256 |  4096, 14336,     1,     1 | F16     | blk.25.ffn_up.weight\r\n    257:       4096 |  4096,     1,     1,     1 | F32     | blk.25.ffn_norm.weight\r\n    258:    4194304 |  4096,  1024,     1,     1 | F16     | blk.25.attn_k.weight\r\n    259:   16777216 |  4096,  4096,     1,     1 | F16     | blk.25.attn_output.weight\r\n    260:   16777216 |  4096,  4096,     1,     1 | F16     | blk.25.attn_q.weight\r\n    261:         64 |    64,     1,     1,     1 | F32     | blk.25.attn_rot_embd\r\n    262:    4194304 |  4096,  1024,     1,     1 | F16     | blk.25.attn_v.weight\r\n    263:       4096 |  4096,     1,     1,     1 | F32     | blk.26.attn_norm.weight\r\n    264:   58720256 | 14336,  4096,     1,     1 | F16     | blk.26.ffn_down.weight\r\n    265:   58720256 |  4096, 14336,     1,     1 | F16     | blk.26.ffn_gate.weight\r\n    266:   58720256 |  4096, 14336,     1,     1 | F16     | blk.26.ffn_up.weight\r\n    267:       4096 |  4096,     1,     1,     1 | F32     | blk.26.ffn_norm.weight\r\n    268:    4194304 |  4096,  1024,     1,     1 | F16     | blk.26.attn_k.weight\r\n    269:   16777216 |  4096,  4096,     1,     1 | F16     | blk.26.attn_output.weight\r\n    270:   16777216 |  4096,  4096,     1,     1 | F16     | blk.26.attn_q.weight\r\n    271:         64 |    64,     1,     1,     1 | F32     | blk.26.attn_rot_embd\r\n    272:    4194304 |  4096,  1024,     1,     1 | F16     | blk.26.attn_v.weight\r\n    273:       4096 |  4096,     1,     1,     1 | F32     | blk.27.attn_norm.weight\r\n    274:   58720256 | 14336,  4096,     1,     1 | F16     | blk.27.ffn_down.weight\r\n    275:   58720256 |  4096, 14336,     1,     1 | F16     | blk.27.ffn_gate.weight\r\n    276:   58720256 |  4096, 14336,     1,     1 | F16     | blk.27.ffn_up.weight\r\n    277:       4096 |  4096,     1,     1,     1 | F32     | blk.27.ffn_norm.weight\r\n    278:    4194304 |  4096,  1024,     1,     1 | F16     | blk.27.attn_k.weight\r\n    279:   16777216 |  4096,  4096,     1,     1 | F16     | blk.27.attn_output.weight\r\n    280:   16777216 |  4096,  4096,     1,     1 | F16     | blk.27.attn_q.weight\r\n    281:         64 |    64,     1,     1,     1 | F32     | blk.27.attn_rot_embd\r\n    282:    4194304 |  4096,  1024,     1,     1 | F16     | blk.27.attn_v.weight\r\n    283:       4096 |  4096,     1,     1,     1 | F32     | blk.28.attn_norm.weight\r\n    284:   58720256 | 14336,  4096,     1,     1 | F16     | blk.28.ffn_down.weight\r\n    285:   58720256 |  4096, 14336,     1,     1 | F16     | blk.28.ffn_gate.weight\r\n    286:   58720256 |  4096, 14336,     1,     1 | F16     | blk.28.ffn_up.weight\r\n    287:       4096 |  4096,     1,     1,     1 | F32     | blk.28.ffn_norm.weight\r\n    288:    4194304 |  4096,  1024,     1,     1 | F16     | blk.28.attn_k.weight\r\n    289:   16777216 |  4096,  4096,     1,     1 | F16     | blk.28.attn_output.weight\r\n    290:   16777216 |  4096,  4096,     1,     1 | F16     | blk.28.attn_q.weight\r\n    291:         64 |    64,     1,     1,     1 | F32     | blk.28.attn_rot_embd\r\n    292:    4194304 |  4096,  1024,     1,     1 | F16     | blk.28.attn_v.weight\r\n    293:       4096 |  4096,     1,     1,     1 | F32     | blk.29.attn_norm.weight\r\n    294:   58720256 | 14336,  4096,     1,     1 | F16     | blk.29.ffn_down.weight\r\n    295:   58720256 |  4096, 14336,     1,     1 | F16     | blk.29.ffn_gate.weight\r\n    296:   58720256 |  4096, 14336,     1,     1 | F16     | blk.29.ffn_up.weight\r\n    297:       4096 |  4096,     1,     1,     1 | F32     | blk.29.ffn_norm.weight\r\n    298:    4194304 |  4096,  1024,     1,     1 | F16     | blk.29.attn_k.weight\r\n    299:   16777216 |  4096,  4096,     1,     1 | F16     | blk.29.attn_output.weight\r\n    300:   16777216 |  4096,  4096,     1,     1 | F16     | blk.29.attn_q.weight\r\n    301:         64 |    64,     1,     1,     1 | F32     | blk.29.attn_rot_embd\r\n    302:    4194304 |  4096,  1024,     1,     1 | F16     | blk.29.attn_v.weight\r\n    303:       4096 |  4096,     1,     1,     1 | F32     | blk.30.attn_norm.weight\r\n    304:   58720256 | 14336,  4096,     1,     1 | F16     | blk.30.ffn_down.weight\r\n    305:   58720256 |  4096, 14336,     1,     1 | F16     | blk.30.ffn_gate.weight\r\n    306:   58720256 |  4096, 14336,     1,     1 | F16     | blk.30.ffn_up.weight\r\n    307:       4096 |  4096,     1,     1,     1 | F32     | blk.30.ffn_norm.weight\r\n    308:    4194304 |  4096,  1024,     1,     1 | F16     | blk.30.attn_k.weight\r\n    309:   16777216 |  4096,  4096,     1,     1 | F16     | blk.30.attn_output.weight\r\n    310:   16777216 |  4096,  4096,     1,     1 | F16     | blk.30.attn_q.weight\r\n    311:         64 |    64,     1,     1,     1 | F32     | blk.30.attn_rot_embd\r\n    312:    4194304 |  4096,  1024,     1,     1 | F16     | blk.30.attn_v.weight\r\n    313:       4096 |  4096,     1,     1,     1 | F32     | blk.31.attn_norm.weight\r\n    314:   58720256 | 14336,  4096,     1,     1 | F16     | blk.31.ffn_down.weight\r\n    315:   58720256 |  4096, 14336,     1,     1 | F16     | blk.31.ffn_gate.weight\r\n    316:   58720256 |  4096, 14336,     1,     1 | F16     | blk.31.ffn_up.weight\r\n    317:       4096 |  4096,     1,     1,     1 | F32     | blk.31.ffn_norm.weight\r\n    318:    4194304 |  4096,  1024,     1,     1 | F16     | blk.31.attn_k.weight\r\n    319:   16777216 |  4096,  4096,     1,     1 | F16     | blk.31.attn_output.weight\r\n    320:   16777216 |  4096,  4096,     1,     1 | F16     | blk.31.attn_q.weight\r\n    321:         64 |    64,     1,     1,     1 | F32     | blk.31.attn_rot_embd\r\n    322:    4194304 |  4096,  1024,     1,     1 | F16     | blk.31.attn_v.weight\r\n    323:       4096 |  4096,     1,     1,     1 | F32     | output_norm.weight\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-02T16:02:28+00:00",
    "closed_at": "2024-05-02T23:49:10+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7046/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7046"
  },
  {
    "number": 266,
    "title": "Prevent user from setting a context size that is too big",
    "body": "Hey!\r\n\r\nI tasked the 30B model to write a little story... it worked really well until some point where it went off rails from one line to the next, suddenly talking about some girl and stuff that has nothing to do with the rest:\r\n\r\n```\r\nThe way out of me that started looking at them. It'ould be lying there was standing near-the first time what could see an older than the girl had held they looked like it, and just how hard. In order I wasn't really when my hands on his head down to myself in front seat and the car door were with me before you.\r\n\u201cI realy as she staring that laying to a moment of him. \"It was so lying next to about two, but it looked at her eyes had already when there looking for holding my hand from what I'with his head was on both shoulders. And not through and suddenly, he realized 212.\r\nI couldn\u2019t with the car seat, in fronted again because of one. The second, so that didn'sit seems like a young girl sitting me when \"We weren near. But I started. 'mom. Withered and to drive my legs.\r\nT was not asnthe right now. It looked and suddenly. That it has the car on fire in fronted from this wasn\u20192, butt before you, The most as he cried me at 13rd for four years were there were looking: I had taken when I was me and then. My younger\r\nI'6 was only so much years and a friend in my age, nott. Was the because him event and our while to from his 'the, the year? The right \"didn as you he before, that after.\r\n\"\r\nas this and\r\nbut?\" I? It\r\nThe way, but then of tapered\r\nand the. What and had: As a\r\n\u201cwas on myt was, not-I with me when 'd because trying while my so for it - The A F 1 to as you \" W We from what in f and your S C I ( A\r\n I A G In F ... For T E In The. It the or! There You O I This I\r\n My of, The M D P * I [ a K When R N L S At The So B D  D  I If V and \" W s S the to\r\n  I ( ( \" , \u201c All as G A The D \" O A A \" \" F T \" W C The In W in E S I As I \" M The * In I I For a R H - The that At  D ...\r\n L N the In I You We [ B and U A I  This\r\n for ( so \" of F as D Re The A I I  K ( New O F G The  C  D E S All \u201c P I V B \" The * W If in O a with and For I \" R Le C C H At Al ( A We In F L When W I ... It [ T A Are C What  A I N En  W As \" K  \" The Ch I I - So Our S M  This . The D W P A V J\r\n * A You A O \" in G If the * D \u2013 I  and W At \u201c We B Do F In E S ( a  T It All With For L R I Le The , En A to  There K What The As U When Ch The S A - [  An M of S A S So C In ( ( V You A  * Are O If the A T After New \" This ( We D as W I F G Al  . \u201c H in W ... All R I The In \u2013 A On E It a In B The We P What  A L\r\n A This A When and [ M As S For  A The The Our  En C Are K In V W O You The The \" ( Le U \" N So There In This F The If . * The as \" * I T S At \u201c A D The With This B \u2013  The H A The The D G ... the ( We W  When [ All L It W W P \" We * What As If C Is  O  The Our M Are \" U The R There W A   K E The  The You For I In A F F C The I ( in S After  The This \u201c A W I At D A \" A With H I C  One So L I On A N [ P We The As B I * This G The I The - I ...  It V If and \u2013 When The  There I  A For In M A En T O A The How New Do ( K The I After Our  \u201c \" S . the E of At W [ D F A A Are D A C C With , The So U An W This We S The On  F N * What G If V P R As ... ( All - For *  There B You We This A ( In I When The M Do In  L W \"  \u201c How En . T At The [ The O  Are A It The ( J F In \u2013 and C K [ Our S U * D Looking the a N Are This With Re If H P As A The On An The  At There The F E The I All When A You B The M Q S R The The W \" , One In We S A The \u201c  L in The ... I  - In We In It [ V After Are In G and ( The U O C   In The A H A D En P For If A An In S F \" S New The Re I Our W  the B K C I * You  If As This The At D W A S The  We All ... M So - T * The With \u2013 \" The  and ( It [ Q The C The N This L I A A When O En If Are U In D V in On  P G H In R S W Le The F . An A K The As W One At ( , For  There the A J Al I F A \" ( If B Our The At The A * All [ T We C \u201c M In This A C - I It If ... I O This a Re You D N In W S F When Are The I The and A P V \" Do The K Are U Q The W A  A With F The G If The the \" In So Our An En At ( The S A Are L The C ( There Are The * B In We A A [ For It Are The  In The C This At O You This  D E   This I C P In V The K Do - A All One T When F As This The If D This The F ( a W ( With M W \" N S W \u2013 L This The D There Our This In the B En This It  A Are  ( *  J The We At For Are and On The  Q R We G O U So A (  I A C V I In The C T If The [ P At , A You This K  The In One S A C The I \" The There - L E W Our D En N We With When The The It Do The A A At W and A H F \u201c What As Are For  in G  All F Are C The The *  the T ( O  U If A  On \u2013 F K Al In This At R B So I \" \"  M US This S \"  Our En  J ( When [ The . You A E D L P W The It - A N  In D \" If In ( As One C The W A * We Are T the \" Re W \u201c G All ... C  F If A At After O So I ( If For M Q R This The Do \u2013 [  With In S B At The A A The K H The There We [ L The W - in U  It The N I  At D F This D A I * The W [ , En When A \" Are C What *  F All *\r\n```\r\n\r\nThe model is quantized (q4_0) and I am on Linux (x86_64) with 64 GB of RAM.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T15:11:33+00:00",
    "closed_at": "2023-03-19T10:33:41+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/266"
  },
  {
    "number": 3825,
    "title": "llama_kv_cache_seq_shift delta does not appear to be calculated properly",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [Y] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [Y] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [Y] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [Y] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n\r\nNot 100% certain if this is a bug or not, but I was playing with the kv cache shifting functionality and I was getting some weird results so I figured I'd step through it and see what was going on.\r\n\r\nI noticed that after performing a double shift on a chunk of the kv cache, that the cell delta only reflected the second shift. The cell positions are properly updated, however. It looks like the shift code treats the cell pos differently than the cell delta.\r\n\r\nSee `cache.cells[i].pos += delta` vs. `cache.cells[i].delta = delta`\r\n\r\nThe position is cumulative, but the delta maintains the value of the last shift, which throws the position out of sync with the delta once its been shifted more than once. \r\n\r\nOf course, the example in main.cpp discards half the disposable context on every shift, so if my understanding is correct, this isn't something that would be noticed through normal usage of the \"main.cpp\" application. A block of the KV cache wouldn't be shifted twice using that code in the first place, since the second shift would dispose of the block that was moved during the first shift.\r\n\r\nSo at least superficially it looks like this might be an oversight that wouldn't have come up without directly calling the API. I cant think of why the delta would be fixed to the last shift operation instead of cumulative like the position either. I figured I would make a note of this here in case this was an actual issue and not intentional\r\n\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-10-28T06:47:12+00:00",
    "closed_at": "2023-10-29T16:32:52+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3825/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3825"
  },
  {
    "number": 4463,
    "title": "Adding MistralForCausalLM architecture to convert-hf-to-gguf.py",
    "body": "Hi.\r\n\r\nI'm trying to deploy [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) locally, as the documentation mentions it's supported, but it fails to generate the GGUF model file.\r\n\r\nThe error is `NotImplementedError: Architecture \"MistralForCausalLM\" not supported!`\r\n\r\nAs using GGUF files is a breaking change and the Mistral-7B model should be supported, I think adding support for `MistralForCausalLM` architecture to `convert-hf-to-gguf.py` is essential.\r\n\r\nI'm running the latest version as of Dec 14, 2023, which is `b1637`\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-14T10:00:26+00:00",
    "closed_at": "2024-05-10T01:28:38+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4463/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4463"
  },
  {
    "number": 5981,
    "title": "llama : add Deepseek support",
    "body": "Support is almost complete. There is a dangling issue with the pre-tokenizer: https://github.com/ggerganov/llama.cpp/pull/7036\r\n\r\nA useful discussion related to that is here: https://github.com/ggerganov/llama.cpp/discussions/7144\r\n\r\n-----\r\n\r\n## Outdated below\r\n\r\nCreating this issue for more visibility\r\n\r\nThe main problem is around tokenization support, since the models use some variation of the BPE pre-processing regex. There are also some issues with the conversion scripts.\r\n\r\nAnyway, looking for contributions to help with this\r\n\r\nPrevious unfinished work:\r\n\r\n- #4070 \r\n- #5464 \r\n\r\nPossible implementation plan: https://github.com/ggerganov/llama.cpp/pull/5464#issuecomment-1974818993",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-03-10T18:56:56+00:00",
    "closed_at": "2024-05-08T17:03:57+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5981/reactions",
      "total_count": 14,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5981"
  },
  {
    "number": 7757,
    "title": "Bug: `-ins` command gone from main.exe",
    "body": "### What happened?\n\nThere seems to be no way to activate instruct mode in main.exe, this causes my scripts to break.\n\n### Name and Version\n\nversion: 3089 (c90dbe02)\r\nbuilt with MSVC 19.39.33523.0 for x64\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nerror: unknown argument: -ins\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-05T03:00:56+00:00",
    "closed_at": "2024-06-05T07:03:06+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7757/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7757"
  },
  {
    "number": 42,
    "title": "Maybe lower default temp and switch to top_k 40",
    "body": "Per [this twitter thread](https://twitter.com/theshawwn/status/1632569215348531201). See commit [here](https://github.com/shawwn/llama/commit/40d99d329a5e38d85904d3a6519c54e6dd6ee9e1).",
    "labels": [
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-12T10:12:43+00:00",
    "closed_at": "2023-03-13T17:26:16+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/42/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/42"
  },
  {
    "number": 2047,
    "title": "Question: How to access feature vector of the intermediate layer of network?",
    "body": "# Prerequisites\r\n\r\n- [Yes] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [Yes] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [Yes] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [Yes] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI am interested in the difference between the feature vectors of the intermediate layer of the `llama.cpp` and PyTorch versions of the LLaMa model.\r\nFor this purpose, I would like to know how I can get the feature vectors of the middle layer, such as\r\n`torchvision.models.feature_extraction.create_feature_extractor` and `register_forward_hook` method in PyTorch.\r\n\r\n# Current Behavior\r\n\r\nI browsed C++ programs but could not figure out how to get the feature vector.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-29T06:31:32+00:00",
    "closed_at": "2024-04-09T01:08:36+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2047/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2047"
  },
  {
    "number": 6715,
    "title": "Feature request: Graphical GGUF viewer",
    "body": "# Motivation\r\n\r\nWith the recent introduction of `eval-callback` example, we now having more tools for debugging when working with llama.cpp. However, one of the tool that I feel missing is the ability to dump everything inside a gguf file into a human-readable (and interactive) interface.\r\n\r\nInspired from `huggingface.js` where users can visualize the KV and list of tensors on huggingface.com, I would like to implement the same thing in llama.cpp. I find this helpful in these situations:\r\n- Debugging `convert.py` script when adding a new architecture\r\n- Debugging tokenizers\r\n- Debugging changes related to gguf (model splits for example)\r\n- Debugging tensors (i.e. display N first elements of a tensor, just like `eval-callback`)\r\n- Debugging control vectors\r\n- ... (maybe other usages in the future)\r\n\r\nThe reason why I can't use `huggingface.js` is because it's based on browser, which make it tricky when reading a huge local file. It also don't have access to quantized types (same for `gguf-py`).\r\n\r\n# Possible Implementation\r\n\r\nIdeally, I want the implementation to be a binary named `gguf-viewer` that when run, will open a web page in `localhost:8080`. User can then go to the web page to explore the gguf file. It will have these sections:\r\n- Complete list of KV\r\n- Tokenizer-related info (for example: list all tokens, lookup one token)\r\n- List of all tensors",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-04-17T04:30:46+00:00",
    "closed_at": null,
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6715/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6715"
  },
  {
    "number": 4016,
    "title": "Finetune GPU Utilization fell to 0%",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nfinetune.cpp should fully utilize the GPU. We were trying to finetune Llama2-7b 16 bit model with a sample dataset of 1000 samples.\r\n\r\n# Current Behavior\r\nWhen running finetune and offloading all layers to the GPU, the GPU was utilized at around 30% at the beginning of processing each batch and fell to 0% until the batch was finished.\r\n\r\n# Environment and Context\r\nWe use GCP instance of a2-highgpu-1g | 1 GPU | 40 GB HBM2 | 12 vCPUs | 85 GB. It has Nvidia A100 40GB GPU.\r\n\r\n\r\n* Physical (or virtual) hardware you are using\r\n```\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             12\r\nOn-line CPU(s) list:                0-11\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 6\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\r\nStepping:                           7\r\nCPU MHz:                            2200.170\r\nBogoMIPS:                           4400.34\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          192 KiB\r\nL1i cache:                          192 KiB\r\nL2 cache:                           6 MiB\r\nL3 cache:                           38.5 MiB\r\nNUMA node0 CPU(s):                  0-11\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`Linux instance-1 5.10.0-25-cloud-amd64 #1 SMP Debian 5.10.191-1 (2023-08-16) x86_64 GNU/Linux`\r\n\r\n* SDK version\r\n\r\n```\r\n$ Python 3.10.12\r\n$ cmake version 3.18.4\r\n$ g++ (Debian 10.2.1-6) 10.2.1 20210110\r\n```\r\n\r\n# Failure Information \r\n# Steps to Reproduce\r\n\r\n1. Build command\r\n`cmake .. -DLLAMA_CUBLAS=ON && cmake --build . --config Release -j8`\r\n\r\n2. Finetune\r\n`./finetune --model-base ../../../Llama-2-7b-32.gguf --lora-out ./loraout.bin --train-data ../train-sample.txt --save-every 1 --threads 12 --adam-iter 30 --batch 4 --ctx 1100  -ngl 35 --no-checkpointing --epochs 3 --checkpoint-out ./check-ITERATION.gguf --sample-start \"<s>\"`\r\n\r\n# Logs\r\n```\r\n./finetune --model-base ../../../Llama-2-7b.gguf --lora-out ./loraout.bin --train-data ../train-sample.txt --save-every 1 --threads 12 --adam-iter 30 --batch 4 --ctx 1100  -ngl 35 --no-checkpointing --epochs 3 --checkpoint-out ./check-ITERATION.gguf --sample-start \"<s>\"\r\nmain: seed: 1699603790\r\nmain: model base = '../../../Llama-2-7b.gguf'\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0\r\nllama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from ../../../Llama-2-7b.gguf (version GGUF V3 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   65:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   66:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   67:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   69:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   70:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   71:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   72:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   74:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   75:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   76:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   78:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   79:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   80:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   81:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   83:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   84:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   85:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   87:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   88:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   89:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   90:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   92:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   93:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   94:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   96:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   97:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   98:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   99:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  101:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  102:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  103:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  105:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  106:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  107:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  108:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  110:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  111:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  112:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  114:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  115:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  116:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  117:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  119:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  120:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  121:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  123:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  124:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  125:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  126:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  128:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  129:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  130:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  132:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  133:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  134:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  135:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  137:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  138:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  139:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  142:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  143:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  144:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  146:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  147:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  148:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  151:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  152:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  153:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  155:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  156:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  157:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  159:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  160:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  161:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  162:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  164:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  165:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  166:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  168:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  169:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  170:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  171:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  173:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  174:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  175:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  177:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  178:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  179:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  180:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  182:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  183:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  184:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  186:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  187:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  188:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  189:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  191:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  192:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  193:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  195:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  196:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  197:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  198:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  200:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  201:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  202:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  204:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  205:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  206:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  207:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  209:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  210:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  211:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  213:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  214:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  215:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  216:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  217:                    output.weight f16      [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  219:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  220:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  221:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  223:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  224:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  225:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  226:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  228:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  229:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  230:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  232:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  233:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  234:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  235:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  237:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  238:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  239:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  241:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  242:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  243:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  244:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  246:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  247:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  248:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  250:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  251:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  252:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  253:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  255:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  256:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  257:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  259:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  260:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  261:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  262:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  264:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  265:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  266:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  268:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  269:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  270:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  271:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  273:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  274:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  275:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  277:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  278:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  279:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  280:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str     \r\nllama_model_loader: - kv   1:                               general.name str     \r\nllama_model_loader: - kv   2:                       llama.context_length u32     \r\nllama_model_loader: - kv   3:                     llama.embedding_length u32     \r\nllama_model_loader: - kv   4:                          llama.block_count u32     \r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \r\nllama_model_loader: - kv  10:                          general.file_type u32     \r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str     \r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type  f16:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = mostly F16\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: mem required  =  250.11 MB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 35/35 layers to GPU\r\nllm_load_tensors: VRAM used: 12603.02 MB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init: offloading v cache to GPU\r\nllama_kv_cache_init: offloading k cache to GPU\r\nllama_kv_cache_init: VRAM kv self = 256.00 MB\r\nllama_new_context_with_model: kv self size  =  256.00 MB\r\nllama_build_graph: non-view tensors processed: 740/740\r\nllama_new_context_with_model: compute buffer total size = 77.13 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 70.50 MB\r\nllama_new_context_with_model: total VRAM used: 12929.52 MB (model: 12603.02 MB, context: 326.50 MB)\r\nmain: init model\r\nprint_params: n_vocab:   32000\r\nprint_params: n_ctx:     1100\r\nprint_params: n_embd:    4096\r\nprint_params: n_ff:      11008\r\nprint_params: n_head:    32\r\nprint_params: n_head_kv: 32\r\nprint_params: n_layer:   32\r\nprint_params: norm_rms_eps          : 0.000010\r\nprint_params: rope_freq_base        : 10000.000000\r\nprint_params: rope_freq_scale       : 1.000000\r\nprint_lora_params: n_rank_attention_norm : 1\r\nprint_lora_params: n_rank_wq             : 4\r\nprint_lora_params: n_rank_wk             : 4\r\nprint_lora_params: n_rank_wv             : 4\r\nprint_lora_params: n_rank_wo             : 4\r\nprint_lora_params: n_rank_ffn_norm       : 1\r\nprint_lora_params: n_rank_w1             : 4\r\nprint_lora_params: n_rank_w2             : 4\r\nprint_lora_params: n_rank_w3             : 4\r\nprint_lora_params: n_rank_tok_embeddings : 4\r\nprint_lora_params: n_rank_norm           : 1\r\nprint_lora_params: n_rank_output         : 4\r\nmain: total train_iterations 0\r\nmain: seen train_samples     0\r\nmain: seen train_tokens      0\r\nmain: completed train_epochs 0\r\nmain: lora_size = 84845152 bytes (80.9 MB)\r\nmain: opt_size  = 126592960 bytes (120.7 MB)\r\nmain: opt iter 0\r\nmain: input_size = 563217632 bytes (537.1 MB)\r\nmain: compute_size = 80585396608 bytes (76852.2 MB)\r\nmain: evaluation order = RIGHT_TO_LEFT\r\nmain: tokenize training data\r\ntokenize_file: warning: found 42 samples (max length 1989) that exceed context length of 1100. samples will be cut off.\r\ntokenize_file: warning: found 958 samples (min length 32) that are shorter than context length of 1100.\r\ntokenize_file: total number of samples: 1000\r\nmain: number of training tokens: 356967\r\nmain: number of unique tokens: 12763\r\nmain: train data seems to have changed. restarting shuffled epoch.\r\nmain: begin training\r\nmain: work_size = 1536784 bytes (1.5 MB)\r\ntrain_opt_callback: iter=     0 sample=1/1000 sched=0.000000 loss=0.000000 |->\r\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-10T08:17:21+00:00",
    "closed_at": "2024-04-02T01:11:46+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4016/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4016"
  },
  {
    "number": 8469,
    "title": "llama-cli chat templates ignored?",
    "body": "`llama-cli -c 1024 -t 6 -m codegeex4-all-9b.q4_k.gguf -p \"You are my assistant.\" -e -cnv --chat-template chatml`\r\n\r\n```\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to the AI.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\n<|im_start|>system\r\nYou are my assistant.<|im_end|>\r\n\r\n> Hello.\r\nHello! How can I assist you today?\r\n<|im_end|>\r\n<|im_start|>user\r\nI'm a developer and I want [....]\r\n```\r\nand it continues by itself.\r\nwhat am I missing?\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-13T14:15:48+00:00",
    "closed_at": "2024-09-09T01:07:19+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8469"
  },
  {
    "number": 4331,
    "title": "Qwen-72B-Chat conversion script does not treat <|im_start|> and <|im_end|> correctly.",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Description\r\n\r\n(This is specifically for the latest 72B models. I have never tried the smaller ones).\r\n\r\nI'm using this model: https://huggingface.co/Qwen/Qwen-72B-Chat\r\n\r\nCommit: `33e171d1e9fc4903f9314b490d77fb8d58331b63`\r\n\r\nI think the current `convert-hf-to-gguf.py` does not produce a `.gguf` file that treats these two tokens correctly for `<|im_start|>` and `<|im_end|>`.\r\n\r\nThe prompt I used is \"<|im_start|>system\" for the examples below.\r\n\r\nFollowing steps in https://github.com/ggerganov/llama.cpp/pull/4281 to produce some `.gguf` files (I personally used the Q6_K on a MacStudio) I tried the `tokenize` tool:\r\n\r\n```\r\n    27 -> '<'\r\n    91 -> '|'\r\n   318 -> 'im'\r\n  4906 -> '_start'\r\n    91 -> '|'\r\n    29 -> '>'\r\n  8948 -> 'system'\r\n```\r\n\r\nCompare this to a Yi model with exact same prompt:\r\n\r\n```\r\n     6 -> '<|im_start|>'\r\n 10707 -> 'system'\r\n```\r\n\r\nI saw the Qwen model code (https://huggingface.co/Qwen/Qwen-72B/blob/main/tokenization_qwen.py#L37) and I think these are intended to be single tokens. But the current script does not handle it properly.\r\n\r\n# Steps to Reproduce\r\n\r\n1. Download the Qwen models. (https://huggingface.co/Qwen/Qwen-72B-Chat)\r\n2. Use the `convert-hf-to-gguf.py` script to convert one into a `.gguf` file. (This is the exact command I found on my Mac Studio: `python3 convert-hf-to-gguf.py --outfile /Volumes/T9/qwen_72b_chat_v3_f16.gguf --outtype f16 ~/text-generation-webui/models/Qwen_Qwen-72B-Chat`)\r\n3. Run `tokenize` on them to see what tokens are interpreted.\r\n\r\nIf I'm honest, I'm not sure if this would be a bug to `llama.cpp` repository or something Qwen team might want to fix in their repo. But I'm submitting it here for awareness.\r\n\r\nAlso, the model seems to work fine despite this. But maybe it would work better if they were interpreted correctly? No idea.",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-04T23:16:13+00:00",
    "closed_at": "2024-04-08T01:06:38+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4331/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4331"
  },
  {
    "number": 7922,
    "title": "converting phi-3-small error.",
    "body": "`python llama.cpp/convert-hf-to-gguf.py --outtype f16 --outfile /content/Phi-3-small-128k-instruct.f16.gguf /content/Phi-3-small-128k-instruct`\r\n\r\n```\r\nINFO:hf-to-gguf:Loading model: Phi-3-small-128k-instruct\r\nERROR:hf-to-gguf:Model Phi3SmallForCausalLM is not supported\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-13T17:17:16+00:00",
    "closed_at": "2024-08-14T01:06:57+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7922/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7922"
  },
  {
    "number": 9202,
    "title": "Feature Request: support embedding stella_en_400M and stella_en_400M.gguf conversion",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nNeed help supporting stella_en_400M, observed that we have embedding model\r\nhttps://ollama.com/Losspost/stella_en_1.5b_v5\r\n\r\nbut there I couldn't convert stella_en_400M myself\r\n\r\nModel Download:\r\nhttps://hf.rst.im/dunzhang/stella_en_400M_v5\r\n\r\nD:\\llama.cpp>python convert_hf_to_gguf.py d:/llama.cpp/stella_en_400M_v5 --outfile stella_en_400M.gguf --outtype q8_0\r\nINFO:hf-to-gguf:Loading model: stella_en_400M_v5\r\nERROR:hf-to-gguf:Model NewModel is not supported\n\n### Motivation\n\nTo have better embedding model\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-27T15:14:55+00:00",
    "closed_at": "2024-12-03T01:07:39+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9202/reactions",
      "total_count": 17,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9202"
  },
  {
    "number": 7924,
    "title": "Bug: Error while converting BERT to GGUF: Can not map tensor 'bert.embeddings.LayerNorm.beta'",
    "body": "### What happened?\r\n\r\nI am trying to convert the [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) HuggingFace model to GGUF with [convert-hf-to-gguf.py](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py) Unfortunately, it fails to convert because the script looks for `embeddings.position_embeddings`, etc. in [tensor-mapping.py](https://github.com/ggerganov/llama.cpp/blob/172c8256840ffd882ab9992ecedbb587d9b21f15/gguf-py/gguf/tensor_mapping.py#L44) but not `bert.embeddings.position_embeddings`, etc. This is important because most of the tensor names in the model start with `bert`.\r\n\r\nThere is a similar issue in [modify-tensors](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py#L2192). It does not skip the `cls` tensors that are present in [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased), so it fails in the same way.\r\n\r\nFinally, the bert-base-uncased `config.json` has its architecture set to `BertForMaskedLM`. But unless I change this to `BertModel` the script will return `ERROR:hf-to-gguf:Model BertForMaskedLM is not supported`.\r\n\r\nI have modified [write-tensors](https://github.com/ggerganov/llama.cpp/blob/172c8256840ffd882ab9992ecedbb587d9b21f15/convert-hf-to-gguf.py#L247) to drop 'bert.' from each name, and modified [modify-tensors](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py#L2192) to ignore names with `cls.` in them. Re-running the script will output \"Model successfully exported\", but when I try to generate an embedding with the model on the `llama-cli` I get this subsequent error:\r\n`llama_model_load: error loading model: check_tensor_dims: tensor 'token_embd_norm.weight' not found`. \r\n\r\n### Name and Version\r\n\r\nversion: 3051 (5921b8f0)\r\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Steps to Reproduce\r\n\r\nClone [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) and run the convert script on it.\r\nTo see my attempted solution, do these steps:\r\n\r\n1. Change `BertForMaskedLM` to `BertModel` in `bert-base-uncased/config.json`.\r\n2. Run the convert script in [my fork](https://github.com/Wheelspawn/llama.cpp) of llama.cpp on the directory.\r\n3. Run `./llama-cli -m` on the converted model.\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n$ python convert-hf-to-gguf.py /home/nsage/bert-base-uncased/ --outtype f16 --outfile /home/nsage/bert-base-uncased/bert_converted.gguf\r\nINFO:hf-to-gguf:Loading model: bert-base-uncased\r\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\r\nINFO:hf-to-gguf:Set model parameters\r\nINFO:hf-to-gguf:gguf: context length = 512\r\nINFO:hf-to-gguf:gguf: embedding length = 768\r\nINFO:hf-to-gguf:gguf: feed forward length = 3072\r\nINFO:hf-to-gguf:gguf: head count = 12\r\nINFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12\r\nINFO:hf-to-gguf:gguf: file type = 1\r\nINFO:hf-to-gguf:Set model tokenizer\r\nINFO:gguf.vocab:Setting special token type pad to 0\r\nINFO:hf-to-gguf:Exporting model to '/home/nsage/bert-base-uncased/bert_converted.gguf'\r\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\r\nTraceback (most recent call last):\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 2862, in <module>\r\n    main()\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 2856, in main\r\n    model_instance.write()\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 328, in write\r\n    self.write_tensors()\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 265, in write_tensors\r\n    for new_name, data in ((n, d.squeeze().numpy()) for n, d in self.modify_tensors(data_torch, name, bid)):\r\n                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 2190, in modify_tensors\r\n    return [(self.map_tensor_name(name), data_torch)]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nsage/ollama/llm/llama.cpp/convert-hf-to-gguf.py\", line 180, in map_tensor_name\r\n    raise ValueError(f\"Can not map tensor {name!r}\")\r\nValueError: Can not map tensor 'bert.embeddings.LayerNorm.beta'\r\n```\r\n\r\n```\r\n$ ./llama-cli -m /home/nsage/bert-base-uncased/ggml-model-Q4_0.gguf -p \"The sky is blue.\"\r\n...\r\nllama_model_load: error loading model: check_tensor_dims: tensor 'token_embd_norm.weight' not found\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '/home/nsage/bert-base-uncased/ggml-model-Q4_0.gguf'\r\nmain: error: unable to load model\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-06-13T19:36:58+00:00",
    "closed_at": "2024-07-28T01:07:05+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7924/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7924"
  },
  {
    "number": 9440,
    "title": "Feature Request: Pixtral by Mistral support (pixtral-12b-240910)",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nDear llama.cpp team,\r\n\r\nMistral has just released Pixtral and I would like to request support for it, if possible.\r\n\r\nHere are some relevant links:\r\n\r\n**X announcement:** https://x.com/mistralai/status/1833758285167722836\r\n\r\n**Magnet link:** `xt=urn:btih:7278e625de2b1da598b23954c13933047126238a&dn=pixtral-12b-240910&tr=udp%3A%2F%http://2ftracker.opentrackr.org/%3A1337%2Fannounce&tr=udp%3A%2F%http://2fopen.demonii.com/%3A1337%2Fannounce&tr=http%3A%2F%http://2ftracker.ipv6tracker.org/%3A80%2Fannounce`\r\n\r\n**HuggingFace alternative download link:** https://huggingface.co/mistral-community/pixtral-12b-240910\r\n\r\n**Additional information:** https://github.com/mistralai/mistral-common/releases/tag/v1.4.0\r\n\r\n**Important notes from the readme:** \r\n- Use GELU for the vision adapter\r\n- Use 2D ROPE for the vision encoder\r\n\r\n---------------\r\n\r\nThank you for your time and consideration!",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-11T18:03:29+00:00",
    "closed_at": "2025-02-08T01:07:14+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9440/reactions",
      "total_count": 145,
      "+1": 117,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 28,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9440"
  },
  {
    "number": 6868,
    "title": "Support for OpenELM of Apple",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nSupport for OpenELM of Apple\r\n\r\nhttps://huggingface.co/apple/OpenELM-3B-Instruct/tree/main\r\n",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-04-24T08:10:16+00:00",
    "closed_at": "2024-07-04T17:14:22+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6868/reactions",
      "total_count": 48,
      "+1": 48,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6868"
  },
  {
    "number": 13405,
    "title": "Misc. bug: llama-sampling.cpp:204: GGML_ASSERT(cur_p->size > 0) failed",
    "body": "### Name and Version\n\n```\n$./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nversion: 5329 (611aa914)\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\n```\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-cli\n\n### Command line\n\n```shell\nllama-cli \\\n    --log-file /tmp/llamacpp-Qwen3-30B-A3B-Q8_K_XL.log \\\n    --hf-repo unsloth/Qwen3-30B-A3B-GGUF:Q8_K_XL \\\n    --override-tensor '([0-9]+).ffn_.*_exps.=CPU' \\\n    --n-gpu-layers 48 \\\n    --jinja \\\n    --cache-type-k q8_0 \\\n    --ctx-size 32768 \\\n    --samplers \"top_k;dry;min_p;temperature;top_p\" \\\n    --min-p 0.005 \\\n    --top-p 0.97 \\\n    --top-k 40 \\\n    --temp 0.7 \\\n    --dry-multiplier 0.7 \\\n    --dry-allowed-length 4 \\\n    --dry-penalty-last-n 2048 \\\n    --presence-penalty 0.05 \\\n    --frequency-penalty 0.005 \\\n    --repeat-penalty 1.01 \\\n    --repeat-last-n 16 \\\n    --verbose \\\n    --file generic-prompt-for-testing-1906words.txt\n```\n\n### Problem description & steps to reproduce\n\nThe log file of the output, together with what I hope is all the relevant information can be found in this ephemeral repo I put up for this bug report:\nhttps://github.com/bjodah/bug-reproducer-llamacpp-assert-triggering/tree/main\n\nIt might very well that I'm doing something awfully wrong here, but since it's an assert that is triggering, I'm thinking that you might be interested in a bug report?\n\nI first observed this error using llama-serve on my laptop (ubuntu 24.04, geforce 1050 mobile), but everything in this bug report was reproduced on a more modern system (debian, geforce rtx 3090).\n\n### First Bad Commit\n\nQwen 3 support is pretty recent, so I haven't figured out what's the relevant oldest commit for a bisection.\n\n### Relevant log output\n\n```shell\n/... lots of output, see log file in repo linked in issue description .../ \neval: [ 'G':38 ]\nGn_past = 2620\n/home/bjorn/vc/llama.cpp/src/llama-sampling.cpp:204: GGML_ASSERT(cur_p->size > 0) failed\n/home/bjorn/.gdbinit:2: Error in sourced command file:\n/home/bjorn/dotfiles/per-file/.gdbinit:22: Error in sourced command file:\nScripting in the \"Python\" language is not supported in this copy of GDB.\nptrace: Operation not permitted.\nNo stack.\nThe program is not being run.\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-09T14:05:48+00:00",
    "closed_at": "2025-05-27T09:07:54+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13405/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13405"
  },
  {
    "number": 7060,
    "title": "llava 1.5 invalid output after first inference (llamacpp server)",
    "body": "I use this server config:\r\n```{\r\n    \"host\": \"0.0.0.0\",\r\n    \"port\": 8085,\r\n    \"api_key\": \"api_key\",\r\n    \"models\": [\r\n        {\r\n            \"model\": \"models/phi3_mini_model/phi3_mini_model.gguf\",\r\n            \"model_alias\": \"gpt-3.5-turbo\",\r\n            \"chat_format\": \"chatml\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 2048\r\n        },\r\n        {\r\n            \"model\": \"models/phi3_mini_model/phi3_mini_model.gguf\",\r\n            \"model_alias\": \"gpt-4\",\r\n            \"chat_format\": \"chatml\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 4096\r\n        },\r\n        {\r\n            \"model\": \"models/llava15_vision_model/ggml-model-q4_k.gguf\",\r\n            \"model_alias\": \"gpt-4-vision-preview\",\r\n            \"chat_format\": \"llava-1-5\",\r\n            \"clip_model_path\": \"models/llava15_vision_model/mmproj-model-f16.gguf\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 2048,\r\n            \"flash_attn\": true\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nstart server with this command:\r\n```\r\npython3 -m llama_cpp.server --config_file server_config.json\r\n```\r\n\r\nAll works good for only text mode. But for llava 1.5, works only first run, after this for any image response is invalid.\r\n\r\nI execute folllow notebook cells:\r\n\r\n```\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://localtest.me:8085/v1\", api_key=\"api_key\")\r\n```\r\n\r\n```\r\nimport base64\r\nimport io\r\nfrom PIL import Image\r\nimport requests\r\n\r\ndef load_image_and_convert_to_base64(url):\r\n    image = Image.open(requests.get(url, stream=True).raw)\r\n    image = image.resize((336, 336))\r\n    buffered = io.BytesIO()\r\n    image.save(buffered, format=\"PNG\")\r\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\r\n    return img_str\r\n\r\n\r\nurl_1 = \"https://www.princeton.edu/sites/default/files/styles/1x_full_2x_half_crop/public/images/2022/02/KOA_Nassau_2697x1517.jpg?itok=Bg2K7j7J\"\r\nurl_2 = \"https://images.pexels.com/photos/106399/pexels-photo-106399.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"\r\n\r\n\r\nfirst_image_b64 = load_image_and_convert_to_base64(url_1)\r\nsecond_image_b64 = load_image_and_convert_to_base64(url_2)\r\n```\r\n\r\n```\r\ndef generate_caption(image_b64):\r\n    response = client.chat.completions.create(\r\n        model=\"gpt-4-vision-preview\",\r\n        max_tokens=1000,\r\n        stop=[\"<|end|>\"],\r\n        temperature=0.1,\r\n        messages=[\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are an assistant who perfectly describes images.\"\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": [\r\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\r\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}}\r\n                ]\r\n            }\r\n        ]\r\n    )\r\n    return response.choices[0].message.content\r\n```\r\n\r\nFor first run works correctly:\r\n\r\n![CleanShot 2024-05-03 at 17 35 45@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/5d0398f3-b4cd-4211-9cba-930b099053f3)\r\n\r\nSecond run with another image dosen't work:\r\n\r\n![CleanShot 2024-05-03 at 17 36 06@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/65cf9b26-ce6a-4802-be22-4b8a21647b6b)\r\n\r\nAgain with first image:\r\n![CleanShot 2024-05-03 at 17 42 01@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/28699a45-25e3-4b22-8240-4379dca28aeb)\r\n\r\nHere are logs for model loading:\r\n```\r\nclip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from models/llava15_vision_model/mmproj-model-f16.gguf\r\nclip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nclip_model_load: - kv   0:                       general.architecture str              = clip\r\nclip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\r\nclip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\r\nclip_model_load: - kv   3:                   clip.has_llava_projector bool             = true\r\nclip_model_load: - kv   4:                          general.file_type u32              = 1\r\nclip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336\r\nclip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA\r\nclip_model_load: - kv   7:                     clip.vision.image_size u32              = 336\r\nclip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14\r\nclip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024\r\nclip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096\r\nclip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768\r\nclip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16\r\nclip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\r\nclip_model_load: - kv  14:                    clip.vision.block_count u32              = 23\r\nclip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\r\nclip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\r\nclip_model_load: - kv  17:                              clip.use_gelu bool             = false\r\nclip_model_load: - type  f32:  235 tensors\r\nclip_model_load: - type  f16:  142 tensors\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Max\r\nggml_metal_init: picking default device: Apple M3 Max\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M3 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\r\nclip_model_load: CLIP using Metal backend\r\nclip_model_load: params backend buffer size =  595.49 MB (377 tensors)\r\nkey clip.vision.image_grid_pinpoints not found in file\r\nkey clip.vision.mm_patch_merge_type not found in file\r\nkey clip.vision.image_crop_resolution not found in file\r\nclip_model_load: compute allocated memory: 32.89 MB\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llava15_vision_model/ggml-model-q4_k.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  3820.94 MiB, ( 4460.03 / 27648.00)\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\nllm_load_tensors:      Metal buffer size =  3820.93 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Max\r\nggml_metal_init: picking default device: Apple M3 Max\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M3 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\r\nllama_kv_cache_init:      Metal KV buffer size =  1024.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.14 MiB\r\nllama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nAVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nModel metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\r\nencode_image_with_clip: image embedding created: 576 tokens\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-03T14:47:40+00:00",
    "closed_at": "2024-05-10T06:41:11+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7060/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7060"
  },
  {
    "number": 4925,
    "title": "can' quantize deekseek model",
    "body": "When I download the model from the deepseek huggingface official repository, I cannot convert it into a gguf file\u3002\r\n\r\npython D:\\Ai\\convert.py D:\\Ai\\deepseek-coder-6.7b-instruct\r\n\r\nD:\\Ai\\gguf-py\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00001-of-00002.bin\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00001-of-00002.bin\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00002-of-00002.bin\r\nparams = Params(n_vocab=32256, n_embd=4096, n_layer=32, n_ctx=16384, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-06, n_experts=None, n_experts_used=None, rope_scaling_type=<RopeScalingType.LINEAR: 'linear'>, f_rope_freq_base=100000, f_rope_scale=4.0, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=WindowsPath('D:/Ai/deepseek-coder-6.7b-instruct'))\r\nTraceback (most recent call last):\r\n  File \"D:\\Ai\\convert.py\", line 1658, in <module>\r\n    main(sys.argv[1:])  # Exclude the first element (script name) from sys.argv\r\n    ^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1614, in main\r\n    vocab, special_vocab = vocab_factory.load_vocab(args.vocab_type, model_parent_path)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1409, in load_vocab\r\n    path = self._select_file(vocabtype)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1384, in _select_file\r\n    raise FileNotFoundError(f\"{vocabtype} {file_key} not found.\")\r\nFileNotFoundError: spm tokenizer.model not found.",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-14T07:19:27+00:00",
    "closed_at": "2024-04-18T01:06:43+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4925/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4925"
  },
  {
    "number": 3640,
    "title": "New models Sequelbox/StellarBright and ValiantLabs/ShiningValiant cannot be converted due to \"Unexpected tensor name: model.layers.0.self_attn.q_proj.lora_A.default.weight\"",
    "body": "The Open LLM Leaderboard is currently being lead by https://huggingface.co/ValiantLabs/ShiningValiant with https://huggingface.co/sequelbox/StellarBright in third place.\r\n\r\nUnfortunately these models cannot be converted to GGUF, due to this error from `convert.py`:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 1193, in <module>\r\n    main()\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 1180, in main\r\n    model   = convert_model_names(model, params)\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 984, in convert_model_names\r\n    raise Exception(f\"Unexpected tensor name: {name}\")\r\nException: Unexpected tensor name: model.layers.0.self_attn.q_proj.lora_A.default.weight\r\n```\r\n\r\nSeems they have some kind of non-standard model layout here, which doesn't affect Transformers-based loading, but does break `convert.py`.\r\n\r\nIf anyone could find a fix or workaround, that'd be much appreciated - I'm getting quite a few requests for GGUFs of these models.\r\n\r\nHere's the full output of `convert.py` on the base model, StellerBright:\r\n\r\n```\r\n [venv] tomj@51e7fe45966e:/workspace/git/gguf-llama (master \u2714) \u1405 python3 ./convert.py  --outtype f16 /workspace/process/sequelbox_stellarbright/source --outfile /workspace/process/sequelbox_stellarbright/gguf/stellarbright.fp16.gguf\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00001-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00001-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00002-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00003-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00004-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00005-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00006-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00007-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00008-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00009-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00010-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00011-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00012-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00013-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00014-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00015-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00016-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00017-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00018-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00019-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00020-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00021-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00022-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00023-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00024-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00025-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00026-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00027-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00028-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00029-of-00030.safetensors\r\nLoading model file /workspace/process/sequelbox_stellarbright/source/model-00030-of-00030.safetensors\r\nparams = Params(n_vocab=32000, n_embd=8192, n_layer=80, n_ctx=4096, n_ff=28672, n_head=64, n_head_kv=8, f_norm_eps=1e-05, f_rope_freq_base=10000.0, f_rope_scale=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/workspace/process/sequelbox_stellarbright/source'))\r\nLoading vocab file '/workspace/process/sequelbox_stellarbright/source/tokenizer.model', type 'spm'\r\nPermuting layer 0\r\nPermuting layer 1\r\nPermuting layer 2\r\nPermuting layer 3\r\nPermuting layer 4\r\nPermuting layer 5\r\nPermuting layer 6\r\nPermuting layer 7\r\nPermuting layer 8\r\nPermuting layer 9\r\nPermuting layer 10\r\nPermuting layer 11\r\nPermuting layer 12\r\nPermuting layer 13\r\nPermuting layer 14\r\nPermuting layer 15\r\nPermuting layer 16\r\nPermuting layer 17\r\nPermuting layer 18\r\nPermuting layer 19\r\nPermuting layer 20\r\nPermuting layer 21\r\nPermuting layer 22\r\nPermuting layer 23\r\nPermuting layer 24\r\nPermuting layer 25\r\nPermuting layer 26\r\nPermuting layer 27\r\nPermuting layer 28\r\nPermuting layer 29\r\nPermuting layer 30\r\nPermuting layer 31\r\nPermuting layer 32\r\nPermuting layer 33\r\nPermuting layer 34\r\nPermuting layer 35\r\nPermuting layer 36\r\nPermuting layer 37\r\nPermuting layer 38\r\nPermuting layer 39\r\nPermuting layer 40\r\nPermuting layer 41\r\nPermuting layer 42\r\nPermuting layer 43\r\nPermuting layer 44\r\nPermuting layer 45\r\nPermuting layer 46\r\nPermuting layer 47\r\nPermuting layer 48\r\nPermuting layer 49\r\nPermuting layer 50\r\nPermuting layer 51\r\nPermuting layer 52\r\nPermuting layer 53\r\nPermuting layer 54\r\nPermuting layer 55\r\nPermuting layer 56\r\nPermuting layer 57\r\nPermuting layer 58\r\nPermuting layer 59\r\nPermuting layer 60\r\nPermuting layer 61\r\nPermuting layer 62\r\nPermuting layer 63\r\nPermuting layer 64\r\nPermuting layer 65\r\nPermuting layer 66\r\nPermuting layer 67\r\nPermuting layer 68\r\nPermuting layer 69\r\nPermuting layer 70\r\nPermuting layer 71\r\nPermuting layer 72\r\nPermuting layer 73\r\nPermuting layer 74\r\nPermuting layer 75\r\nPermuting layer 76\r\nPermuting layer 77\r\nPermuting layer 78\r\nPermuting layer 79\r\nmodel.embed_tokens.weight                        -> token_embd.weight                        | F32    | [32000, 8192]\r\nmodel.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F32    | [8192]\r\nmodel.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F32    | [8192, 28672]\r\nmodel.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F32    | [28672, 8192]\r\nmodel.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F32    | [28672, 8192]\r\nmodel.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F32    | [8192]\r\nmodel.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F32    | [1024, 8192]\r\nmodel.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F32    | [8192, 8192]\r\nTraceback (most recent call last):\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 1193, in <module>\r\n    main()\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 1180, in main\r\n    model   = convert_model_names(model, params)\r\n  File \"/workspace/git/gguf-llama/./convert.py\", line 984, in convert_model_names\r\n    raise Exception(f\"Unexpected tensor name: {name}\")\r\nException: Unexpected tensor name: model.layers.0.self_attn.q_proj.lora_A.default.weight\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-15T21:29:11+00:00",
    "closed_at": "2024-04-04T01:08:13+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3640/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3640"
  },
  {
    "number": 1146,
    "title": "[User] Dependency Installation steps for ubuntu linux",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nWhen following the provided documentation, I expect to be able to successfully set up and use the software, with clear and complete instructions guiding me through each step of the process. This includes:\r\n\r\n1. Instructions for installing any dependencies and prerequisites.\r\n2. A step-by-step guide for configuring the software, including any necessary configuration files, environment variables, or command-line options.\r\n3. Clear and concise examples of how to use the software, demonstrating its main features and functionalities.\r\n4. Troubleshooting tips or common issues that users may encounter during the setup and usage process, along with their corresponding solutions.\r\n5. Reference to a comprehensive API documentation, if applicable, to allow for customization and integration with other projects.\r\nBy following the documentation, I should be able to achieve the desired results and utilize the software as intended by the developers.\r\n\r\n# Current Behavior\r\n\r\nWhen attempting to follow the provided documentation, I am unable to successfully set up and use the software due to several gaps and inconsistencies in the instructions. The current issues I am facing missing information on installing dependencies and prerequisites, making it difficult to determine which versions or packages are required for the software to function correctly.\r\n\r\n# Environment and Context\r\n\r\nOS: Ubuntu 20.04 (running on Windows 11 through WSL2)\r\n\r\n# Current Efforts on reconstructing dependency installation steps\r\n\r\n```\r\nsudo apt-get install make cmake build-essentials python3 pip\r\n\r\nmake clean\r\nmake\r\npython3 -m pip install -r requirements.txt\r\n\r\ncd ..\r\ngit clone https://huggingface.co/chavinlo/gpt4-x-alpaca\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-23T19:50:20+00:00",
    "closed_at": "2023-05-18T11:13:26+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1146/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1146"
  }
]
[
  {
    "number": 12012,
    "title": "Eval bug: Several models producing gibberish",
    "body": "### Name and Version\n\n[root@localhost ~]# ~/llama.cpp/build/bin/llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 ROCm devices:\n  Device 0: AMD Radeon VII, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64\n  Device 1: AMD Radeon VII, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64\nregister_backend: registered backend ROCm (2 devices)\nregister_device: registered device ROCm0 (AMD Radeon VII)\nregister_device: registered device ROCm1 (AMD Radeon VII)\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (Intel(R) Celeron(R) CPU G3930 @ 2.90GHz)\nload_backend: failed to find ggml_backend_init in /root/llama.cpp/build/bin/libggml-hip.so\nload_backend: failed to find ggml_backend_init in /root/llama.cpp/build/bin/libggml-cpu.so\nversion: 4753 (51f311e0)\nbuilt with cc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-23) for x86_64-redhat-linux\n\n### Operating systems\n\nMac, Linux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\nCPU = G3930\nGPU = 2x Instinct Mi50\n\n### Models\n\nhttps://huggingface.co/microsoft/phi-4-gguf/blob/main/phi-4-q4.gguf\nhttps://huggingface.co/YorkieOH10/Meta-Llama-3.1-8B-Instruct-Q8_0-GGUF/resolve/main/meta-llama-3.1-8b-instruct-q8_0.gguf?download=true\n\n\n### Problem description & steps to reproduce\n\nGetting random character strings when offloading to GPU.\n\n`~/llama.cpp/build/bin/llama-cli -m ~/phi-4-q4.gguf -p \"Hello!\" -ngl 999`\n\nInstalled ROCm following the below steps for Alma8.10:\nhttps://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager/package-manager-rhel.html\n\nbuilt llama.cpp follwing the below steps:\nhttps://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#hip\n\nIt seems to work fine when not offloading to the GPU and just running on CPU. Slowly of course, but it works.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n[root@localhost ~]# ~/llama.cpp/build/bin/llama-cli -m ~/phi-4-q4.gguf -p \"Hello!\" -ngl 999\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 ROCm devices:\n  Device 0: AMD Radeon VII, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64\n  Device 1: AMD Radeon VII, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64\nregister_backend: registered backend ROCm (2 devices)\nregister_device: registered device ROCm0 (AMD Radeon VII)\nregister_device: registered device ROCm1 (AMD Radeon VII)\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (Intel(R) Celeron(R) CPU G3930 @ 2.90GHz)\nload_backend: failed to find ggml_backend_init in /root/llama.cpp/build/bin/libggml-hip.so\nload_backend: failed to find ggml_backend_init in /root/llama.cpp/build/bin/libggml-cpu.so\nbuild: 4753 (51f311e0) with cc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-23) for x86_64-redhat-linux (debug)\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device ROCm0 (AMD Radeon VII) - 16348 MiB free\nllama_model_load_from_file_impl: using device ROCm1 (AMD Radeon VII) - 16348 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 243 tensors from /root/phi-4-q4.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Phi 4\nllama_model_loader: - kv   3:                            general.version str              = 4\nllama_model_loader: - kv   4:                       general.organization str              = Microsoft\nllama_model_loader: - kv   5:                           general.basename str              = phi\nllama_model_loader: - kv   6:                         general.size_label str              = 15B\nllama_model_loader: - kv   7:                            general.license str              = mit\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/phi-...\nllama_model_loader: - kv   9:                               general.tags arr[str,7]       = [\"phi\", \"nlp\", \"math\", \"code\", \"chat\"...\nllama_model_loader: - kv  10:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  11:                        phi3.context_length u32              = 16384\nllama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 16384\nllama_model_loader: - kv  13:                      phi3.embedding_length u32              = 5120\nllama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 17920\nllama_model_loader: - kv  15:                           phi3.block_count u32              = 40\nllama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 40\nllama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 10\nllama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 250000.000000\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 0\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = dbrx\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,100000]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 100257\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 100257\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 100257\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - kv  32:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q4_K:  101 tensors\nllama_model_loader: - type q5_K:   40 tensors\nllama_model_loader: - type q6_K:   21 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 8.43 GiB (4.94 BPW) \nload: special tokens cache size = 96\nload: token to piece cache size = 0.6151 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 16384\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 40\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 10\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1280\nprint_info: n_embd_v_gqa     = 1280\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 17920\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 250000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 16384\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 14B\nprint_info: model params     = 14.66 B\nprint_info: general.name     = Phi 4\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 100352\nprint_info: n_merges         = 100000\nprint_info: BOS token        = 100257 '<|endoftext|>'\nprint_info: EOS token        = 100257 '<|endoftext|>'\nprint_info: EOT token        = 100257 '<|endoftext|>'\nprint_info: PAD token        = 100257 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 100258 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 100260 '<|fim_suffix|>'\nprint_info: FIM MID token    = 100259 '<|fim_middle|>'\nprint_info: EOG token        = 100257 '<|endoftext|>'\nprint_info: EOG token        = 100265 '<|im_end|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 40 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 41/41 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   275.62 MiB\nload_tensors:        ROCm0 model buffer size =  4163.91 MiB\nload_tensors:        ROCm1 model buffer size =  4190.80 MiB\n.......................................................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 4096\nllama_init_from_model: n_ctx_per_seq = 4096\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 250000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\nllama_kv_cache_init:      ROCm0 KV buffer size =   420.00 MiB\nllama_kv_cache_init:      ROCm1 KV buffer size =   380.00 MiB\nllama_init_from_model: KV self size  =  800.00 MiB, K (f16):  400.00 MiB, V (f16):  400.00 MiB\nllama_init_from_model:  ROCm_Host  output buffer size =     0.38 MiB\nllama_init_from_model: pipeline parallelism enabled (n_copies=4)\nllama_init_from_model:      ROCm0 compute buffer size =   437.01 MiB\nllama_init_from_model:      ROCm1 compute buffer size =   437.02 MiB\nllama_init_from_model:  ROCm_Host compute buffer size =    42.02 MiB\nllama_init_from_model: graph nodes  = 1606\nllama_init_from_model: graph splits = 3\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 2\nmain: chat template is available, enabling conversation mode (disable it with -no-cnv)\nmain: chat template example:\n<|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|><|im_start|>user<|im_sep|>Hello<|im_end|><|im_start|>assistant<|im_sep|>Hi there<|im_end|><|im_start|>user<|im_sep|>How are you?<|im_end|><|im_start|>assistant<|im_sep|>\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 2 | ROCm : NO_VMM = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 1123923216\nsampler params: \n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n\nsystemHello!\n\n\n> \n&3%9C(6B>;#$C/F;;8/49=0%41%588-6.D5>BB8;)/H@=!$9+,GC51(>40=&89&$'G>2GFF0C*69F-8/<$A88>;@+CB6-#C1B!*=<\"5-:4.<'*&E7/A>(G%!-:G*72D+/G+B*://;;3\"A'9,*E<FDHG4-524\"E:$5F1:7;AA4(45/%%%2;81;8./#5'C'2E$@>@8(%;2<<F\n> hello!\n%':8B2+F@H!/;,7*;F$\"'@!&/&<E6;06:@H(8);-;50>337\";*\n> \nllama_perf_sampler_print:    sampling time =      37.41 ms /    60 runs   (    0.62 ms per token,  1603.93 tokens per second)\nllama_perf_context_print:        load time =   13567.06 ms\nllama_perf_context_print: prompt eval time =    4097.00 ms /    17 tokens (  241.00 ms per token,     4.15 tokens per second)\nllama_perf_context_print:        eval time =    6391.57 ms /   251 runs   (   25.46 ms per token,    39.27 tokens per second)\nllama_perf_context_print:       total time =  104277.98 ms /   268 tokens\nInterrupted by user\n[root@localhost ~]# ^C\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-02-21T20:52:18+00:00",
    "closed_at": "2025-02-26T00:59:59+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12012/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12012"
  },
  {
    "number": 9473,
    "title": "Bug: Build failure in master on Ubuntu 24.04 with CUDA enabled",
    "body": "### What happened?\n\nBuild failure starting ~Sep 11/12.\r\n\r\nI run fresh builds periodically - about once every 1-2 days and this started recently. Build command:\r\nmake GGML_CUDA=1 -j 16\n\n### Name and Version\n\nEnvironment is Ubuntu 24.04 updated as of submission.\r\n\r\ncommit feff4aa8461da7c432d144c11da4802e41fef3cf (HEAD -> master, tag: b3751, origin/master, origin/HEAD)\r\ngcc (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Wed_Aug_14_10:10:22_PDT_2024\r\nCuda compilation tools, release 12.6, V12.6.68\r\nBuild cuda_12.6.r12.6/compiler.34714021_0\r\n\r\ncuda-toolkit-12-6 is already the newest version (12.6.1-1).\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nnvcc -std=c++11 -O3 -g -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -DGGML_CUDA_USE_GRAPHS  -Xcompiler \"-std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml/src/ggml-cuda/fattn-tile-f16.cu -o ggml/src/ggml-cuda/fattn-tile-f16.o\r\n/usr/lib/gcc/x86_64-linux-gnu/13/include/amxtileintrin.h(42): error: identifier \"__builtin_ia32_ldtilecfg\" is undefined\r\n    __builtin_ia32_ldtilecfg (__config);\r\n    ^\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/13/include/amxtileintrin.h(49): error: identifier \"__builtin_ia32_sttilecfg\" is undefined\r\n    __builtin_ia32_sttilecfg (__config);\r\n    ^\r\n\r\n2 errors detected in the compilation of \"ggml/src/ggml-cuda.cu\".\r\nmake: *** [Makefile:753: ggml/src/ggml-cuda.o] Error 2\r\nmake: *** Waiting for unfinished jobs....\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-09-13T15:35:09+00:00",
    "closed_at": "2024-09-16T14:22:09+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9473/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9473"
  },
  {
    "number": 771,
    "title": "Running a Vicuna-13B 4it model ?",
    "body": "I found this model : \r\n[[ggml-vicuna-13b-4bit](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit)](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/tree/main) and judging by their online demo it's very impressive.\r\nI tried to run it with llama.cpp latest version - the model loads fine, but as soon as it loads it starts hallucinating and quits by itself. \r\nDo I need to have it converted or something like that ?",
    "labels": [
      "model",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-04-05T07:33:04+00:00",
    "closed_at": "2023-07-28T19:47:57+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/771/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/771"
  },
  {
    "number": 6120,
    "title": "llama: add Grok support",
    "body": "Hi,\r\nPlease add support for Grok.\r\nThanks!\r\n\r\nRelevant links:\r\n* https://github.com/xai-org/grok\r\n* https://x.ai/blog/grok-os\r\n* https://twitter.com/grok/status/1769441648910479423\r\n* [NEW] Official Upload (thx to @dranger003) for linking: https://huggingface.co/xai-org/grok-1",
    "labels": [
      "enhancement",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-03-17T20:31:28+00:00",
    "closed_at": "2024-05-08T17:05:36+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6120/reactions",
      "total_count": 86,
      "+1": 57,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 9,
      "rocket": 14,
      "eyes": 6
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6120"
  },
  {
    "number": 1217,
    "title": "ClBlast - no gpu load, no perfomans difference.",
    "body": "How i build:\r\n\r\n1.  I use [w64devkit](https://github.com/skeeto/w64devkit/releases)\r\n2. I download [CLBlast](https://github.com/CNugteren/CLBlast) and [OpenCL-SDK](https://github.com/KhronosGroup/OpenCL-SDK)\r\n3. Put folders lib and include from [CLBlast](https://github.com/CNugteren/CLBlast) and [OpenCL-SDK](https://github.com/KhronosGroup/OpenCL-SDK) to w64devkit_1.18.0\\x86_64-w64-mingw32\r\n4. Using w64devkit.exe cd to llama.cpp\r\n5. make LLAMA_CLBLAST=1\r\n6. Put clblast.dll near main.exe\r\n\r\nWhen load i got this: \r\n\r\n> Initializing CLBlast (First Run)...\r\n> Attempting to use: Platform=0, Device=0 (If invalid, program will crash)\r\n> Using Platform: AMD Accelerated Parallel Processing Device: gfx90c\r\n> llama_init_from_file: kv self size  = 1600.00 MB\r\n> \r\n> system_info: n_threads = 7 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\n> main: interactive mode on.\r\n> Reverse prompt: '### Human:'\r\n> Reverse prompt: '### Instruction:\r\n\r\nBut no gpu load, no perfomans difference. Btw when i use koboldcpp i got ~40-60% gpu load.\r\n\r\nWhat could have gone wrong? And how build CLBlast with static libraries?\r\n\r\nP.S. I use ryzen 5700u without dgpu.\r\n\r\n",
    "labels": [
      "performance",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-04-28T16:05:41+00:00",
    "closed_at": "2023-05-05T00:51:53+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1217/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1217"
  },
  {
    "number": 12234,
    "title": "Eval bug: Excessive stack usage during tool calling",
    "body": "### Name and Version\n\n./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\nversion: 4840 (3ffbbd5c)\nbuilt with Ubuntu clang version 18.1.8 (++20240731024944+3b5b5c1ec4a3-1~exp1~20240731145000.144) for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\ni9-13900HX + NVIDIA GeForce RTX 4070\n\n\n\n### Models\n\n[bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M](https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/blob/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf)\n\n### Problem description & steps to reproduce\n\ncc/@ochafik\n\nI am attempting to run BFCL on llama-server, and so far I have triggered a crash twice.  It does not appear to be deterministic, unfortunately.  In one instance, I was able to catch the crash with gdb.  Here is the end of the backtrace:\n\n```\n#87097 0x00005669dac2b7f9 in bool std::__detail::__regex_algo_impl<__gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::sub_match<__gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, char, std::__cxx11::regex_traits<char> >(__gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, __gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__cxx11::match_results<__gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::sub_match<__gnu_cxx::__normal_iterator<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >&, std::__cxx11::basic_regex<char, std::__cxx11::regex_traits<char> > const&, std::regex_constants::match_flag_type, std::__detail::_RegexExecutorPolicy, bool) ()\n#87098 0x00007116a7f3ac54 in llama_grammar_accept_impl(llama_grammar&, int) () from /home/ed/Projects/llama.cpp/build/bin/libllama.so\n#87099 0x00005669dadb179a in common_sampler_accept(common_sampler*, int, bool) ()\n#87100 0x00005669dac5c626 in server_context::update_slots() ()\n#87101 0x00005669dabe4886 in server_queue::start_loop() ()\n#87102 0x00005669dabb0bc8 in main ()\n```\nThe remaining 87096 stack frames were identical.  So while I have not been able to find the exact input that triggered the crash yet, I hoped that this might be enough of a clue as to what is going on.\n\nHere is some more information about what I am doing:\n* `/home/ed/Projects/llama.cpp/build/bin/llama-server --ctx-size 0 --jinja -fa -hf bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M --host 0.0.0.0 -ngl 100`\n* `python /home/ed/Projects/gorilla/berkeley-function-call-leaderboard/venv/bin/bfcl generate --model gpt-4-turbo-2024-04-09-FC --test-category all --include-input-log`\n* I added this patch:\n``` diff\ndiff --git a/berkeley-function-call-leaderboard/bfcl/model_handler/api_inference/openai.py b/berkeley-function-call-leaderboard/bfcl/model_handler/api_inference/openai.py\nindex fbf7c0f..fc0da1f 100644\n--- a/berkeley-function-call-leaderboard/bfcl/model_handler/api_inference/openai.py\n+++ b/berkeley-function-call-leaderboard/bfcl/model_handler/api_inference/openai.py\n@@ -22,7 +22,7 @@ class OpenAIHandler(BaseHandler):\n     def __init__(self, model_name, temperature) -> None:\n         super().__init__(model_name, temperature)\n         self.model_style = ModelStyle.OpenAI\n-        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n+        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=\"http://localhost:8080\")\n \n     def decode_ast(self, result, language=\"Python\"):\n         if \"FC\" in self.model_name or self.is_fc_model:\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /chat/completions 127.0.0.1 200\nsrv  params_from_: Chat format: Hermes 2 Pro\nslot launch_slot_: id  0 | task 48450 | processing task\nslot update_slots: id  0 | task 48450 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 326\nslot update_slots: id  0 | task 48450 | kv cache rm [67, end)\nslot update_slots: id  0 | task 48450 | prompt processing progress, n_past = 326, n_tokens = 259, progress = 0.794479\nslot update_slots: id  0 | task 48450 | prompt done, n_past = 326, n_tokens = 259\nslot      release: id  0 | task 48450 | stop processing: n_past = 504, truncated = 0\nslot print_timing: id  0 | task 48450 | \nprompt eval time =     104.08 ms /   259 tokens (    0.40 ms per token,  2488.52 tokens per second)\n       eval time =    3465.17 ms /   179 tokens (   19.36 ms per token,    51.66 tokens per second)\n      total time =    3569.24 ms /   438 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /chat/completions 127.0.0.1 200\nsrv  params_from_: Chat format: Hermes 2 Pro\nslot launch_slot_: id  0 | task 48630 | processing task\nslot update_slots: id  0 | task 48630 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 326\nslot update_slots: id  0 | task 48630 | kv cache rm [67, end)\nslot update_slots: id  0 | task 48630 | prompt processing progress, n_past = 326, n_tokens = 259, progress = 0.794479\nslot update_slots: id  0 | task 48630 | prompt done, n_past = 326, n_tokens = 259\n/home/ed/.local/share/dorothy/user/commands/llama-cpp-server: line 8: 709629 Segmentation fault      (core dumped) ~/Projects/llama.cpp/build/bin/llama-server --ctx-size $CTX_SIZE --jinja -fa -hf \"$MODEL\" --host 0.0.0.0 -ngl $OFFLOAD_NUM $OTHERARGS\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-06T21:08:46+00:00",
    "closed_at": "2025-05-02T01:07:57+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12234/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12234"
  },
  {
    "number": 11474,
    "title": "Research: Benchmarking DeepSeek-R1 IQ1_S 1.58bit",
    "body": "### Research Stage\n\n- [ ] Background Research (Let's try to avoid reinventing the wheel)\n- [ ] Hypothesis Formed (How do you think this will work and it's effect?)\n- [ ] Strategy / Implementation Forming\n- [x] Analysis of results\n- [ ] Debrief / Documentation (So people in the future can learn from us)\n\n### Previous existing literature and research\n\n# Command\n```\n ./llama.cpp/build/bin/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --n-gpu-layers 61 --prio 2 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<\uff5cUser\uff5c>What is the capital of Italy?<\uff5cAssistant\uff5c>\"\n```\n\n# Model\n[DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S\n](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) 1.58Bit, 131GB\n\n# Hardware\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:27:00.0 Off |                    0 |\n| N/A   34C    P0              58W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:2A:00.0 Off |                    0 |\n| N/A   32C    P0              60W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n```\n\n\n\n\n### Hypothesis\n\n[Reported](https://unsloth.ai/blog/deepseekr1-dynamic) performances is 140 token/second\n\n### Implementation\n\n_No response_\n\n### Analysis\n\n# Llama.cpp Performance Analysis\n\n## Raw Benchmarks\n```\nllama_perf_sampler_print:    sampling time =       2.45 ms /    35 runs   (    0.07 ms per token, 14297.39 tokens per second)\nllama_perf_context_print:        load time =   20988.11 ms\nllama_perf_context_print: prompt eval time =    1233.88 ms /    10 tokens (  123.39 ms per token,     8.10 tokens per second)\nllama_perf_context_print:        eval time =    2612.63 ms /    24 runs   (  108.86 ms per token,     9.19 tokens per second)\nllama_perf_context_print:       total time =    3869.00 ms /    34 tokens\n```\n\n## Detailed Analysis\n\n### 1. Token Sampling Performance\n- **Total Time**: 2.45 ms for 35 runs\n- **Per Token**: 0.07 ms\n- **Speed**: 14,297.39 tokens per second\n- **Description**: This represents the speed at which the model can select the next token after processing. This is extremely fast compared to the actual generation speed, as it only involves the final selection process.\n\n### 2. Model Loading\n- **Total Time**: 20,988.11 ms (\u224821 seconds)\n- **Description**: One-time initialization cost to load the model into memory. This happens only at startup and doesn't affect ongoing performance.\n\n### 3. Prompt Evaluation\n- **Total Time**: 1,233.88 ms for 10 tokens\n- **Per Token**: 123.39 ms\n- **Speed**: 8.10 tokens per second\n- **Description**: Initial processing of the prompt is slightly slower than subsequent token generation, as it needs to establish the full context for the first time.\n\n### 4. Generation Evaluation\n- **Total Time**: 2,612.63 ms for 24 runs\n- **Per Token**: 108.86 ms\n- **Speed**: 9.19 tokens per second\n- **Description**: This represents the actual speed of generating new tokens, including all neural network computations.\n\n### 5. Total Processing Time\n- **Total Time**: 3,869.00 ms\n- **Tokens Processed**: 34 tokens\n- **Average Speed**: \u22488.79 tokens per second\n\n## Key Insights\n\n1. **Performance Bottlenecks**:\n   - The main bottleneck is in the evaluation phase (actual token generation)\n   - While sampling can handle 14K+ tokens per second, actual generation is limited to about 9 tokens per second\n   - This difference highlights that the neural network computations, not the token selection process, are the limiting factor\n\n2. **Processing Stages**:\n   - Model loading is a significant but one-time cost\n   - Prompt evaluation is slightly slower than subsequent token generation\n   - Sampling is extremely fast compared to evaluation\n\n3. **Overall Performance**:\n   - The system demonstrates typical performance characteristics for a CPU-based language model\n   - The total processing rate of ~9 tokens per second is reasonable for local inference on consumer hardware\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "research \ud83d\udd2c",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-28T23:39:28+00:00",
    "closed_at": "2025-04-25T01:07:52+00:00",
    "comments": 45,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11474"
  },
  {
    "number": 6877,
    "title": "Add support to ArcticForCausalLM",
    "body": "First open LLM from [@SnowflakeDB](https://twitter.com/SnowflakeDB)! Arctic is 480B Dense-MoE with a 10B dense transformer model and a 128x3.66B MoE MLP designed specifically for enterprise AI. \ud83e\udd14\r\n\r\nTL;DR:\r\n\ud83e\udde0 480B parameters with 17B active during generation\r\n\ud83d\udc68\u200d\ud83c\udfeb  128 experts with 2 active in generation\r\n2\ufe0f\u20e3 Instruct & Base versions released\r\n\ud83c\udfd9\ufe0f Focused on Enterprise task (Code, SQL, Reasoning, Following)\r\n\ud83d\udd13 Released under Apache 2.0\r\n\ud83d\uddfb in fp16 ~900GB Memory & in int4 ~240GB\r\n\ud83e\udd17 Available on [@huggingface](https://twitter.com/huggingface)\r\n\r\n\ud83c\udfcb\ud83c\udffb Trained with DeepSpeed-MoE\r\n\r\n\r\nBlog: [https://snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/](https://t.co/RAgYE44tBA)\r\n\r\nModels: [https://huggingface.co/Snowflake/snowflake-arctic-instruct](https://t.co/Mdd9XfAKfe)",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-04-24T14:45:07+00:00",
    "closed_at": "2024-05-24T12:31:15+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6877/reactions",
      "total_count": 12,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 3,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6877"
  },
  {
    "number": 3284,
    "title": "examples for iOS (objc / swift ui)",
    "body": "I really enjoyed the examples for running whisper.cpp on iOS using both objective-c and swift-ui (found at https://github.com/ggerganov/whisper.cpp/tree/master/examples/whisper.objc and https://github.com/ggerganov/whisper.cpp/tree/master/examples/whisper.swiftui respectively) and was wondering if the process can be recreated for this repository. I believe that having a minimal example repository would be useful. \r\n\r\nI'd be willing to make an attempt, but I need to familiarize myself with the process that was performed in the whisper.cpp repository. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-20T20:48:39+00:00",
    "closed_at": "2023-11-28T16:21:00+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3284/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3284"
  },
  {
    "number": 3051,
    "title": "Multi-GPU support for AMD?",
    "body": "Do you have multi-GPU support for AMD, if not, do you see it as something you might add in the future?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-07T00:15:22+00:00",
    "closed_at": "2024-06-12T01:06:50+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3051/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3051"
  },
  {
    "number": 4437,
    "title": "Will llama.cpp be able to use Phi-2 ?",
    "body": "Surely we have to wait for a GGUF version, but in the meantime just curious about it\r\n\r\nthanks",
    "labels": [
      "enhancement",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-12-13T12:02:56+00:00",
    "closed_at": "2023-12-18T17:27:49+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4437/reactions",
      "total_count": 20,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4437"
  },
  {
    "number": 637,
    "title": "Performance investigation using AMD BLIS instead of OpenBLAS on 16 core AMD Zen1",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nCompiling against AMD optimized BLS implementation of BLAS allows me to run perplexity tests\r\n\r\n# Current Behavior\r\nCompiling against AMD optimized BLS implementation of BLAS causes perplexity command to process 0 chunks\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n```\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\r\nVirtualization:                  AMD-V\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n```\r\nllama.cpp$ uname -a\r\nLinux asushimu 5.15.0-60-generic #66-Ubuntu SMP Fri Jan 20 14:29:49 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 3df890aef432ce68143cfafcd7caf828bc4c3e55\r\nllama.cpp$ python3 --version\r\nPython 3.10.9\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\nllama.cpp$ g++ --version | head -1\r\ng++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n```\r\n# Steps to Reproduce\r\n\r\n1. Install latest bliss libs from github\r\n\r\n```\r\nblis$ sudo make install\r\nInstalling libblis.a into /usr/local/lib/\r\nInstalling libblis.so.4.0.0 into /usr/local/lib/\r\nGenerating monolithic cblas.h.........\r\nGenerated include/zen/cblas.h\r\nInstalling blis.h cblas.h blis.hh cblas.hh into /usr/local/include/blis/\r\nInstalling config.mk common.mk into /usr/local/share/blis/\r\nInstalling config/zen/make_defs.mk into /usr/local/share/blis/config/zen\r\nmkdir -p /usr/local/share/pkgconfig\r\nInstalling blis.pc into /usr/local/share/pkgconfig/\r\ninstall -c -m 0644 blis.pc /usr/local/share/pkgconfig\r\n```\r\n3. Update Makefile to use blis instead of blas\r\n\r\n```\r\nllama.cpp$ diff Makefile.bliss Makefile.dist \r\n183,184c183,184\r\n< \tCFLAGS  += -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\n< \tLDFLAGS += -lblis\r\n---\r\n> \tCFLAGS  += -DGGML_USE_OPENBLAS -I/usr/local/include/openblas\r\n> \tLDFLAGS += -lopenblas\r\n```\r\n\r\n5. Compile against blis, perplexity processes 0 chunks\r\n\r\n174 second run just calling `./main` linked against OpenBLAS:\r\n```\r\nllama.cpp$ make -f Makefile.dist clean && LLAMA_OPENBLAS=1 make -f Makefile.dist;ldd ./main;time ./main -t 16 -m ./models/7B/ggml-model-q4_0.bin -b 256 -n 512 -p \"blis or blas\"\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/openblas\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lopenblas\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/openblas   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lopenblas\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lopenblas\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lopenblas\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lopenblas\r\n\tlinux-vdso.so.1 (0x00007ffd8c7a7000)\r\n\tlibopenblas.so.0 => /lib/x86_64-linux-gnu/libopenblas.so.0 (0x00007f3bb8880000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f3bb8656000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3bb856f000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3bb854f000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3bb8327000)\r\n\tlibgfortran.so.5 => /lib/x86_64-linux-gnu/libgfortran.so.5 (0x00007f3bb804a000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f3bbadd8000)\r\n\tlibquadmath.so.0 => /lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f3bb8002000)\r\nmain: seed = 1680212228\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 256, n_predict = 512, n_keep = 0\r\n\r\n\r\n blis or blas\r\n\"Blas,\" said the voice in the dark. \"There is a Blas. He's been here all day long. He hasn't moved. I think he might be asleep.\"\r\nI could hear breathing, but it was too distant to place where it was coming from. Blas. The name came to me from some forgotten dream. I couldn't recall why I had remembered it or where the thought had come from, but the name was there in my head: Blas. And then it was gone again, like a bird in flight.\r\n\"Is he still here?\" said the voice. \"I can't see him.\"\r\n\"Here,\" I said. \"Yes.\" The word hung on the air of our cave, suspended between us. But that word was also gone.\r\nWe had been together for three days now. Three days ago we had met in the woods; on day two we had found a cave deep in the forest and had made it into our own little world. Now it was nighttime again and Blas slept. I could hear his breathing, but there were other sounds too: waves, a distant breeze, the creaking of tree limbs heavy with snow.\r\n\"I think he might be asleep,\" said the voice in the dark.\r\nIt was a strange thing to hear that voice again: we had come to know it so well since we'd met\u2014it had been there in my head for weeks and weeks, but now suddenly it seemed like an old friend, someone I knew very well from childhood days: my mother's voice, or the sound of the sea. I couldn't quite work out what it was. And then again, that name was gone, swirling round in me like a leaf flung against a stone in a river. Blas. It must have been some kind of bird, perhaps a small bird with a short tail.\r\n\"I think he might be asleep,\" said the voice. \"What shall we do?\"\r\n\"Shall we go to bed?\" I asked. I could hear my own words coming out of the dark cave like birdsong: they had been there in me for days and now suddenly they were back again, like a message from the past. And then it was gone too.\r\nThe voice sighed with relief as if at some unsaid thing that was now gone\u2014and it came to me again: \"Shall we\r\nllama_print_timings:        load time =  1072.38 ms\r\nllama_print_timings:      sample time =   401.94 ms /   512 runs   (    0.79 ms per run)\r\nllama_print_timings: prompt eval time = 15402.35 ms /   263 tokens (   58.56 ms per token)\r\nllama_print_timings:        eval time = 157868.10 ms /   510 runs   (  309.55 ms per run)\r\nllama_print_timings:       total time = 174278.01 ms\r\n\r\nreal\t2m54.504s\r\nuser\t46m6.640s\r\nsys\t3m35.773s\r\n```\r\n\r\n47 second run calling `./main` linked against AMD bliss BLAS libs:\r\n```\r\nllama.cpp$ make -f Makefile.bliss clean && LLAMA_OPENBLAS=1 make -f Makefile.bliss;ldd ./main;time ./main -t 16 -m ./models/7B/ggml-model-q4_0.bin -b 256 -n 512 -p \"blis or blas\"\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lblis\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lblis\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lblis\r\n\tlinux-vdso.so.1 (0x00007fff553ed000)\r\n\tlibblis.so.4 => /usr/local/lib/libblis.so.4 (0x00007f1011a8c000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f1011862000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f101177b000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f101175b000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1011533000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f101200e000)\r\nmain: seed = 1680212135\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 256, n_predict = 512, n_keep = 0\r\n\r\n\r\n blis or blas\r\nI know the word is spelled either with a B, L or an S. I believe it was used for either a sword (blade) or a small dagger or short knife.\r\nDoes anyone know the correct spelling?\r\nThanks in advance to everyone who might answer this question.\r\nblis, blas\r\nIt is indeed possible that it's spelt both ways: https://en.wikipedia.org/wiki/Bliss_(disambiguation)\r\nSo you are correct it could be either way but it would depend on the context. It is used in Scottish names like Bliss-Carver or Blaisdell for example.\r\nI agree with @MikeSteeden, it's possible to find it spelled both ways. I am not sure what exactly is your question: do you want to know if either one of these variants is correct? If so, then the answer is yes: bliss and blaise are acceptable spellings.\r\nI wanted to know which is correct. It's a family name and I just got confused as to which spelling is correct since I have seen it in two different ways. Thanks for your answers. [end of text]\r\n\r\nllama_print_timings:        load time =  1076.06 ms\r\nllama_print_timings:      sample time =   190.66 ms /   243 runs   (    0.78 ms per run)\r\nllama_print_timings: prompt eval time =   482.64 ms /     6 tokens (   80.44 ms per token)\r\nllama_print_timings:        eval time = 46036.34 ms /   242 runs   (  190.23 ms per run)\r\nllama_print_timings:       total time = 47307.07 ms\r\n\r\nreal\t0m47.525s\r\nuser\t12m20.192s\r\nsys\t0m1.248s\r\n```\r\nPerplexity run with blis doesn't process any chunks :\r\n```\r\nllama.cpp$ make -f Makefile.bliss clean && LLAMA_OPENBLAS=1 make -f Makefile.bliss;ldd ./perplexity ;time ./perplexity -t 16 -m ./models/7B/ggml-model-q4_0.bin -f /data/llama/wikitext-2-raw/wiki.wiki.test.raw\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lblis\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lblis\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lblis\r\n\tlinux-vdso.so.1 (0x00007ffced7f6000)\r\n\tlibblis.so.4 => /usr/local/lib/libblis.so.4 (0x00007fbe1ee26000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fbe1ebfc000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbe1eb15000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fbe1eaf5000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbe1e8cd000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbe1f3a6000)\r\nmain: seed = 1680214250\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nperplexity : calculating perplexity over 0 chunks\r\n\r\n\r\nllama_print_timings:        load time =     9.90 ms\r\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\r\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings:       total time =   578.24 ms\r\n\r\nreal\t0m0.700s\r\nuser\t0m0.105s\r\nsys\t0m0.579s\r\n```\r\n",
    "labels": [
      "enhancement",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-30T22:14:53+00:00",
    "closed_at": "2023-04-13T08:09:16+00:00",
    "comments": 30,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/637/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/637"
  },
  {
    "number": 6841,
    "title": "Is it normal that ROCm+HIPBLAS produces different results than on CPU or breaks completely?",
    "body": "Hello. I did some perplexity tests while investigating issue with Llama-3. Initially the issue was that Llama-3 base 70b model outputs garbage with small quants with iMatrix. Can't find any information regarding that ROCm possibly causes corruption.\r\n\r\n### GPU test (RX 7600 + RX 7600 XT)\r\nhttps://huggingface.co/mradermacher/Meta-Llama-3-70B-i1-GGUF/tree/main\r\nMeta-Llama-3-70B.i1-Q2_K.gguf prints [1]-nan,[2]-nan,[3]-nan,[4]-nan with -ngl 30 or 0 (prints garbage unless -ngl 0)\r\nhttps://huggingface.co/mradermacher/Meta-Llama-3-70B-GGUF/tree/main\r\nMeta-Llama-3-70B.Q2_K.gguf - seems OK, [1]4.1839,[2]4.7300,[3]4.2751,[4]4.6444,[5]4.6942,[6]5.0426,[7]5.1405,[8]5.4747\r\nFinal estimate: PPL = 5.9315 +/- 0.03553\r\n\r\n### Pure CPU test\r\nMeta-Llama-3-70B.i1-Q2_K.gguf with pure CPU 'perplexity' build (146 seconds per 512 tokens - ETA 26 hours 55.67 minutes)\r\n[1]6.3962,[2]7.1886,[3]6.9886,[4]7.3853,[5]7.8924,[6]8.2982,[7]8.8956,[8]9.3799, (can't wait for many hours, stopped)\r\nMeta-Llama-3-70B.Q2_K.gguf (static Q2_K):\r\n[1]4.1675,[2]4.6952,[3]4.2374,[4]4.6452,[5]4.6677,[6]5.0459,[7]5.1258,[8]5.4649,^C\r\nIt's slightly better than on ROCm but the difference is very small.\r\n\r\nI also found strange holes in the imatrix.dat that was used:\r\n![Screenshot from 2024-04-23 13-34-09](https://github.com/ggerganov/llama.cpp/assets/6028184/43f7b889-618e-4e47-afab-a12603b22ade)\r\nBut the author seems uninterested in discussing that.\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-23T11:12:58+00:00",
    "closed_at": "2024-04-26T16:39:59+00:00",
    "comments": 33,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6841/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6841"
  },
  {
    "number": 1337,
    "title": "Implement Together Computer's Red Pajama 3B Base/Chat model",
    "body": "- [announcement][0]\r\n- [base model, 3B][1]\r\n- [instruct model, 3B][3]\r\n- [chat model, 3B][2]\r\n\r\nHopefully this can be blazingly fast!\r\n\r\n[0]: https://www.together.xyz/blog/redpajama-models-v1\r\n[1]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\r\n[2]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1\r\n[3]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
    "labels": [
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-06T01:48:53+00:00",
    "closed_at": "2024-04-09T01:09:36+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1337/reactions",
      "total_count": 19,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1337"
  },
  {
    "number": 9106,
    "title": "Bug: Intel Arc - not working at all",
    "body": "### What happened?\n\nGoing through the manual - SYCL I mean. Everything compiles okay. Running it always thows an error. Can't make it work. OS used: Linux Gentoo. P.S. docker doesn't work either. P.P.S. device IS listed in the list.\n\n### Name and Version\n\n# ./build/bin/llama-cli --version\r\nversion: 3609 (2f3c1466)\r\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2024.2.1 (2024.2.1.20240711) for x86_64-unknown-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n# ZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -m models/llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm none -mg 0\r\nLog start\r\nmain: build = 3609 (2f3c1466)\r\nmain: built with Intel(R) oneAPI DPC++/C++ Compiler 2024.2.1 (2024.2.1.20240711) for x86_64-unknown-linux-gnu\r\nmain: seed  = 1724182694\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b.Q4_0.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 3\r\nllm_load_vocab: token to piece cache size = 0.1684 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0                                                  llm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0                                               llm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)                                llm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'                                            llm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'                                          llm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48                                                 ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes                                                          ggml_sycl_init: found 2 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU                                    llm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  3577.56 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 2048                                            llama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1                                               [SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: yes\r\nfound 2 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A380 Graphics|    1.3|    128|    1024|   32|  6064M|            1.3.29735|\r\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 630|    1.3|     24|     256|   32| 46333M|            1.3.29735|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  2048.00 MiB\r\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   296.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    16.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nUnexpected pattern!\r\nUNREACHABLE executed at /var/tmp/portage/dev-util/spirv-llvm-translator-15.0.0-r1/work/SPIRV-LLVM-Translator-15.0.0/lib/SPIRV/SPIRVUtil.cpp:2037!\r\nThe program was built for 1 devices\r\nBuild program log for 'Intel(R) Arc(TM) A380 Graphics':\r\n -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:/home/username/llama/ggml/src/ggml-sycl.cpp, line:2722\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "SYCL",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-20T19:45:26+00:00",
    "closed_at": "2024-12-17T01:07:43+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9106"
  },
  {
    "number": 12147,
    "title": "Misc. bug: vulkan on 6900xt",
    "body": "### Name and Version\n\nLatest vulkan patches cause problems in koboldCPP for radeon 6900xt.\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-quantize\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nI reported a problem with vulkan on the KoboldCPP project page, and was asked to report the problem here (newly released koboldcpp version with latest fixes for vulkan).\n\nhttps://github.com/LostRuins/koboldcpp/issues/1398\n\nAs I wrote in the bug report on KoboldCPP, I have two cards GTX 1080ti and Radeon 6900xt. When the creator of Koboldcpp included the latest patches for vulkan, my radeon stopped working properly with vulkan.\n\nI have tested various models (llama 3.1, nemo, mistral small 22/24b) and in none of them vulkan on radeon 6900xt works correctly anymore, either some random characters are generated from the very beginning, or the response loops very quickly and repeats some word.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-02T16:35:57+00:00",
    "closed_at": "2025-04-11T15:24:47+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12147/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12147"
  },
  {
    "number": 8455,
    "title": "Bug: llama.cpp with Vulkan not running on Snapdragon X + Windows (Copilot+PCs)",
    "body": "### What happened?\r\n\r\nThe new Copilot+PCs with Qualcomm Snapdragon X processors (in my case a Surface 11 Pro with Snapdragon X Plus and 16GB RAM) are fast, and run llama.cpp on the CPU w/o issues. They also include a Vulkan driver and run the Vulkan samples w/o problems. But llama.cpp built with Vulkan does (now finally build,) but not run.\r\n\r\n_llama-cli is terminating on model-load with:_\r\nllama_model_load: error loading model: vk::Device::createComputePipeline: ErrorUnknown\r\nllama_load_model_from_file: failed to load model\r\nmain: error: unable to load model\r\n\r\n### Name and Version\r\n\r\nllama-cli version: 3378 (71c1121d) with a quick-fix to compile (see #8446), built with MSVC 19.40.33812.0 for ARM64\r\n\r\n_built with:_\r\nInstalled VulkanSDK for Windows x64, then built a Windows arm64 version of KhronosGroup/Vulkan-Loader vulkan-1.lib (+tested its functionality with tests+samples) and copied it to VulkanSDK lib-directory for llama.cpp building.\r\n```shell\r\nREM including Vulkan diagnostics\r\n> cmake -B build -DGGML_VULKAN=1 -DGGML_VULKAN_DEBUG=1 -DGGML_VULKAN_MEMORY_DEBUG=1\r\n> cmake --build build --config Release --target llama-cli\r\n```\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n[console output.txt](https://github.com/user-attachments/files/16192377/console.output.txt)\r\n[main.log](https://github.com/user-attachments/files/16192378/main.log)\r\n[vulkaninfo.txt](https://github.com/user-attachments/files/16192379/vulkaninfo.txt)\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-12T10:25:53+00:00",
    "closed_at": "2025-01-31T01:07:14+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8455/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8455"
  },
  {
    "number": 2754,
    "title": "[User] -n -2 generates nothing",
    "body": "See this post: https://github.com/ggerganov/llama.cpp/issues/2754#issuecomment-1691682835",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-23T23:20:31+00:00",
    "closed_at": "2023-08-24T15:48:35+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2754/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2754"
  },
  {
    "number": 4123,
    "title": "does not compile on CUDA 10 anymore",
    "body": "Ever since this got merged:\r\n[https://github.com/ggerganov/llama.cpp/pull/3370](url)\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-18T10:38:49+00:00",
    "closed_at": "2024-05-07T01:06:47+00:00",
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4123"
  },
  {
    "number": 8025,
    "title": "Bug: Qwen2-72B-Instruct (and finetunes) Q4_K_M, Q5_K_M generates random output with CuBLAS prompt processing ",
    "body": "### What happened?\r\n\r\nQwen2-72B-Instruct Q4_K_M generates output with random tokens (numbers, special symbols, random chunks of words from different languages, etc).\r\n\r\nHas been tested on:\r\n1) Tesla P40 24gb + CPU partitioning with offloating half of the layers\r\n2) Inference fully on RAM (on another pc from 1)\r\n\r\nOther people say it works with Q6, maybe the problem is with Q4_K_M (i can't test q6).\r\n\r\nI've tried with both FlashAttention on and off and MMQ on and off, doesn't work.\r\n\r\nI tested with llama.cpp binaries, koboldcpp, text-generation-webui - doesn't work everywhere.\r\n\r\nrelated: https://github.com/LostRuins/koboldcpp/issues/909\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/54563399/e866dcbb-56b0-4eea-8ff8-6b1816520f3a)\r\n\r\n\r\n### Name and Version\r\n\r\nversion: 3181 (37bef894)\r\nbuilt with MSVC 19.29.30154.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-20T03:39:38+00:00",
    "closed_at": "2024-09-07T01:07:11+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8025/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8025"
  },
  {
    "number": 4099,
    "title": "Compilation error on Nvidia Jetson Nano",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed). \r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\r\n\r\nHi! I'm trying to compile llamacpp on an Nvidia Jetson Nano 2GB with CuBLAS, because I want to use the cuda cores, but I'm facing some issues with compilation. \r\n\r\n# Current Behavior\r\n\r\nPlease provide a detailed written description of what `llama.cpp` did, instead.\r\n\r\nBoth make and cmake compilation methods results in various errors.\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n\r\n```\r\nArchitecture:        aarch64\r\nByte Order:          Little Endian\r\nCPU(s):              4\r\nOn-line CPU(s) list: 0-3\r\nThread(s) per core:  1\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nVendor ID:           ARM\r\nModel:               1\r\nModel name:          Cortex-A57\r\nStepping:            r1p1\r\nCPU max MHz:         1479,0000\r\nCPU min MHz:         102,0000\r\nBogoMIPS:            38.40\r\nL1d cache:           32K\r\nL1i cache:           48K\r\nL2 cache:            2048K\r\nFlags:               fp asimd evtstrm aes pmull sha1 sha2 crc32\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\nLinux rover-NVIDIA-JETSON 4.9.337-tegra #1 SMP PREEMPT Thu Jun 8 21:19:14 PDT 2023 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.6.9\r\n$ make --version\r\nGNU Make 4.1\r\n$ g++ --version\r\ng++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n$ cmake --version\r\ncmake version 3.28.0-rc4\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_28_22:34:44_PST_2021\r\nCuda compilation tools, release 10.2, V10.2.300\r\nBuild cuda_10.2_r440.TC440_70.29663091_0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure / bug.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. make\r\n2. make LLAMA_CUBLAS=1\r\n3. make LLAMA_CUBLAS=1 with -arch=compute_53\r\n4. make LLAMA_CUBLAS=1 with -arch=compute_53 and removed -mcpu\r\n5. cmake .. + cmake --build . --config Release (failure)\r\n6. cmake .. -DLLAMA_CUBLAS=ON + cmake --build . --config Release\r\n7. \r\n\r\n# Failure Logs\r\n\r\n```\r\n$ make\r\nI llama.cpp build info: \r\nI UNAME_S:   Linux\r\nI UNAME_P:   aarch64\r\nI UNAME_M:   aarch64\r\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -mcpu=native \r\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native  -Wno-array-bounds -Wno-format-truncation \r\nI NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \"\r\nI LDFLAGS:    \r\nI CC:        cc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\nI CXX:       g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -mcpu=native     -c ggml-quants.c -o ggml-quants.o\r\nggml-quants.c: In function \u2018ggml_vec_dot_q2_K_q8_K\u2019:\r\nggml-quants.c:403:27: error: implicit declaration of function \u2018vld1q_s16_x2\u2019; did you mean \u2018vld1q_s16\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\nggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\nggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\nggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\nggml-quants.c:3680:41: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t mins16 = {vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))};\r\n                                         ^\r\n                                          {                                                                                                      }\r\nggml-quants.c:404:27: error: implicit declaration of function \u2018vld1q_u8_x2\u2019; did you mean \u2018vld1q_u32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c:406:27: error: implicit declaration of function \u2018vld1q_s8_x2\u2019; did you mean \u2018vld1q_s32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\nggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:406:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\nggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\nggml-quants.c:3723:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\nggml-quants.c:3725:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\nggml-quants.c:3727:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml-quants.c: In function \u2018ggml_vec_dot_q3_K_q8_K\u2019:\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:4353:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:4371:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c:407:27: error: implicit declaration of function \u2018vld1q_s8_x4\u2019; did you mean \u2018vld1q_s64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\nggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\nggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\nggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\nggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\nggml-quants.c:4373:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\nggml-quants.c: In function \u2018ggml_vec_dot_q4_K_q8_K\u2019:\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:5273:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q4bits = ggml_vld1q_u8_x2(q4); q4 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c:5291:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\nggml-quants.c:5300:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\nggml-quants.c: In function \u2018ggml_vec_dot_q5_K_q8_K\u2019:\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:5918:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:5926:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q5bits = ggml_vld1q_u8_x2(q5); q5 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\nggml-quants.c:5927:46: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                              ^~~~~~~~~~~~~~~~\r\nggml-quants.c: In function \u2018ggml_vec_dot_q6_K_q8_K\u2019:\r\nggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\nggml-quants.c:6627:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\nggml-quants.c:6629:43: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t q6scales = {vmovl_s8(vget_low_s8(scales)), vmovl_s8(vget_high_s8(scales))};\r\n                                           ^\r\n                                            {                                                            }\r\nggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\nggml-quants.c:6641:40: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh); qh += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:405:27: error: implicit declaration of function \u2018vld1q_u8_x4\u2019; did you mean \u2018vld1q_u64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\nggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:405:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\nggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\nggml-quants.c:6643:40: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\nggml-quants.c:6686:21: error: incompatible types when assigning to type \u2018int8x16x4_t {aka struct int8x16x4_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                     ^\r\ncc1: some warnings being treated as errors\r\nMakefile:542: recipe for target 'ggml-quants.o' failed\r\nmake: *** [ggml-quants.o] Error 1\r\n```\r\n\r\n```\r\n$ make LLAMA_CUBLAS=1\r\nI llama.cpp build info: \r\nI UNAME_S:   Linux\r\nI UNAME_P:   aarch64\r\nI UNAME_M:   aarch64\r\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -mcpu=native \r\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native  -Wno-array-bounds -Wno-format-truncation \r\nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \"\r\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \r\nI CC:        cc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\nI CXX:       g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\nnvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \" -c ggml-cuda.cu -o ggml-cuda.o\r\nnvcc fatal   : Value 'native' is not defined for option 'gpu-architecture'\r\nMakefile:439: recipe for target 'ggml-cuda.o' failed\r\nmake: *** [ggml-cuda.o] Error 1\r\n```\r\n\r\nSetting the -arch to `compute_53` in the Makefile\r\n```\r\nmake LLAMA_CUBLAS=1\r\nI llama.cpp build info: \r\nI UNAME_S:   Linux\r\nI UNAME_P:   aarch64\r\nI UNAME_M:   aarch64\r\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -mcpu=native \r\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native  -Wno-array-bounds -Wno-format-truncation \r\nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=compute_53 -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \"\r\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \r\nI CC:        cc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\nI CXX:       g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\nnvcc --forward-unknown-to-host-compiler -use_fast_math -arch=compute_53 -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \" -c ggml-cuda.cu -o ggml-cuda.o\r\nnvcc fatal   : 'cpu=native': expected a number\r\nMakefile:439: recipe for target 'ggml-cuda.o' failed\r\nmake: *** [ggml-cuda.o] Error 1\r\n```\r\n\r\nI could not find what number I had to insert here for the cpu variable, and removing it also fails. \r\n```\r\nmake LLAMA_CUBLAS=1\r\nI llama.cpp build info: \r\nI UNAME_S:   Linux\r\nI UNAME_P:   aarch64\r\nI UNAME_M:   aarch64\r\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread \r\nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation \r\nI NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=compute_53 -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \"\r\nI LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \r\nI CC:        cc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\nI CXX:       g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread    -c ggml.c -o ggml.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c llama.cpp -o llama.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c common/common.cpp -o common.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c common/sampling.cpp -o sampling.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c common/grammar-parser.cpp -o grammar-parser.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c common/build-info.cpp -o build-info.o\r\ng++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation  -c common/console.cpp -o console.o\r\nnvcc --forward-unknown-to-host-compiler -use_fast_math -arch=compute_53 -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation \" -c ggml-cuda.cu -o ggml-cuda.o\r\nggml-cuda.cu(5933): error: identifier \"CUBLAS_TF32_TENSOR_OP_MATH\" is undefined\r\n\r\nggml-cuda.cu(6578): error: identifier \"CUBLAS_COMPUTE_16F\" is undefined\r\n\r\nggml-cuda.cu(7514): error: identifier \"CUBLAS_COMPUTE_16F\" is undefined\r\n\r\nggml-cuda.cu(7548): error: identifier \"CUBLAS_COMPUTE_16F\" is undefined\r\n\r\n4 errors detected in the compilation of \"/tmp/tmpxft_00001bc3_00000000-6_ggml-cuda.cpp1.ii\".\r\nMakefile:439: recipe for target 'ggml-cuda.o' failed\r\nmake: *** [ggml-cuda.o] Error 1\r\n```\r\n\r\n```\r\n$ cmake ..\r\n-- cuBLAS found\r\n-- Using CUDA architectures: 53\r\nGNU ld (GNU Binutils for Ubuntu) 2.30\r\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\r\n-- ARM detected\r\n-- Configuring done (0.4s)\r\n-- Generating done (0.9s)\r\n-- Build files have been written to: /home/rover/llama.cpp/build\r\ncmake --build . --config Release\r\n[  1%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q2_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: implicit declaration of function \u2018vld1q_s16_x2\u2019; did you mean \u2018vld1q_s16\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3680:41: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t mins16 = {vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))};\r\n                                         ^\r\n                                          {                                                                                                      }\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: implicit declaration of function \u2018vld1q_u8_x2\u2019; did you mean \u2018vld1q_u32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:406:27: error: implicit declaration of function \u2018vld1q_s8_x2\u2019; did you mean \u2018vld1q_s32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:406:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3723:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3725:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3727:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q3_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4353:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4371:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: implicit declaration of function \u2018vld1q_s8_x4\u2019; did you mean \u2018vld1q_s64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4373:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q4_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5273:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q4bits = ggml_vld1q_u8_x2(q4); q4 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:5291:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\n/home/rover/llama.cpp/ggml-quants.c:5300:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q5_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5918:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5926:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q5bits = ggml_vld1q_u8_x2(q5); q5 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5927:46: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q6_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6627:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:6629:43: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t q6scales = {vmovl_s8(vget_low_s8(scales)), vmovl_s8(vget_high_s8(scales))};\r\n                                           ^\r\n                                            {                                                            }\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6641:40: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh); qh += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:405:27: error: implicit declaration of function \u2018vld1q_u8_x4\u2019; did you mean \u2018vld1q_u64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:405:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6643:40: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:6686:21: error: incompatible types when assigning to type \u2018int8x16x4_t {aka struct int8x16x4_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                     ^\r\ncc1: some warnings being treated as errors\r\nCMakeFiles/ggml.dir/build.make:117: recipe for target 'CMakeFiles/ggml.dir/ggml-quants.c.o' failed\r\nmake[2]: *** [CMakeFiles/ggml.dir/ggml-quants.c.o] Error 1\r\nCMakeFiles/Makefile2:623: recipe for target 'CMakeFiles/ggml.dir/all' failed\r\nmake[1]: *** [CMakeFiles/ggml.dir/all] Error 2\r\nMakefile:145: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n```\r\n$ cmake .. -DLLAMA_CUBLAS=ON\r\n-- cuBLAS found\r\n-- Using CUDA architectures: 53\r\nGNU ld (GNU Binutils for Ubuntu) 2.30\r\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\r\n-- ARM detected\r\n-- Configuring done (0.3s)\r\n-- Generating done (0.3s)\r\n-- Build files have been written to: /home/rover/llama.cpp/build\r\n\r\n$ cmake --build . --config Release\r\n[  1%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q2_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: implicit declaration of function \u2018vld1q_s16_x2\u2019; did you mean \u2018vld1q_s16\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3679:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3680:41: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t mins16 = {vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))};\r\n                                         ^\r\n                                          {                                                                                                      }\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: implicit declaration of function \u2018vld1q_u8_x2\u2019; did you mean \u2018vld1q_u32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3716:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:406:27: error: implicit declaration of function \u2018vld1q_s8_x2\u2019; did you mean \u2018vld1q_s32\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:406:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x2  vld1q_s8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:3718:40: note: in expansion of macro \u2018ggml_vld1q_s8_x2\u2019\r\n             ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3723:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3725:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:3708:17: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n         q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\r\n                 ^\r\n/home/rover/llama.cpp/ggml-quants.c:3727:13: note: in expansion of macro \u2018SHIFT_MULTIPLY_ACCUM_WITH_SCALE\u2019\r\n             SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q3_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4353:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4371:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: implicit declaration of function \u2018vld1q_s8_x4\u2019; did you mean \u2018vld1q_s64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4372:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:4373:48: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                                ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q4_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5273:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q4bits = ggml_vld1q_u8_x2(q4); q4 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:5291:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\n/home/rover/llama.cpp/ggml-quants.c:5300:21: error: incompatible types when assigning to type \u2018int8x16x2_t {aka struct int8x16x2_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\r\n                     ^\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q5_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5918:36: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n         ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\r\n                                    ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5926:46: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             const ggml_uint8x16x2_t q5bits = ggml_vld1q_u8_x2(q5); q5 += 32;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:5927:46: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                              ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_q6_K_q8_K\u2019:\r\n/home/rover/llama.cpp/ggml-quants.c:403:27: error: invalid initializer\r\n #define ggml_vld1q_s16_x2 vld1q_s16_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6627:41: note: in expansion of macro \u2018ggml_vld1q_s16_x2\u2019\r\n         const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\r\n                                         ^~~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:6629:43: warning: missing braces around initializer [-Wmissing-braces]\r\n         const ggml_int16x8x2_t q6scales = {vmovl_s8(vget_low_s8(scales)), vmovl_s8(vget_high_s8(scales))};\r\n                                           ^\r\n                                            {                                                            }\r\n/home/rover/llama.cpp/ggml-quants.c:404:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x2  vld1q_u8_x2\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6641:40: note: in expansion of macro \u2018ggml_vld1q_u8_x2\u2019\r\n             ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh); qh += 32;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:405:27: error: implicit declaration of function \u2018vld1q_u8_x4\u2019; did you mean \u2018vld1q_u64\u2019? [-Werror=implicit-function-declaration]\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:405:27: error: invalid initializer\r\n #define ggml_vld1q_u8_x4  vld1q_u8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6642:40: note: in expansion of macro \u2018ggml_vld1q_u8_x4\u2019\r\n             ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:407:27: error: invalid initializer\r\n #define ggml_vld1q_s8_x4  vld1q_s8_x4\r\n                           ^\r\n/home/rover/llama.cpp/ggml-quants.c:6643:40: note: in expansion of macro \u2018ggml_vld1q_s8_x4\u2019\r\n             ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                                        ^~~~~~~~~~~~~~~~\r\n/home/rover/llama.cpp/ggml-quants.c:6686:21: error: incompatible types when assigning to type \u2018int8x16x4_t {aka struct int8x16x4_t}\u2019 from type \u2018int\u2019\r\n             q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\r\n                     ^\r\ncc1: some warnings being treated as errors\r\nCMakeFiles/ggml.dir/build.make:117: recipe for target 'CMakeFiles/ggml.dir/ggml-quants.c.o' failed\r\nmake[2]: *** [CMakeFiles/ggml.dir/ggml-quants.c.o] Error 1\r\nCMakeFiles/Makefile2:623: recipe for target 'CMakeFiles/ggml.dir/all' failed\r\nmake[1]: *** [CMakeFiles/ggml.dir/all] Error 2\r\nMakefile:145: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n\r\nSimilar error to #3880, \r\n\r\nI'm not sure what more to do, or if it is even supported, because the Nvidia Jetson Nano is running Ubuntu 18.04 with cuda 10.2, which is older but I cannot upgrade. Would really appreciate if someone could help me figure this out, if I need to provide more information let me know! ",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-16T11:56:41+00:00",
    "closed_at": "2024-03-24T11:35:49+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4099/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4099"
  },
  {
    "number": 2990,
    "title": "Converting GGML->GGUF: ValueError: Only GGJTv3 supported",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nMy GGML converted models should be easy to convert to GGUF.\r\nI know the conversion tools aren't guaranteed but I'd like to file this one in case anybody else has a workaround or more version flexible option. I would love to see any version of GGML/GGJT supported if possible. Instead my GGML files converted earlier are apparently not supported for conversion to GGUF.\r\n\r\nIs there any tool to show the standard version details of a model file? Happy to contribute one if there isn't.\r\n\r\n# Current Behavior\r\n\r\n```\r\npython3 ./convert-llama-ggmlv3-to-gguf.py -i llama-2-70b/ggml-model-f32.bin -o test.gguf\r\n=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===\r\n\r\n* Scanning GGML input file\r\nTraceback (most recent call last):\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 353, in <module>\r\n    main()\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 335, in main\r\n    offset = model.load(data, 0)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 125, in load\r\n    offset += self.validate_header(data, offset)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 121, in validate_header\r\n    raise ValueError('Only GGJTv3 supported')\r\nValueError: Only GGJTv3 supported\r\n```\r\n# Environment and Context\r\n\r\nWorking with models\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\nPhysical Fedora 38, probably irrelevant give the Python.\r\n\r\n`$ lscpu`\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  56\r\n  On-line CPU(s) list:   0-55\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Xeon(R) CPU E5-2660 v4 @ 2.00GHz\r\n    CPU family:          6\r\n    Model:               79\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  14\r\n    Socket(s):           2\r\n    Stepping:            1\r\n    CPU(s) scaling MHz:  40%\r\n    CPU max MHz:         3200.0000\r\n    CPU min MHz:         1200.0000\r\n    BogoMIPS:            3990.92\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts a\r\n                         cpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_per\r\n                         fmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes\r\n                         64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_\r\n                         2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowp\r\n                         refetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb st\r\n                         ibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bm\r\n                         i2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc\r\n                          cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts vnmi md_clear flush_l1d\r\nVirtualization features: \r\n  Virtualization:        VT-x\r\nCaches (sum of all):     \r\n  L1d:                   896 KiB (28 instances)\r\n  L1i:                   896 KiB (28 instances)\r\n  L2:                    7 MiB (28 instances)\r\n  L3:                    70 MiB (2 instances)\r\nNUMA:                    \r\n  NUMA node(s):          2\r\n  NUMA node0 CPU(s):     0-13,28-41\r\n  NUMA node1 CPU(s):     14-27,42-55\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\nLinux z840 6.4.12-200.fc38.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Aug 23 17:46:49 UTC 2023 x86_64 GNU/Linux\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.11.4\r\n$ make --version\r\n$ g++ --version\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\n```\r\npython3 ./convert-llama-ggmlv3-to-gguf.py -i llama-2-70b/ggml-model-f32.bin -o test.gguf\r\n=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===\r\n\r\n* Scanning GGML input file\r\nTraceback (most recent call last):\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 353, in <module>\r\n    main()\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 335, in main\r\n    offset = model.load(data, 0)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 125, in load\r\n    offset += self.validate_header(data, offset)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[PATH]/llama.cpp/convert-llama-ggmlv3-to-gguf.py\", line 121, in validate_header\r\n    raise ValueError('Only GGJTv3 supported')\r\nValueError: Only GGJTv3 supported\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. step 1 convert any of the PTH models to GGML (using previous unversioned commits of convert)\r\n2. step 2 convert the GGML to GGUF with the command given above.\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-03T11:07:07+00:00",
    "closed_at": "2023-09-06T08:49:12+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2990/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2990"
  },
  {
    "number": 1333,
    "title": "Implement MosiacML's 7B model.",
    "body": "Comparative to Llama in results I believe and also commercially available for use!\r\n\r\nhttps://huggingface.co/mosaicml/mpt-7b\r\n\r\nhttps://www.mosaicml.com/blog/mpt-7b",
    "labels": [
      "help wanted",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-05-05T17:44:48+00:00",
    "closed_at": "2023-11-02T00:54:44+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1333/reactions",
      "total_count": 72,
      "+1": 54,
      "-1": 0,
      "laugh": 0,
      "hooray": 9,
      "confused": 0,
      "heart": 0,
      "rocket": 9,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1333"
  },
  {
    "number": 10843,
    "title": "Eval bug: Qwen2-VL Hallucinates image content on Vulkan backend",
    "body": "### Name and Version\r\n\r\n.\\build\\bin\\Release\\llama-cli.exe --version\r\n\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = AMD Radeon RX 5700 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: none\r\nversion: 4329 (89d604f2)\r\nbuilt with MSVC 19.41.34120.0 for x64\r\n\r\n### Operating systems\r\n\r\nWindows\r\n\r\n### GGML backends\r\n\r\nVulkan\r\n\r\n### Hardware\r\n\r\nRyzen 5900X +RX 5700 XT\r\n\r\n### Models\r\n\r\nQwen2-VL-7B-Instruct-IQ4_NL + mmproj-Qwen2-VL-7B-Instruct-f32\r\n\r\n### Problem description & steps to reproduce\r\n\r\nWhen I run it on Vulkan build, the description given by the model has nothing to do with the image given as argument (no matter the `-ngl` value, even `-ngl 0` is broken). The exact same setup works perfectly fine on CPU backend.\r\n\r\nI know the Vulkan backend doesn't support Qwen2-VL yet, but according to https://github.com/ggerganov/llama.cpp/pull/10361#issuecomment-2543938139, this should only cause slowdowns, not invalid outputs.\r\n\r\n### Relevant log output\r\n\r\n#### Image input:\r\n![Untitled](https://github.com/user-attachments/assets/7477d53e-2ee0-478a-aeb5-d82c5e44ca3a)\r\n\r\n\r\n#### -ngl 0\r\n```shell\r\n> .\\build\\bin\\Release\\llama-qwen2vl-cli.exe -m .\\models\\Qwen2-VL-7B-Instruct-IQ4_NL.gguf --mmproj .\\models\\mmproj-Qwen2-VL-7B-Instruct-f32.gguf -p 'What could be the context of this image.' --image '.\\Pictures\\Untitled.png' --seed 0 --temp 0\r\n[...]\r\nencode_image_with_clip: step 1 of 1 encoded in   843.10 ms\r\nencode_image_with_clip: all 1 segments encoded in   843.17 ms\r\nencode_image_with_clip: load_image_size 512 512\r\nencode_image_with_clip: image embedding created: 361 tokens\r\n\r\nencode_image_with_clip: image encoded in   845.06 ms by CLIP (    2.34 ms per image patch)\r\n\r\nThe image shows a person wearing a black and white striped shirt, a black jacket, and black pants, standing in front of a black background. The person is also holding a black and white striped umbrella. The context of this image could be a fashion or clothing advertisement, showcasing the person's outfit and accessories. The black and white striped shirt, jacket, and umbrella create a monochromatic look, which is often used in fashion photography to emphasize the clothing and accessories. The black background helps to highlight the person and their outfit, making them the focal point of the image.\r\nllama_perf_context_print:        load time =    6644.91 ms\r\nllama_perf_context_print: prompt eval time =    2276.84 ms /   391 tokens (    5.82 ms per token,   171.73 tokens per second)\r\nllama_perf_context_print:        eval time =   11500.85 ms /   115 runs   (  100.01 ms per token,    10.00 tokens per second)\r\nllama_perf_context_print:       total time =   18275.28 ms /   506 tokens\r\n```\r\n\r\n#### -ngl 99\r\n```shell\r\n> .\\build\\bin\\Release\\llama-qwen2vl-cli.exe -m .\\models\\Qwen2-VL-7B-Instruct-IQ4_NL.gguf --mmproj .\\models\\mmproj-Qwen2-VL-7B-Instruct-f32.gguf -p 'What could be the context of this image.' --image '.\\Pictures\\Untitled.png' --seed 0 --temp 0 -ngl 99\r\n[...]\r\nencode_image_with_clip: step 1 of 1 encoded in  3248.68 ms\r\nencode_image_with_clip: all 1 segments encoded in  3248.76 ms\r\nencode_image_with_clip: load_image_size 512 512\r\nencode_image_with_clip: image embedding created: 361 tokens\r\n\r\nencode_image_with_clip: image encoded in  3249.79 ms by CLIP (    9.00 ms per image patch)\r\n\r\nThe image appears to be a logo or a symbol, but it is not clear what it represents. It could be a brand logo, a company logo, or a symbol for a specific organization or group. Without additional context or information, it is difficult to determine the exact meaning or purpose of the image.\r\nllama_perf_context_print:        load time =    9346.17 ms\r\nllama_perf_context_print: prompt eval time =    1009.47 ms /   391 tokens (    2.58 ms per token,   387.33 tokens per second)\r\nllama_perf_context_print:        eval time =    1500.12 ms /    61 runs   (   24.59 ms per token,    40.66 tokens per second)\r\nllama_perf_context_print:       total time =   10889.94 ms /   452 tokens\r\n```\r\n#### CPU backend for comparison\r\n```shell\r\n> .\\buildcpu\\bin\\Release\\llama-qwen2vl-cli.exe -m .\\models\\Qwen2-VL-7B-Instruct-IQ4_NL.gguf --mmproj .\\models\\mmproj-Qwen2-VL-7B-Instruct-f32.gguf -p 'What could be the context of this image.' --image '.\\Pictures\\Untitled.png' --seed 0 --temp 0\r\n[...]\r\nencode_image_with_clip: step 1 of 1 encoded in  8483.38 ms\r\nencode_image_with_clip: all 1 segments encoded in  8483.47 ms\r\nencode_image_with_clip: load_image_size 512 512\r\nencode_image_with_clip: image embedding created: 361 tokens\r\n\r\nencode_image_with_clip: image encoded in  8484.85 ms by CLIP (   23.50 ms per image patch)\r\n\r\nThe image appears to be a simple text-based graphic with the words \"READABLE TEXT\" written in a bold, black font. The context of this image could be related to demonstrating or emphasizing the importance of clear and legible text, possibly in the context of design, typography, or user interface (UI) design. It might be used to highlight the importance of making text easy to read and understand for users.\r\nllama_perf_context_print:        load time =   21741.16 ms\r\nllama_perf_context_print: prompt eval time =   10924.92 ms /   391 tokens (   27.94 ms per token,    35.79 tokens per second)\r\nllama_perf_context_print:        eval time =    8322.39 ms /    83 runs   (  100.27 ms per token,     9.97 tokens per second)\r\nllama_perf_context_print:       total time =   30185.33 ms /   474 tokens\r\n``` ",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-15T17:36:55+00:00",
    "closed_at": "2025-02-17T22:43:03+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10843/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10843"
  },
  {
    "number": 12597,
    "title": "Misc. bug: \"Unexpected empty grammar stack after accepting piece\" tool crash",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\nversion: 4958 (ef19c717)\nbuilt with Ubuntu clang version 18.1.8 (++20240731024944+3b5b5c1ec4a3-1~exp1~20240731145000.144) for x86_64-pc-linux-gnu\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 llama-server --ctx-size 0 --jinja -fa -hf bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M --host 0.0.0.0 -ngl 100\n```\n\n### Problem description & steps to reproduce\n\nIf I run BFVL v3 on the above `llama-server`, it eventually  crashes with:\n\n```\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Unexpected empty grammar stack after accepting piece: ```json\n{\n    \"\n/home/ed/.local/share/dorothy/user/commands/llama-cpp-server: line 69: 736575 Aborted  \n```\n\n```bash\ncurl -X POST http://localhost:8080/v1/chat/completions -d \\{\\\"messages\\\":\\ \\[\\{\\\"role\\\":\\ \\\"user\\\"\\,\\ \\\"content\\\":\\ \\\"\\\\nWhat\\'s\\ the\\ weather\\ like\\ in\\ the\\ two\\ cities\\ of\\ Boston\\ and\\ San\\ Francisco\\?\\ Your\\ output\\ are\\ the\\ function\\ calling\\ and\\ should\\ be\\ json\\ dict\\ format\\\\nBelow\\ is\\ the\\ given\\ function\\\\nfunctions\\ =\\ \\[\\\\n\\ \\ \\ \\ \\{\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"name\\\\\\\":\\ \\\\\\\"get_current_weather\\\\\\\"\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"description\\\\\\\":\\ \\\\\\\"Get\\ the\\ current\\ weather\\ in\\ a\\ given\\ location\\\\\\\"\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"parameters\\\\\\\":\\ \\{\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"type\\\\\\\":\\ \\\\\\\"object\\\\\\\"\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"properties\\\\\\\":\\ \\{\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"location\\\\\\\":\\ \\{\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"type\\\\\\\":\\ \\\\\\\"string\\\\\\\"\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"description\\\\\\\":\\ \\\\\\\"The\\ city\\ and\\ state\\,\\ e.g.\\ San\\ Francisco\\\\\\\"\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\}\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"unit\\\\\\\":\\ \\{\\\\\\\"type\\\\\\\":\\ \\\\\\\"string\\\\\\\"\\,\\ \\\\\\\"enum\\\\\\\":\\ \\[\\\\\\\"celsius\\\\\\\"\\,\\ \\\\\\\"fahrenheit\\\\\\\"\\]\\}\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\}\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\\"required\\\\\\\":\\ \\[\\\\\\\"location\\\\\\\"\\]\\,\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\}\\,\\\\n\\ \\ \\ \\ \\}\\\\n\\]\\\\n\\\"\\}\\]\\,\\ \\\"model\\\":\\ \\\"gpt-4-turbo-2024-04-09\\\"\\,\\ \\\"temperature\\\":\\ 0.001\\}\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-26T17:30:06+00:00",
    "closed_at": "2025-04-16T14:32:28+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12597/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12597"
  },
  {
    "number": 105,
    "title": "Create a logo",
    "body": "We should probably make a logo for this project. Like an image of a \ud83e\udd99 and some C++",
    "labels": [
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-13T21:15:21+00:00",
    "closed_at": "2023-07-28T19:20:49+00:00",
    "comments": 47,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/105/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/105"
  },
  {
    "number": 7661,
    "title": "When using GPU (OpenCL), the reply speed is slower and all replies are incorrect\uff1f\uff1f",
    "body": "### What happened?\n\nI used termux to compile llama.cpp with gpu.  and i found that the response speed has been slower and they are all errors\u3002\r\n\r\n\r\n\r\nlike:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/46549527/e5009839-a3fa-432c-bd7e-3e5827dcf5db)\r\nerror response:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/46549527/763810a2-ba27-43d0-9077-abf9a1b58d72)\r\n\r\nWho can tell me what the reason is\uff1f\uff1f\uff1f\uff1f\r\n\n\n### Name and Version\n\n./bin/main -t 8 -ngl 33 -m ../llama-2-7b-chat.Q4_0.gguf --color -n -1 -ins -b 256\r\n\r\nenvironment\uff1a  linux+termux   GPU: Qualcomm  8gen2\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-05-31T07:45:45+00:00",
    "closed_at": "2024-09-16T01:07:32+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7661/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7661"
  },
  {
    "number": 12080,
    "title": "Eval bug: getting assertion error when trying to use a gguf quantized model at inference \"GGML_ASSERT(n_outputs_enc > 0 && \"call llama_encode() first\") failed\"",
    "body": "### Name and Version\n\nthe latest version of llama.cpp\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nCPU 16 GB RAM Intel I5 core 10th Gen\n\n### Models\n\nFlan T5 Large\n\n### Problem description & steps to reproduce\n\nI have a finetuned Flan T5 Model in my local which I have quantized and converted to gguf format using llama.cpp using the below line of command:\n\n!python {path to convert_hf_to_gguf.py} {path to hf_model} --outfile {name_of_outputfile.gguf} --outtype {quantization type}\n\nand loaded the gguf file using llama.cpp Llama\n\nfrom llama_cpp import Llama \ngguf_model_path = \"t5_8bit.gguf\"\nmodel = Llama(model_path=gguf_model_path)\n\nand when trying to use the model at inference in Jupyter Notebook, the kernel is Dying. When tried the same in Command Prompt, getting the aasertion issue \"GGML_ASSERT(n_outputs_enc > 0 && \"call llama_encode() first\") failed\"\n\nused the below code for inference in CPU and the issue is detected at model.eval()\n\nCode:\nprompt = \"Extract Tags and Relevant Text: Please Annotate that the market rates has fallen drastically\"\n\ntokens = model.tokenize(prompt.encode())\noutput_tokens = model.eval(tokens)\noutput = model.detokenize(tokens)\nprint(output)\n\nWhy is this issue coming, and what is the solution to it. I am trying to use quantized models in the Local for inference\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nfrom llama_cpp import Llama \ngguf_model_path = \"t5_8bit.gguf\"\nmodel = Llama(model_path=gguf_model_path)\n\nprompt = \"Extract Tags and Relevant Text: Please Annotate that the market rates has fallen drastically\"\n\n\ntokens = model.tokenize(prompt.encode())\noutput_tokens = model.eval(tokens)\noutput = model.detokenize(tokens)\nprint(output)\n\n\noutput: \nGGML_ASSERT(n_outputs_enc > 0 && \"call llama_encode() first\") failed\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-26T10:41:48+00:00",
    "closed_at": "2025-05-04T01:08:01+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12080/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12080"
  },
  {
    "number": 1037,
    "title": "4bit version of gpt4all-alpaca-oa-codealpaca-Lora-13b?",
    "body": "Hello, \nto reduce my brain usage even more I thought i'd be nice to run AI which is specifically trained to code and thus hopefully make better code than other language models which are trained for e.g. natural language.\n\nSo I found this: https://huggingface.co/jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b\n\nI of course wanted to try and run it but there's a problem, there aren't even any pytorch_model files or any 4bit variants listed here: https://github.com/underlines/awesome-marketing-datascience/blob/master/awesome-ai.md\n\nThank your for your support!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-18T04:47:11+00:00",
    "closed_at": "2023-05-08T09:34:10+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1037/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1037"
  },
  {
    "number": 103,
    "title": "How to build on windows?",
    "body": "Please give instructions. There is nothing in README but it says that it supports it ",
    "labels": [
      "documentation",
      "good first issue",
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-13T20:13:14+00:00",
    "closed_at": "2023-07-28T19:20:41+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/103/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/103"
  }
]
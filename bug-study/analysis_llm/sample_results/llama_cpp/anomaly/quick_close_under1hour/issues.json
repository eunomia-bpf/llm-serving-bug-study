[
  {
    "number": 3987,
    "title": "Question on recommended resources",
    "body": "Love this project!\r\nI am currently only writing Python and don't really understand what you did here, can you recommend some resources for converting models to C++ and to use hardware acceleration on Mac?\r\nThank you in advance!",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-11-08T05:39:37+00:00",
    "closed_at": "2023-11-08T05:53:16+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3987/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3987"
  },
  {
    "number": 1559,
    "title": "ggjt v2 models don't load (or error gracefully)",
    "body": "I freshly pulled 7e4ea5beff and `make clean && make`d and it fails to load a model converted from pytorch using the tools from revision 63d2046 (using https://github.com/akx/ggify):\r\n\r\n```\r\nllama.cpp: loading model from models/ausboss-llama-30b-supercot-q8_0.bin\r\nerror loading model: llama.cpp: tensor '\ufffd+\ufffd \ufffd\ufffds\ufffd\ufffd93:\ufffda-\ufffd%\ufffd\ufffdY\ufffd\ufffd8\u01810\ufffd&\ufffdM,\ufffd9\ufffd4\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\"/\ufffd@\ufffd\u0579\u007f\"*+c\ufffd5\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd9\ufffd>+n\ufffd\ufffd!\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdO...' should not be 2563577093-dimensional\r\nllama_init_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model 'models/ausboss-llama-30b-supercot-q8_0.bin'\r\nmain: error: unable to load model\r\n```\r\n\r\nI re-converted the model with 7e4ea5beff; apparently the old file had been \r\n\r\n```\r\nllama_model_load_internal: format     = ggjt v2 (latest)\r\n```\r\nand the new one is\r\n```\r\nllama_model_load_internal: format     = ggjt v3 (latest)\r\n```\r\n(and 6% smaller!)\r\n\r\nIt would be nice if there was an error saying that ggjt v2 is not supported, instead of dumping out garbage tensor names and mind-bendingly large tensor dimensionalities \ud83d\ude01 but I suppose this doesn't necessarily need any action right now.\r\n\r\nThis seems to be related to \r\n\r\n* https://github.com/ggerganov/llama.cpp/issues/1525#issuecomment-1556218247\r\n* https://github.com/ggerganov/llama.cpp/issues/1537",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-22T06:47:05+00:00",
    "closed_at": "2023-05-22T07:22:49+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1559/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1559"
  },
  {
    "number": 6785,
    "title": "Flaky server responses with llama 3",
    "body": "I noticed that some of the responses I got from llama-cpp server (latest master) are unnaturally fast for 70b model, and it happens randomly. And when this happens the response has worse quality. The model I'm using is https://huggingface.co/NousResearch/Meta-Llama-3-70B-Instruct-GGUF/blob/main/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf with the command line `llama-server -m Meta-Llama-3-70B-Instruct-Q5_K_M.gguf -c 0 -t 24 -ngl 24`. It's only partially offloaded to gpu (with rocm on linux) so maybe somehow llama-cpp doesn't use all layers when it responds quickly.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-20T14:15:11+00:00",
    "closed_at": "2024-04-20T14:18:06+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6785/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6785"
  },
  {
    "number": 1072,
    "title": "Add OpenCL clBLAS support",
    "body": "Please consider adding OpenCL clBLAS Support similar to what as Done in [Pull Request 1044](https://github.com/ggerganov/llama.cpp/pull/1044)\r\n\r\nHere is one such [Library ](https://github.com/clMathLibraries/clBLAS)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-19T21:45:32+00:00",
    "closed_at": "2023-04-19T22:08:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1072/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1072"
  },
  {
    "number": 9642,
    "title": "Feature Request: Add support for LLaMA 3.2 ",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd gguf support for new Llama 3.2 models released today (https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf). Tried converting the 1B parameter model to .gguf and ran into tokenizer issues as I believe there is now a new tokenizer being used. \r\n\r\nRunning: `version: 3826 (ea9c32be)` built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\r\n\r\nSteps to reproduce:\r\nRun `python convert_hf_to_gguf.py /yourmodeldir`\r\nWill give assertion error in `/gguf-py/gguf/vocab.py`\r\n```\r\n        if is_llama3:\r\n            raise TypeError('Llama 3 must be converted with BpeVocab')\r\n```\r\nIf adding `tokenizer.model` another error is thrown.\r\n\r\nCommenting out this line and quantizing works until trying to inference on the resulting `.gguf` file.\r\n```\r\n./llama-cli -m examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\"\r\nbuild: 3826 (ea9c32be) with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 147 tensors from examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Small Llama\r\nllama_model_loader: - kv   3:                         general.size_label str              = 1.2B\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 16\r\nllama_model_loader: - kv   5:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\r\nllama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\r\nllama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\r\nllama_model_loader: - kv  14:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128259]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,128259]  = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128259]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   34 tensors\r\nllama_model_loader: - type q4_K:   96 tensors\r\nllama_model_loader: - type q6_K:   17 tensors\r\nllm_load_vocab: SPM vocabulary, but newline token not found: unordered_map::at: key not found! Using special_pad_id instead.llm_load_vocab: control-looking token: '<|eot_id|>' was not control-type; this is probably a bug in the model. its type will be overridden\r\nllm_load_vocab: control-looking token: '<|eom_id|>' was not control-type; this is probably a bug in the model. its type will be overridden\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 259\r\nllm_load_vocab: token to piece cache size = 1.0237 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_layer          = 16\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 1.24 B\r\nllm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \r\nllm_load_print_meta: general.name     = Small Llama\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 2 '#'\r\nllm_load_print_meta: UNK token        = 0 '!'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 2 '#'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllama_model_load: error loading model: vocab size mismatch\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: failed to load model 'examples/quantize/small-llama/Llama-3.2-1B-Instruct-Q4_K_M.gguf'\r\nmain: error: unable to load model\r\n```\n\n### Motivation\n\nThese are new SOTA models and are extremely impactful for edge devices, and the larger models as well for full multimodal support. \n\n### Possible Implementation\n\nAdding support for the new tokenizer.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-09-25T19:48:36+00:00",
    "closed_at": "2024-09-25T19:51:33+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9642/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9642"
  },
  {
    "number": 14176,
    "title": "Compile bug: llama-vocab.cpp Error",
    "body": "### Git commit\n\n40643edb86eb10b471b0f57d4f3f7eb0e06a0df7\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nBLAS\n\n### Problem description & steps to reproduce\n\n# Bug\n# When It happend\nWhen I used `make -j2` to compile llama.cpp\n\nIt threw a error:\n```\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp: In function \u2018std::wstring unicode_wstring_from_utf8(const std::string&)\u2019:\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp:209:10: warning: \u2018template<class _Codecvt, class _Elem, class _Wide_alloc, class _Byte_alloc> class std::__cxx11::wstring_convert\u2019 is deprecated [-Wdeprecated-declarations]\n  209 |     std::wstring_convert<std::codecvt_utf8<wchar_t>> conv;\n      |          ^~~~~~~~~~~~~~~\nIn file included from /usr/include/c++/15.1.1/locale:47,\n                 from /run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp:13:\n/usr/include/c++/15.1.1/bits/locale_conv.h:262:33: note: declared here\n  262 |     class _GLIBCXX17_DEPRECATED wstring_convert\n      |                                 ^~~~~~~~~~~~~~~\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp: In lambda function:\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:46: error: \u2018numeric_limits\u2019 is not a member of \u2018std\u2019\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                              ^~~~~~~~~~~~~~\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:68: error: expected primary-expression before \u2018>\u2019 token\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                                                    ^\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:71: error: \u2018::max\u2019 has not been declared; did you mean \u2018std::max\u2019?\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                                                       ^~~\n      |                                                                       std::max\nIn file included from /usr/include/c++/15.1.1/algorithm:63,\n                 from /run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:10:\n/usr/include/c++/15.1.1/bits/stl_algo.h:5794:5: note: \u2018std::max\u2019 declared here\n 5794 |     max(initializer_list<_Tp> __l, _Compare __comp)\n      |     ^~~\nmake[2]: *** [src/CMakeFiles/llama.dir/build.make:387: src/CMakeFiles/llama.dir/llama-vocab.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nmake[1]: *** [CMakeFiles/Makefile2:1233: src/CMakeFiles/llama.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\n```\n\n# My Cmake compilation opinion\n\n```\nGGML_BLAS=ON\nGGML_BLAS_VENDOR=OpenBLAS\nGGML_CPU=ON\nGGML_SSE42=ON\nGGML_NATIVE=ON\n```\nEvery other opinions are all default.\n\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS -DGGML_CPU=ON -DGGML_SSE42=ON -DGGML_NATIVE=ON\n\nmake -j2\n```\n\n### Relevant log output\n\n```shell\n[  5%] Built target ggml-base\n[  5%] Built target build_info\n[  6%] Built target sha256\n[  7%] Built target xxhash\n[  7%] Built target sha1\n[  8%] Built target llama-llava-cli\n[  9%] Built target llama-gemma3-cli\n[ 10%] Built target llama-minicpmv-cli\n[ 12%] Built target llama-qwen2vl-cli\n[ 13%] Built target ggml-blas\n[ 23%] Built target ggml-cpu\n[ 24%] Built target ggml\n[ 25%] Built target llama-gguf-hash\n[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[ 27%] Built target llama-gguf\n[ 27%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp: In function \u2018std::wstring unicode_wstring_from_utf8(const std::string&)\u2019:\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp:209:10: warning: \u2018template<class _Codecvt, class _Elem, class _Wide_alloc, class _Byte_alloc> class std::__cxx11::wstring_convert\u2019 is deprecated [-Wdeprecated-declarations]\n  209 |     std::wstring_convert<std::codecvt_utf8<wchar_t>> conv;\n      |          ^~~~~~~~~~~~~~~\nIn file included from /usr/include/c++/15.1.1/locale:47,\n                 from /run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/unicode.cpp:13:\n/usr/include/c++/15.1.1/bits/locale_conv.h:262:33: note: declared here\n  262 |     class _GLIBCXX17_DEPRECATED wstring_convert\n      |                                 ^~~~~~~~~~~~~~~\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp: In lambda function:\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:46: error: \u2018numeric_limits\u2019 is not a member of \u2018std\u2019\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                              ^~~~~~~~~~~~~~\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:68: error: expected primary-expression before \u2018>\u2019 token\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                                                    ^\n/run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:2576:71: error: \u2018::max\u2019 has not been declared; did you mean \u2018std::max\u2019?\n 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {\n      |                                                                       ^~~\n      |                                                                       std::max\nIn file included from /usr/include/c++/15.1.1/algorithm:63,\n                 from /run/media/dust/879c925c-44bf-4fe7-8234-27eb11ca228e/home/dust/llama/llama.cpp/src/llama-vocab.cpp:10:\n/usr/include/c++/15.1.1/bits/stl_algo.h:5794:5: note: \u2018std::max\u2019 declared here\n 5794 |     max(initializer_list<_Tp> __l, _Compare __comp)\n      |     ^~~\nmake[2]: *** [src/CMakeFiles/llama.dir/build.make:387: src/CMakeFiles/llama.dir/llama-vocab.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nmake[1]: *** [CMakeFiles/Makefile2:1233: src/CMakeFiles/llama.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-13T17:26:13+00:00",
    "closed_at": "2025-06-13T17:31:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14176/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14176"
  },
  {
    "number": 6614,
    "title": "llama_tensor_get_type falling back when not necessary?",
    "body": "Noticed while making some quants for Q2_K that I was getting messages:\r\n\r\nllama_tensor_get_type : tensor cols 14464 x 4096 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\r\n\r\nBut by my math, it definitely is? Anything multiplied by 4096 should be. Is the error message misleading or is there some accidental miscalculation going on?",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-11T17:32:32+00:00",
    "closed_at": "2024-04-11T17:38:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6614/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 1,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6614"
  },
  {
    "number": 1737,
    "title": "`save_load_state` example segfaulting after adding Metal inference",
    "body": "# Expected Behavior\r\n\r\nThe example saves and loads a state.\r\n\r\n# Current Behavior\r\n\r\nThe example crashes with a segmentation fault.\r\n\r\n# Environment and Context\r\n\r\nAccording to git bisect the first commit that causes a segmentation fault is `master-ecb-217d`, the one where Metal inference was added.\r\n\r\nHardware:\r\n\r\n<details>\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n```Architecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         43 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 3700X 8-Core Processor\r\n    CPU family:          23\r\n    Model:               113\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            0\r\n    Frequency boost:     enabled\r\n    CPU(s) scaling MHz:  77%\r\n    CPU max MHz:         4935.9370\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            7202.09\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr\r\n                         _opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3\r\n                          fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalign\r\n                         sse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pst\r\n                         ate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsav\r\n                         ec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock\r\n                          nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umi\r\n                         p rdpid overflow_recov succor smca sev sev_es\r\nVirtualization features: \r\n  Virtualization:        AMD-V\r\nCaches (sum of all):     \r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    4 MiB (8 instances)\r\n  L3:                    32 MiB (2 instances)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Linux johannes-pc 6.3.0-1-MANJARO #1 SMP PREEMPT_DYNAMIC Mon Apr  3 10:46:56 UTC 2023 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nPython 3.10.10\r\nGNU Make 4.4.1\r\ng++ (GCC) 12.2.1 20230201\r\n```\r\n\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n```\r\ngit checkout master-ecb217d\r\nmake clean && make save-load-state\r\n./save-load-state --model path/to/model.bin\r\n```\r\n\r\n# Failure Logs\r\n\r\nThe GDB output for the segfault:\r\n\r\n```\r\nThread 1 \"save-load-state\" received signal SIGSEGV, Segmentation fault.\r\n0x000055555556e5fd in ggml_view_3d (ctx=0x55555569da68 <g_state+200>, a=0x7ffb83bff030, ne0=6656, ne1=6, ne2=60, nb1=13312, nb2=6815744, offset=0)\r\n    at ggml.c:5901\r\n5901        memcpy(offs->data, &offset, 2*sizeof(int32_t));\r\n(gdb) bt\r\n#0  0x000055555556e5fd in ggml_view_3d (ctx=0x55555569da68 <g_state+200>, a=0x7ffb83bff030, ne0=6656, ne1=6, ne2=60, nb1=13312, nb2=6815744, \r\n    offset=0) at ggml.c:5901\r\n#1  0x000055555559b73e in llama_copy_state_data (ctx=0x5555556b22c0, dst=0x7ffac15c9010 \":\\032\") at llama.cpp:2751\r\n#2  0x000055555555afa7 in main (argc=3, argv=0x7fffffffd778) at examples/save-load-state/save-load-state.cpp:59\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-06-07T08:40:08+00:00",
    "closed_at": "2023-06-07T08:47:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1737/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1737"
  },
  {
    "number": 655,
    "title": "Error: Invalid model file when using converted GPT4ALL model after following provided instructions",
    "body": "Hello,\r\n\r\nI have followed the instructions provided for using the GPT-4ALL model. I used the `convert-gpt4all-to-ggml.py` script to convert the `gpt4all-lora-quantized.bin` model, as instructed. However, I encountered an error related to an invalid model file when running the example. \r\n\r\nHere are the steps I followed, as described in the instructions:\r\n\r\n1. Convert the model using the `convert-gpt4all-to-ggml.py` script:\r\n```\r\npython3 convert-gpt4all-to-ggml.py models/gpt4all/gpt4all-lora-quantized.bin ./models/tokenizer.model\r\n```\r\n\r\n2. Run the `interactive mode` example with the newly generated `gpt4all-lora-quantized.bin` model:\r\n```\r\n./main -m ./models/gpt4all/gpt4all-lora-quantized.bin -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\r\n```\r\n\r\nHowever, I encountered the following error:\r\n```\r\n./models/gpt4all/gpt4all-lora-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\nyou most likely need to regenerate your ggml files\r\nthe benefit is you'll get 10-100x faster load times\r\nsee https://github.com/ggerganov/llama.cpp/issues/91\r\nuse convert-pth-to-ggml.py to regenerate from original pth\r\nuse migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/gpt4all/gpt4all-lora-quantized.bin'\r\n```\r\n\r\nPlease let me know how to resolve this issue and correctly convert and use the GPT-4ALL model with the `interactive mode` example.\r\n\r\nThank you.\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T17:13:52+00:00",
    "closed_at": "2023-03-31T17:55:16+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/655"
  },
  {
    "number": 12283,
    "title": "Misc. bug: GPU Support Missing in Version >=0.3.5 on Windows with CUDA 12.4 and RTX 3090",
    "body": "### Name and Version\n\nI'm experiencing a discrepancy between version 0.3.4 and later versions (>=0.3.5) regarding GPU utilization:\n\nVersion 0.3.4 (Prebuilt Wheel):\nThe prebuilt wheel for 0.3.4 loads the model onto the GPU; however, it's not compatible with phi4.\n\nVersion >=0.3.5:\nThere are no prebuilt wheels available for these versions, and when building from source, only the CPU is being used\u2014the model does not load onto the GPU.\n\nSystem Details:\n\nOperating System: Windows 11\nCUDA Version: 12.4\nGPU: RTX 3090 24GB\n\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nSteps Taken:\n\nInstalled version 0.3.4 via the prebuilt wheel \u2013 confirmed GPU loading (but phi4 incompatibility remains).\nUpgraded to version 0.3.5 (and above) by building from source with CUDA support enabled.\nVerified that the build settings include -DGGML_CUDA=on and confirmed that the system has CUDA 12.4 installed.\nDespite these configurations, the build defaults to CPU usage, and the model never loads onto the GPU.\n\nCould you please advise on whether this is an expected behavior for versions >=0.3.5, or if there might be an issue with GPU detection/configuration on Windows 11 with CUDA 12.4? Any guidance or troubleshooting steps to enable GPU support for these versions would be greatly appreciated.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-09T09:44:39+00:00",
    "closed_at": "2025-03-09T09:54:59+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12283"
  },
  {
    "number": 14307,
    "title": "Misc. bug: RPC immediate closing of the connection",
    "body": "### Name and Version\n\nversion: 5716 (d27b3ca1)\nbuilt with clang version 18.1.8 for x86_64-pc-windows-msvc\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\n.\\rpc-server.exe -H 0.0.0.0 -p 42227\n\n.\\llama-cli.exe --rpc 192.168.100.178:42227 -m \"E:\\LLMs\\bartowski\\Qwen_Qwen3-30B-A3B-GGUF\\Qwen_Qwen3-30B-A3B-Q6_K_L.gguf\"\n```\n\n### Problem description & steps to reproduce\n\nHello all.\n\nI wanted to use the RPC feature on Windows machines and faced issue.\n\nHost 1 run RPC with command\n\n```\nPS C:\\Users\\xyz1\\Downloads\\llama-b5716-bin-win-cpu-x64> .\\rpc-server.exe -H 0.0.0.0 -p 42227\nload_backend: loaded RPC backend from C:\\Users\\xyz1\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-rpc.dll\nload_backend: loaded CPU backend from C:\\Users\\xyz1\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-cpu-haswell.dll\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nWARNING: Host ('0.0.0.0') is != '127.0.0.1'\n         Never expose the RPC server to an open network!\n         This is an experimental feature and is not secure!\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\ncreate_backend: using CPU backend\nStarting RPC server v2.0.0\n  endpoint       : 0.0.0.0:42227\n  local cache    : n/a\n  backend memory : 27366 MB\n```\n\nLocal PC run llama-cli with:\n\n`PS C:\\Users\\xyz\\Downloads\\llama-b5716-bin-win-cpu-x64> .\\llama-cli.exe --rpc 192.168.100.178:42227 -m \"E:\\LLMs\\bartowski\\Qwen_Qwen3-30B-A3B-GGUF\\Qwen_Qwen3-30B-A3B-Q6_K_L.gguf\"`\n\nResults: on the host pc all I see in the RPC command window is:\n```\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\n```\nThe model is then loaded only on the local PC, nothing is transferred / executed on the remote RPC host.\n\nEither I am doing something wrong, or there is some issue with the RPC. Both machines are on the same LAN.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n############################################ RPC.exe\nload_backend: loaded RPC backend from C:\\Users\\xyz1\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-rpc.dll\nload_backend: loaded CPU backend from C:\\Users\\xyz1\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-cpu-haswell.dll\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nWARNING: Host ('0.0.0.0') is != '127.0.0.1'\n         Never expose the RPC server to an open network!\n         This is an experimental feature and is not secure!\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\ncreate_backend: using CPU backend\nStarting RPC server v2.0.0\n  endpoint       : 0.0.0.0:42227\n  local cache    : n/a\n  backend memory : 27366 MB\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\nAccepted client connection, free_mem=28696162304, total_mem=34261987328\nClient connection closed\n\n\n################################################# LLAMA-CLI exe\n\nload_backend: loaded RPC backend from C:\\Users\\cichy\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-rpc.dll\nload_backend: loaded CPU backend from C:\\Users\\cichy\\Downloads\\llama-b5716-bin-win-cpu-x64\\ggml-cpu-icelake.dll\nbuild: 5716 (d27b3ca1) with clang version 18.1.8 for x86_64-pc-windows-msvc\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device RPC[192.168.100.178:42227] (RPC[192.168.100.178:42227]) - 27432 MiB free\nllama_model_loader: loaded meta data with 41 key-value pairs and 579 tensors from E:\\LLMs\\bartowski\\Qwen_Qwen3-30B-A3B-GGUF\\Qwen_Qwen3-30B-A3B-Q6_K_L.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-30B...\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen3 30B A3B Base\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-30B...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  12:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv  13:                    qwen3moe.context_length u32              = 32768\nllama_model_loader: - kv  14:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv  15:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  16:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  17:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  18:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  19:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  20:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  21:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  22:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  23:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  24:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  35:               general.quantization_version u32              = 2\nllama_model_loader: - kv  36:                          general.file_type u32              = 18\nllama_model_loader: - kv  37:                      quantize.imatrix.file str              = /models_out/Qwen3-30B-A3B-GGUF/Qwen_Q...\nllama_model_loader: - kv  38:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\nllama_model_loader: - kv  39:             quantize.imatrix.entries_count i32              = 384\nllama_model_loader: - kv  40:              quantize.imatrix.chunks_count i32              = 209\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q8_0:   50 tensors\nllama_model_loader: - type q6_K:  288 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q6_K\nprint_info: file size   = 23.52 GiB (6.62 BPW)\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 48\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 6144\nprint_info: n_expert         = 128\nprint_info: n_expert_used    = 8\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 30B.A3B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3 30B A3B\nprint_info: n_ff_exp         = 768\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 0 repeating layers to GPU\nload_tensors: offloaded 0/49 layers to GPU\nload_tensors:   CPU_Mapped model buffer size = 24079.77 MiB\n....................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache_unified:        CPU KV buffer size =   384.00 MiB\nllama_kv_cache_unified: size =  384.00 MiB (  4096 cells,  48 layers,  1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB\nllama_context:        CPU compute buffer size =   300.75 MiB\nllama_context: graph nodes  = 3222\nllama_context: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-20T15:09:35+00:00",
    "closed_at": "2025-06-20T15:30:29+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14307/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14307"
  },
  {
    "number": 6926,
    "title": "`llama_apply_lora_from_file_internal: bad file magic` when trying to load lora from `finetune`",
    "body": "I'm running the latest git version of llama.cpp (`bbe3c6e76157a5d806fdc155451f0ca8936248ee`).\r\n\r\nFinetuned open-llama-3b per [finetune README](https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune):\r\n\r\n```\r\ntime ./finetune \\\r\n        --model-base models/open_llama_3b_v2.Q8_0.gguf \\\r\n        --checkpoint-in  test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.gguf \\\r\n        --checkpoint-out test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-ITERATION.gguf \\\r\n        --lora-out lora-open-llama-3b-v2-q8_0-shakespeare-ITERATION.bin \\\r\n        --train-data \"shakespeare.txt\" \\\r\n        --save-every 10 \\\r\n        --threads 6 --adam-iter 30 --batch 4 --ctx 64 \\\r\n        --use-checkpointing\r\n[...]\r\ntrain_opt_callback: iter=    29 sample=117/26766 sched=0.290000 loss=3.614530 dt=00:04:27 eta=00:04:27 |----->\r\nsave_checkpoint_lora_file: saving to test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-30.gguf\r\nsave_checkpoint_lora_file: saving to test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.gguf\r\nsave_as_llama_lora: saving to lora-open-llama-3b-v2-q8_0-shakespeare-30.bin\r\nsave_as_llama_lora: saving to lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.bin\r\ntrain_opt_callback: iter=    30 sample=121/26766 sched=0.300000 loss=3.289977 dt=00:03:52 eta=0.0ms |-------->\r\nmain: total training time: 02:03:41\r\n\r\nreal\t123m42,508s\r\nuser\t632m22,489s\r\nsys\t1m9,903s\r\n```\r\n\r\nBut when I try to apply lora to test it, I get an error quoted in the subject:\r\n```\r\n$ ./main -m models/open_llama_3b_v2.Q8_0.gguf --lora test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.gguf --prompt \"To be or not to be\"\r\n[...]\r\nllama_apply_lora_from_file_internal: applying lora adapter from 'test-finetune/chk-lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.gguf' - please wait ...\r\nllama_apply_lora_from_file_internal: bad file magic\r\nllama_init_from_gpt_params: error: failed to apply lora adapter\r\nmain: error: unable to load model\r\n```\r\n\r\nEverything happens on CPU.\r\n\r\nWhat I'm doing wrong?  Thanks.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-26T12:04:05+00:00",
    "closed_at": "2024-04-26T12:07:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6926/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6926"
  },
  {
    "number": 10844,
    "title": "Build docker image llama.cpp:server-cuda: CMakeLists.txt missing",
    "body": "### Git commit\n\ndocker build\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nJetson Linux 36.4 on Orin NX 16GB.\r\n[NVidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed\r\n\r\nThe following command fails with an error:\r\n`sudo docker build -t local/llama.cpp:server-cuda -f llama-server-cuda.Dockerfile .`\r\n\r\nError: \r\n`CMake Error: The source directory \"/app\" does not appear to contain CMakeLists.txt`\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nsudo docker build -t local/llama.cpp:server-cuda -f llama-server-cuda.Dockerfile .\r\n[+] Building 112.9s (12/14)                                                                                                            docker:default\r\n => [internal] load build definition from llama-server-cuda.Dockerfile                                                                           0.0s\r\n => => transferring dockerfile: 1.57kB                                                                                                           0.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04                                                                  1.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04                                                                0.8s\r\n => [internal] load .dockerignore                                                                                                                0.0s\r\n => => transferring context: 2B                                                                                                                  0.0s\r\n => [build 1/5] FROM docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04@sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57     91.0s\r\n => => resolve docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04@sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57            0.0s\r\n => => sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57 743B / 743B                                                       0.0s\r\n => => sha256:2426f3e24139b43339d3c4fd093a5881eee4cef8cef3797ff6b43f98e3a9bf2c 2.63kB / 2.63kB                                                   0.0s\r\n => => sha256:dc794b9a764f736a5c96abe4210736b069445e70ab251694fe17126ad0252eb1 18.09kB / 18.09kB                                                 0.0s\r\n => => sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019 56.18MB / 56.18MB                                                 1.8s\r\n => => sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39 27.36MB / 27.36MB                                                 0.5s\r\n => => sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483 4.57MB / 4.57MB                                                   0.5s\r\n => => sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b 186B / 186B                                                       0.9s\r\n => => extracting sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39                                                        1.6s\r\n => => sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99 6.88kB / 6.88kB                                                   0.6s\r\n => => sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de 1.37GB / 1.37GB                                                  26.0s\r\n => => sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8 64.02kB / 64.02kB                                                 1.1s\r\n => => sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25 1.69kB / 1.69kB                                                   1.4s\r\n => => sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8 1.52kB / 1.52kB                                                   1.8s\r\n => => sha256:243b2aeb23835850b360d4d672662196987060d6216b69c7f5bf88216e27aee4 2.11GB / 2.11GB                                                  49.7s\r\n => => sha256:eab8d18930eba6ebd2ceb8d5b8bb73b304855dad51f31248b97279bf780c056f 88.86kB / 88.86kB                                                 1.9s\r\n => => extracting sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483                                                        0.3s\r\n => => extracting sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019                                                        1.5s\r\n => => extracting sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b                                                        0.0s\r\n => => extracting sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99                                                        0.0s\r\n => => extracting sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de                                                       24.9s\r\n => => extracting sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8                                                       60.7s\r\n => => extracting sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25                                                       60.7s\r\n => => extracting sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8                                                       60.6s\r\n => => extracting sha256:243b2aeb23835850b360d4d672662196987060d6216b69c7f5bf88216e27aee4                                                       39.6s\r\n => => extracting sha256:eab8d18930eba6ebd2ceb8d5b8bb73b304855dad51f31248b97279bf780c056f                                                        0.0s\r\n => [runtime 1/4] FROM docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04@sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c18  51.3s\r\n => => resolve docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04@sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c186          0.0s\r\n => => sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39 27.36MB / 27.36MB                                                 0.4s\r\n => => sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c186 743B / 743B                                                       0.0s\r\n => => sha256:238c429588bf3577f56c656fcfa5206a04553c795254b00174a13693f9b9c24a 2.21kB / 2.21kB                                                   0.0s\r\n => => sha256:1faf16d50d35362b5a1f6ddfc78ce5041b31a96b1241408a36cfcaac31f0119f 14.21kB / 14.21kB                                                 0.0s\r\n => => sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483 4.57MB / 4.57MB                                                   0.5s\r\n => => sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019 56.18MB / 56.18MB                                                 1.7s\r\n => => sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b 186B / 186B                                                       0.8s\r\n => => sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99 6.88kB / 6.88kB                                                   0.6s\r\n => => sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de 1.37GB / 1.37GB                                                  26.0s\r\n => => sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8 64.02kB / 64.02kB                                                 1.1s\r\n => => extracting sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39                                                        1.6s\r\n => => sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25 1.69kB / 1.69kB                                                   1.4s\r\n => => sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8 1.52kB / 1.52kB                                                   1.7s\r\n => => extracting sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019                                                      109.0s\r\n => => extracting sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de                                                       85.7s\r\n => => extracting sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8                                                        0.0s\r\n => => extracting sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25                                                        0.0s\r\n => => extracting sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8                                                        0.0s\r\n => [internal] load build context                                                                                                                0.0s\r\n => => transferring context: 1.57kB                                                                                                              0.0s\r\n => [runtime 2/4] RUN apt-get update &&     apt-get install -y libcurl4-openssl-dev libgomp1 curl                                               13.6s\r\n => [build 2/5] RUN apt-get update &&     apt-get install -y build-essential git cmake libcurl4-openssl-dev                                     20.2s\r\n => [build 3/5] WORKDIR /app                                                                                                                     0.0s\r\n => [build 4/5] COPY . .                                                                                                                         0.0s\r\n => ERROR [build 5/5] RUN if [ \"default\" != \"default\" ]; then         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=default\";     fi &&     cma  0.3s\r\n------\r\n > [build 5/5] RUN if [ \"default\" != \"default\" ]; then         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=default\";     fi &&     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . &&     cmake --build build --config Release --target llama-server -j$(nproc) &&     mkdir -p /app/lib &&     find build -name \"*.so\" -exec cp {} /app/lib ;:\r\n0.278 CMake Error: The source directory \"/app\" does not appear to contain CMakeLists.txt.\r\n0.278 Specify --help for usage, or press the help button on the CMake GUI.\r\n------\r\nllama-server-cuda.Dockerfile:22\r\n--------------------\r\n  21 |     # Use the default CUDA archs if not specified\r\n  22 | >>> RUN if [ \"${CUDA_DOCKER_ARCH}\" != \"default\" ]; then \\\r\n  23 | >>>         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\"; \\\r\n  24 | >>>     fi && \\\r\n  25 | >>>     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \\\r\n  26 | >>>     cmake --build build --config Release --target llama-server -j$(nproc) && \\\r\n  27 | >>>     mkdir -p /app/lib && \\\r\n  28 | >>>     find build -name \"*.so\" -exec cp {} /app/lib \\;\r\n  29 |     \r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c if [ \\\"${CUDA_DOCKER_ARCH}\\\" != \\\"default\\\" ]; then         export CMAKE_ARGS=\\\"-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\\\";     fi &&     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . &&     cmake --build build --config Release --target llama-server -j$(nproc) &&     mkdir -p /app/lib &&     find build -name \\\"*.so\\\" -exec cp {} /app/lib \\\\;\" did not complete successfully: exit code: 1\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-15T22:14:51+00:00",
    "closed_at": "2024-12-15T22:29:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10844/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10844"
  },
  {
    "number": 4186,
    "title": "llama 2 13B convert error",
    "body": "```bash\r\n(base) bzhou@Desktop:~/Repos/llama.cpp$ python convert.py models/13B\r\nLoading model file models/13B/consolidated.00.pth\r\nLoading model file models/13B/consolidated.01.pth\r\nparams = Params(n_vocab=-1, n_embd=5120, n_layer=40, n_ctx=4096, n_ff=13824, n_head=40, n_head_kv=40, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/13B'))\r\nLoading vocab file 'models/tokenizer.model', type 'spm'\r\ntok_embeddings.weight                            -> token_embd.weight                        | BF16   | [32000, 5120]\r\nnorm.weight                                      -> output_norm.weight                       | BF16   | [5120]\r\noutput.weight                                    -> output.weight                            | BF16   | [32000, 5120]\r\nlayers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | BF16   | [5120]\r\nlayers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | BF16   | [5120]\r\nlayers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | BF16   | [5120]\r\nlayers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | BF16   | [5120]\r\nlayers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | BF16   | [5120]\r\nlayers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | BF16   | [5120]\r\nlayers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | BF16   | [5120]\r\nlayers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | BF16   | [5120]\r\nlayers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | BF16   | [5120]\r\nlayers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | BF16   | [5120, 5120]\r\nlayers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | BF16   | [5120, 5120]\r\nlayers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | BF16   | [5120, 5120]\r\nlayers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | BF16   | [5120, 5120]\r\nlayers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | BF16   | [13824, 5120]\r\nlayers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | BF16   | [5120, 13824]\r\nlayers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | BF16   | [13824, 5120]\r\nlayers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | BF16   | [5120]\r\nlayers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | BF16   | [5120]\r\nlayers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | BF16   | [5120]\r\nlayers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | BF16   | [5120]\r\nlayers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | BF16   | [5120]\r\nlayers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | BF16   | [5120]\r\nlayers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | BF16   | [5120]\r\nlayers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | BF16   | [5120]\r\nlayers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | BF16   | [5120]\r\nlayers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | BF16   | [5120]\r\nlayers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | BF16   | [5120]\r\nlayers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | BF16   | [5120]\r\nlayers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | BF16   | [5120]\r\nlayers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | BF16   | [5120]\r\nlayers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | BF16   | [5120]\r\nlayers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | BF16   | [5120]\r\nlayers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | BF16   | [5120]\r\nlayers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | BF16   | [5120]\r\nlayers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | BF16   | [5120]\r\nlayers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | BF16   | [5120]\r\nlayers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | BF16   | [5120]\r\nlayers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | BF16   | [5120]\r\nlayers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | BF16   | [5120]\r\nlayers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | BF16   | [5120]\r\nlayers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | BF16   | [5120]\r\nlayers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | BF16   | [5120]\r\nlayers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | BF16   | [5120]\r\nlayers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | BF16   | [5120]\r\nlayers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | BF16   | [5120]\r\nlayers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | BF16   | [5120]\r\nlayers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | BF16   | [5120]\r\nlayers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | BF16   | [5120]\r\nlayers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | BF16   | [5120, 5120]\r\nlayers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | BF16   | [5120, 5120]\r\nlayers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | BF16   | [5120, 5120]\r\nlayers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | BF16   | [5120, 5120]\r\nlayers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | BF16   | [13824, 5120]\r\nlayers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | BF16   | [5120, 13824]\r\nlayers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | BF16   | [13824, 5120]\r\nlayers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | BF16   | [5120]\r\nlayers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | BF16   | [5120]\r\nskipping tensor rope_freqs\r\nWriting models/13B/ggml-model-f16.gguf, format 1\r\nTraceback (most recent call last):\r\n  File \"/home/bzhou/Repos/llama.cpp/convert.py\", line 1228, in <module>\r\n    main()\r\n  File \"/home/bzhou/Repos/llama.cpp/convert.py\", line 1223, in main\r\n    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\r\n  File \"/home/bzhou/Repos/llama.cpp/convert.py\", line 924, in write_all\r\n    check_vocab_size(params, vocab)\r\n  File \"/home/bzhou/Repos/llama.cpp/convert.py\", line 811, in check_vocab_size\r\n    raise Exception(msg)\r\nException: Vocab size mismatch (model has -1, but models/tokenizer.model has 32000).\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-23T15:09:34+00:00",
    "closed_at": "2023-11-23T15:14:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4186/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4186"
  },
  {
    "number": 6643,
    "title": "Some warnings like \"...reallocate multi buffer graph...\" when compiled by Qt",
    "body": "System :\r\nUBUNTU\r\n\r\nGPU\uff1a\r\nRTX 2080ti + Tesla P40\r\n\r\nSoftware env: \r\nQt creator 12.0.2 with Qt 6.6.2\r\n\r\nProblem:\r\n\r\nAfter I download this project, I followed the instruction to complied it with option  -DLLAMA_CUBLAS=ON , using ubuntu system terminal, everything was fine.\r\nI tried the model of Qwen1.5 7B , 14B and 72B , they all works fine on single GPU or 2 GPUs.\r\n\r\nThen I open this project with Qt creator because I want to write some GUI for it . I first compiled the whole project in Qt environment for test. It also looks fine and compile finished seems no warnings.\r\nBut when I run those models, there were below warnings pop up in the terminal from time to time , when the model generating answer.\r\n```\r\nggml_gallocr_needs_realloc: graph has different number of nodes\r\nggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve\r\nggml_backend_sched_alloc_splits: failed to allocate graph, reserving\r\n```\r\n\r\nThere won't be these warnings if I compiled the project in ubuntu terminal, only happens when compiled in Qt.\r\n\r\nPlease advise if something I did wrong, thanks.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-12T18:03:21+00:00",
    "closed_at": "2024-04-12T18:20:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6643/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6643"
  },
  {
    "number": 10678,
    "title": "Compile bug: npu-smi not found / Auto-detach ascend soc type failed",
    "body": "### Git commit\n\ngit rev-parse HEAD\r\nc9c6e01daedac542b174c235872569fce5385982\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nBLAS\n\n### Problem description & steps to reproduce\n\nBuilding clean at master, I get the logs pasted below. \r\n\r\nI don't think the 'npu-smi' util would be expected on a mac, but if it is, I'm not sure how to install it...\r\n\r\n... also not sure that's the actual problem. \n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n-- CMAKE_SYSTEM_PROCESSOR: arm64\r\n-- Including CPU backend\r\n-- Accelerate framework found\r\n-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\r\n-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)\r\n-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)\r\nCMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:49 (message):\r\n  OpenMP not found\r\nCall Stack (most recent call first):\r\n  ggml/src/CMakeLists.txt:305 (ggml_add_cpu_backend_variant_impl)\r\n\r\n\r\n-- ARM detected\r\n-- ARM feature DOTPROD enabled\r\n-- ARM feature MATMUL_INT8 enabled\r\n-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8\r\n-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks/Accelerate.framework\r\n-- BLAS found, Includes:\r\n-- Including BLAS backend\r\nbash: npu-smi: command not found\r\nCMake Error at ggml/src/ggml-cann/CMakeLists.txt:16 (message):\r\n  Auto-detech ascend soc type failed, please specify manually or check ascend\r\n  device working normally.\r\nCall Stack (most recent call first):\r\n  ggml/src/ggml-cann/CMakeLists.txt:22 (detect_ascend_soc_type)\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-05T20:02:01+00:00",
    "closed_at": "2024-12-05T20:10:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10678/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10678"
  },
  {
    "number": 6426,
    "title": "Cannot offload To GPU M1 ",
    "body": "ggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1\r\nggml_metal_init: picking default device: Apple M1\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = /Users/ibrahim/PycharmProjects/IbrahimAIChat/llama.cpp/\r\nggml_metal_init: loading '/Users/ibrahim/PycharmProjects/IbrahimAIChat/llama.cpp/ggml-metal.metal'\r\nggml_metal_init: error: Error Domain=MTLLibraryErrorDomain Code=3 \"program_source:3:10: fatal error: 'ggml-common.h' file not found\r\n#include \"ggml-common.h\"\r\n         ^~~~~~~~~~~~~~~\r\n\" UserInfo={NSLocalizedDescription=program_source:3:10: fatal error: 'ggml-common.h' file not found\r\n#include \"ggml-common.h\"\r\n         ^~~~~~~~~~~~~~~\r\n}\r\nllama_new_context_with_model: failed to initialize Metal backend\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/__main__.py\", line 88, in <module>\r\n    main()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/__main__.py\", line 74, in main\r\n    app = create_app(\r\n          ^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/app.py\", line 138, in create_app\r\n    set_llama_proxy(model_settings=model_settings)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/app.py\", line 75, in set_llama_proxy\r\n    _llama_proxy = LlamaProxy(models=model_settings)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/model.py\", line 31, in __init__\r\n    self._current_model = self.load_llama_from_model_settings(\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/server/model.py\", line 138, in load_llama_from_model_settings\r\n    _model = create_fn(\r\n             ^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/llama.py\", line 328, in __init__\r\n    self._ctx = _LlamaContext(\r\n                ^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_cpp/_internals.py\", line 265, in __init__\r\n    raise ValueError(\"Failed to create llama_context\")\r\nValueError: Failed to create llama_context\r\nwarning: failed to munlock buffer: Cannot allocate memory\r\n\r\nafter:\r\n\r\n\r\npython -m llama_cpp.server --model models/mistral-7b-instruct-v0.1.Q4_0.gguf --n_gpu -1\r\n\r\n<img width=\"350\" alt=\"Screenshot 2024-04-01 at 19 28 40\" src=\"https://github.com/ggerganov/llama.cpp/assets/75740933/62497ae1-94ae-4e2f-9407-fdb4ea9e838f\">\r\n\r\n\r\nI do have ggml-common.h\r\n\r\n\r\nso can anyone help me?\r\n\r\n\r\nI made an environment variable try point to  ggml-common.h \r\n\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = /Users/ibrahim/PycharmProjects/IbrahimAIChat/llama.cpp/\r\n\r\n\r\nstill get the same error ",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-01T18:29:07+00:00",
    "closed_at": "2024-04-01T18:31:03+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6426/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6426"
  },
  {
    "number": 3378,
    "title": "Regarding loading models",
    "body": "In the model path, we provide the path of one of the gguf files. Can't we load the whole model?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-28T13:03:28+00:00",
    "closed_at": "2023-09-28T14:02:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3378"
  },
  {
    "number": 223,
    "title": "[QUESTION] data type",
    "body": "I see that it says using float16 float32 mixed precision, but as we are talking about characters, shouldn't it uses char8 ?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-17T03:51:39+00:00",
    "closed_at": "2023-03-17T04:02:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/223/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/223"
  },
  {
    "number": 357,
    "title": "Interactive mode in Python?",
    "body": "Hello, I have a question. How can i use LLaMa in an interactive mode (i.e. as a chat) in Python, and is it possible at all? So that he would not just generate text, but it would be possible to somehow communicate",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-21T15:40:27+00:00",
    "closed_at": "2023-03-21T16:10:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/357/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/357"
  },
  {
    "number": 4837,
    "title": "Loading GGUF models on iOS with swift package gives assertion error",
    "body": "`ggml.c:4772` -> `b->type == GGML_TYPE_I32`\r\n\r\nUsing TinyLlama-v1.0-Q5. The same model works on android and PC with no issues.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-09T13:56:16+00:00",
    "closed_at": "2024-01-09T14:23:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4837/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4837"
  },
  {
    "number": 620,
    "title": "[build] ARMv8 build problem (OpenWrt)",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n  * `git clone $url; cd llama.cpp; make` \r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expected to build the basic llama.cpp `bin/main` program, to see if building even worked properly.\r\n\r\n# Current Behavior\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# make\r\nI llama.cpp build info:\r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  aarch64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mcpu=native\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -mcpu=native\r\nI LDFLAGS:\r\nI CC:       cc (OpenWrt GCC 11.2.0) 11.2.0\r\nI CXX:      g++ (OpenWrt GCC 11.2.0) 11.2.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mcpu=native   -c ggml.c -o ggml.o\r\nggml.c: In function 'dequantize_row_q4_1':\r\nggml.c:1041:13: note: use '-flax-vector-conversions' to permit conversions between vectors with differing element types or numbers of subparts\r\n 1041 |             const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n      |             ^~~~~\r\nggml.c:1041:46: error: incompatible type for argument 1 of 'vmovl_s8'\r\n 1041 |             const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n      |                                              ^~~~~~~~~~~~~~~~\r\n      |                                              |\r\n      |                                              uint8x8_t\r\nIn file included from ggml.c:164:\r\n/usr/lib/gcc/aarch64-openwrt-linux-musl/11.2.0/include/arm_neon.h:7989:20: note: expected 'int8x8_t' but argument is of type 'uint8x8_t'\r\n 7989 | vmovl_s8 (int8x8_t __a)\r\n      |           ~~~~~~~~~^~~\r\nggml.c:1042:46: error: incompatible type for argument 1 of 'vmovl_s8'\r\n 1042 |             const uint16x8_t vi_1 = vmovl_s8(vget_high_u8(vq));\r\n      |                                              ^~~~~~~~~~~~~~~~\r\n      |                                              |\r\n      |                                              uint8x8_t\r\nIn file included from ggml.c:164:\r\n/usr/lib/gcc/aarch64-openwrt-linux-musl/11.2.0/include/arm_neon.h:7989:20: note: expected 'int8x8_t' but argument is of type 'uint8x8_t'\r\n 7989 | vmovl_s8 (int8x8_t __a)\r\n      |           ~~~~~~~~~^~~\r\nmake: *** [Makefile:226: ggml.o] Error 1\r\n```\r\n\r\n# Environment and Context \r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n# lscpu\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       32-bit, 64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 8\r\n  On-line CPU(s) list:  0-7\r\nVendor ID:              ARM\r\n  Model name:           Cortex-A55\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           r2p0\r\n    CPU(s) scaling MHz: 56%\r\n    CPU max MHz:        1800.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\n  Model name:           Cortex-A76\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 2\r\n    Socket(s):          2\r\n    Stepping:           r4p0\r\n    CPU(s) scaling MHz: 22%\r\n    CPU max MHz:        2352.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nCaches (sum of all):\r\n  L1d:                  384 KiB (8 instances)\r\n  L1i:                  384 KiB (8 instances)\r\n  L2:                   2.5 MiB (8 instances)\r\n  L3:                   3 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:        Not affected\r\n  L1tf:                 Not affected\r\n  Mds:                  Not affected\r\n  Meltdown:             Not affected\r\n  Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:           Mitigation; __user pointer sanitization\r\n  Spectre v2:           Vulnerable: Unprivileged eBPF enabled\r\n  Srbds:                Not affected\r\n  Tsx async abort:      Not affected\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# uname -a\r\nLinux FriendlyWrt 5.10.110 #1 SMP Sat Dec 3 01:25:15 CST 2022 aarch64 GNU/Linux\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# cat /etc/os-release\r\nNAME=\"OpenWrt\"\r\nVERSION=\"22.03.2\"\r\nID=\"openwrt\"\r\nID_LIKE=\"lede openwrt\"\r\nPRETTY_NAME=\"OpenWrt 22.03.2\"\r\nVERSION_ID=\"22.03.2\"\r\nHOME_URL=\"https://openwrt.org/\"\r\nBUG_URL=\"https://bugs.openwrt.org/\"\r\nSUPPORT_URL=\"https://forum.openwrt.org/\"\r\nBUILD_ID=\"r19803-9a599fee93\"\r\nOPENWRT_BOARD=\"rockchip/armv8\"\r\nOPENWRT_ARCH=\"aarch64_generic\"\r\nOPENWRT_TAINTS=\"busybox\"\r\nOPENWRT_DEVICE_MANUFACTURER=\"OpenWrt\"\r\nOPENWRT_DEVICE_MANUFACTURER_URL=\"https://openwrt.org/\"\r\nOPENWRT_DEVICE_PRODUCT=\"Generic\"\r\nOPENWRT_DEVICE_REVISION=\"v0\"\r\nOPENWRT_RELEASE=\"OpenWrt 22.03.2 r19803-9a599fee93\"\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# python --version\r\nPython 3.11.2\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# make --version\r\nGNU Make 4.3\r\nBuilt for aarch64-openwrt-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# gcc --version\r\ngcc (OpenWrt GCC 11.2.0) 11.2.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nQuite simple:\r\n\r\n```\r\ngit clone https://github.com/ggerganov/llama.cpp.git\r\ncd llama.cpp\r\nmake\r\n```\r\n\r\nReading the Makefile, I noticed that a lot of configuration was done automatically, so I assumed I could just go and `make` it.\r\n\r\n# Failure Logs\r\n\r\nSee above. :)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-30T09:24:25+00:00",
    "closed_at": "2023-03-30T10:05:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/620/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/620"
  },
  {
    "number": 589,
    "title": ".dot file of ggml_graph can not be generated to .png file",
    "body": "Hi, I want to generate a picture of the grapj. And I uncommented this 2 lines in \"llama.cpp\", so that to run the function `ggml_graph_dump_dot\uff08\uff09`\r\n```\r\n    //if (n_past%100 == 0) {\r\n        ggml_graph_print   (&gf);\r\n        ggml_graph_dump_dot(&gf, NULL, \"gpt-2.dot\");\r\n    //}\r\n```\r\nAnd I got a file named `gpt-2.dot`\r\nBut when I run command in python:\r\n```\r\nfrom graphviz import Digraph\r\nimport sys\r\nsys.setrecursionlimit(300000) \r\n\r\nimport pydot\r\nimport os\r\n(graph,) = pydot.graph_from_dot_file(\"D:\\\\PIQ\\\\llama.cpp\\\\build\\\\examples\\\\main\\\\gpt-2.dot\")\r\ngraph.write_png(\"gpt-2.png\")\r\n```\r\nI get the error message: `Expect '{' but got '['`\r\nSo I modifid the function `ggml_graph_dump_dot\uff08\uff09` in `ggml.c` like this:\r\n```\r\nvoid ggml_graph_dump_dot(const struct ggml_cgraph * gb, const struct ggml_cgraph * gf, const char * filename) {\r\n    char color[16];\r\n\r\n    FILE * fp = fopen(filename, \"w\");\r\n    GGML_ASSERT(fp);\r\n\r\n    fprintf(fp, \"digraph G {\\n\");\r\n    fprintf(fp, \"  newrank = true;\\n\");\r\n    fprintf(fp, \"  rankdir = LR;\\n\");\r\n\r\n    for (int i = 0; i < gb->n_nodes; i++) {\r\n        struct ggml_tensor * node = gb->nodes[i];\r\n\r\n        if (ggml_graph_get_parent(gb, node) != NULL) {\r\n            continue;\r\n        }\r\n\r\n        if (node->is_param) {\r\n            snprintf(color, sizeof(color), \"yellow\");\r\n        } else if (node->grad) {\r\n            if (ggml_graph_find(gf, node)) {\r\n                snprintf(color, sizeof(color), \"green\");\r\n            } else {\r\n                snprintf(color, sizeof(color), \"lightblue\");\r\n            }\r\n        } else {\r\n            snprintf(color, sizeof(color), \"white\");\r\n        }\r\n\r\n        fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"%d [%d, %d] | <x>%s\",\r\n                (void *) node, color,\r\n                i, node->ne[0], node->ne[1],\r\n                GGML_OP_SYMBOL[node->op]);\r\n\r\n        if (node->grad) {\r\n            fprintf(fp, \" | <g>%s\\\"; }\\n\", GGML_OP_SYMBOL[node->grad->op]);\r\n        } else {\r\n            fprintf(fp, \"\\\"; }\\n\");\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_leafs; i++) {\r\n        struct ggml_tensor * node = gb->leafs[i];\r\n\r\n        snprintf(color, sizeof(color), \"pink\");\r\n\r\n        if (ggml_nelements(node) == 1) {\r\n            fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"<x>%.1e\\\"; }\\n\",\r\n                    (void *) node, color, ggml_get_f32_1d(node, 0));\r\n        } else {\r\n            fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"<x>CONST %d [%d, %d]\\\"; }\\n\",\r\n                    (void *) node, color,\r\n                    i, node->ne[0], node->ne[1]);\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_nodes; i++) {\r\n        struct ggml_tensor * node = gb->nodes[i];\r\n\r\n        struct ggml_tensor * parent = ggml_graph_get_parent(gb, node);\r\n\r\n        if (node->src0) {\r\n            struct ggml_tensor * parent0 = ggml_graph_get_parent(gb, node->src0);\r\n\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s { arrowhead = %s; style = %s; label = \\\"x\\\"; }\\n\",\r\n                    parent0 ? (void *) parent0 : (void *) node->src0,\r\n                    parent0 ? \"g\" : \"x\",\r\n                    parent ? (void *) parent : (void *) node,\r\n                    parent ? \"g\" : \"x\",\r\n                    parent ? \"empty\" : \"vee\",\r\n                    parent ? \"dashed\" : \"solid\");\r\n        }\r\n\r\n        if (node->src1) {\r\n            struct ggml_tensor * parent1 = ggml_graph_get_parent(gb, node->src1);\r\n\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s { arrowhead = %s; style = %s; label = \\\"y\\\"; }\\n\",\r\n                    parent1 ? (void *) parent1 : (void *) node->src1,\r\n                    parent1 ? \"g\" : \"x\",\r\n                    parent ? (void *) parent : (void *) node,\r\n                    parent ? \"g\" : \"x\",\r\n                    parent ? \"empty\" : \"vee\",\r\n                    parent ? \"dashed\" : \"solid\");\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_leafs; i++) {\r\n        struct ggml_tensor * node = gb->leafs[i];\r\n\r\n        if (node->src0) {\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s [ label = \\\"x\\\"; ]\\n\",\r\n                    (void *) node->src0, \"x\",\r\n                    (void *) node, \"x\");\r\n        }\r\n\r\n        if (node->src1) {\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s [ label = \\\"y\\\"; ]\\n\",\r\n                    (void *) node->src1, \"x\",\r\n                    (void *) node, \"x\");\r\n        }\r\n    }\r\n\r\n    fprintf(fp, \"}\\n\");\r\n\r\n    fclose(fp);\r\n\r\n    GGML_PRINT(\"%s: dot -Tpng %s -o %s.png && open %s.png\\n\", __func__, filename, filename, filename);\r\n}\r\n```\r\nTo replace the '[' and `]` to '{' and '}'\r\nThen it doesn't have new error , but the process blocked, can not stop.Blocked in here:\r\n![image](https://user-images.githubusercontent.com/58033505/228439062-708c8eb3-40ea-4f2c-bfe6-4a41d87ce689.png)\r\n\r\nAnd I also run command in windows powershell, but it still blocked like this:\r\n![image](https://user-images.githubusercontent.com/58033505/228439280-999e7a9b-1c34-4a08-9f43-ecb2171064ec.png)\r\n\r\nsame in wsl:\r\n\r\n![image](https://user-images.githubusercontent.com/58033505/228439326-093c1ac5-bd8d-40fd-9afc-eadd7b8b41ee.png)\r\n\r\nSo can anybody know what can I do? Thanks\r\n\r\nAttach is the .dot file generated by me, you can modify the suffix from `gpt-2.txt` to `gpt-2.dot`. The model size is 7B-version.\r\n\r\n[gpt-2.txt](https://github.com/ggerganov/llama.cpp/files/11097020/gpt-2.txt)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-29T05:57:35+00:00",
    "closed_at": "2023-03-29T06:38:43+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/589/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/589"
  },
  {
    "number": 13070,
    "title": "Feature Request:",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\n..\n\n### Motivation\n\n..\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-04-22T15:40:43+00:00",
    "closed_at": "2025-04-22T15:52:37+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13070/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13070"
  },
  {
    "number": 5125,
    "title": "Finetuning Architecture Issue",
    "body": "Hey, I am trying to finetune Zephyr-Quiklang-3b using llama.cpp finetuning feature.\r\n\r\nThe problem is, the material found online would suggest it can fine-tune practically any GGUF format model.\r\nAlthough that has not been my experience this far.\r\n\r\nI get the following output:\r\n`load_model_hparams_gguf: arch=stablelm expected_arch=llama\r\nGGML_ASSERT: examples/finetune/finetune.cpp:243: arch == expected_arch`\r\n\r\nFrom the sounds of it, there is only a single supported model architecture which is llama.\r\n\r\nI will dig a bit deeper into the source code to see if this assumption is wrong and give feedback here.\r\n\r\nIf anyone has seen this issue before please let me know of any work arounds you may have found.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-25T17:11:43+00:00",
    "closed_at": "2024-01-25T17:16:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5125/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5125"
  },
  {
    "number": 253,
    "title": "How to use it in Python",
    "body": "How to use this in my python code?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T04:46:55+00:00",
    "closed_at": "2023-03-18T04:58:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/253/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/253"
  },
  {
    "number": 870,
    "title": "Where is tokenizer.model?",
    "body": "Hello, sorry if this is a simple question but I am trying to convert the GPT4All model with the code giving in the description. \r\n\r\n`python3 convert-gpt4all-to-ggml.py models/gpt4all-7B/gpt4all-lora-quantized.bin ./models/tokenizer.model `\r\n\r\nbut there is no such tokenizer.model file in the repo, no hint on where to get it and even googling comes up with nothing. Where are you supposed to get this file? thanks",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-10T08:16:03+00:00",
    "closed_at": "2023-04-10T08:51:09+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/870/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/870"
  },
  {
    "number": 8503,
    "title": "Bug: Qwen-2-7b-q8 and Qwen-2-7b-instruct-q8 giving weird output when run with CUDA support",
    "body": "### What happened?\n\nInference of llama.cpp using Qwen-2-7b-q8 and Qwen-2-7b-intruct-q8 showing weird output while running on CUDA whereas same thing works fine when switching all layers to CPU. The output with CPU is something meaningful but for GPU it is only printing \"GGGG.....\"\n\n### Name and Version\n\n$./main --version version: 2874 (e0f55618) built with cc (Ubuntu 22.04.3 LTS) for aarch64(ARM Machine)\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n**When all the layers sit on CPU, it generates something meaningful:**\r\n./llama.cpp/build_cuda_normal/bin/main -m ./models/qwen/Qwen2-7B-Q8_0.gguf -p \"Gen AI has application in\" -n 100 -b 32\r\nLog start\r\nmain: build = 2874 (e0f55618)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\nmain: seed  = 1721116246\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 339 tensors from ./models/qwen/Qwen2-7B-Q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = models\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 131072\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q8_0:  198 tensors\r\nllm_load_vocab: special tokens definition check successful ( 421/152064 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 7.54 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = models\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.16 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/29 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  7717.68 MiB\r\n........................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 32\r\nllama_new_context_with_model: n_ubatch   = 32\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =    28.00 MiB\r\nllama_new_context_with_model: KV self size  =   28.00 MiB, K (f16):   14.00 MiB, V (f16):   14.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   571.23 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     0.50 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 396\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nsampling: \r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate: n_ctx = 512, n_batch = 32, n_predict = 100, n_keep = 0\r\n\r\n\r\n<|endoftext|>Gen AI has application in the medical field and is a key driver in the AI in healthcare industry.\r\nThe global AI in healthcare market is expected to grow at a CAGR of 47.9% during the forecast period, from 2022 to 2030. The key factors driving this growth are the increasing prevalence of chronic diseases, the rising demand for personalized medicine, and the need for efficient healthcare delivery systems. Additionally, the adoption of AI in healthcare is also being driven by the increasing availability\r\nllama_print_timings:        load time =    1326.93 ms\r\nllama_print_timings:      sample time =      12.67 ms /   100 runs   (    0.13 ms per token,  7892.66 tokens per second)\r\nllama_print_timings: prompt eval time =     218.92 ms /     6 tokens (   36.49 ms per token,    27.41 tokens per second)\r\nllama_print_timings:        eval time =   12636.99 ms /    99 runs   (  127.65 ms per token,     7.83 tokens per second)\r\nllama_print_timings:       total time =   12971.45 ms /   105 tokens\r\n\r\n\r\n**When we offload all the layers to GPU, then it generates gibberish:**\r\n./llama.cpp/build_cuda_normal/bin/main -m ./models/qwen/Qwen2-7B-Q8_0.gguf -p \"Gen AI has application in\" -n 100 -b 32 -ngl 29\r\nLog start\r\nmain: build = 2874 (e0f55618)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\nmain: seed  = 1721116338\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 339 tensors from ./models/qwen/Qwen2-7B-Q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = models\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 131072\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q8_0:  198 tensors\r\nllm_load_vocab: special tokens definition check successful ( 421/152064 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 7.54 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = models\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.32 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:        CPU buffer size =   552.23 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  7165.44 MiB\r\n........................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 32\r\nllama_new_context_with_model: n_ubatch   = 32\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =    28.00 MiB\r\nllama_new_context_with_model: KV self size  =   28.00 MiB, K (f16):   14.00 MiB, V (f16):   14.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    19.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     0.50 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nsampling: \r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate: n_ctx = 512, n_batch = 32, n_predict = 100, n_keep = 0\r\n\r\n\r\n<|endoftext|>Gen AI has application inGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\r\nllama_print_timings:        load time =   10680.24 ms\r\nllama_print_timings:      sample time =      10.51 ms /   100 runs   (    0.11 ms per token,  9513.84 tokens per second)\r\nllama_print_timings: prompt eval time =      13.16 ms /     6 tokens (    2.19 ms per token,   456.00 tokens per second)\r\nllama_print_timings:        eval time =     878.61 ms /    99 runs   (    8.87 ms per token,   112.68 tokens per second)\r\nllama_print_timings:       total time =     998.16 ms /   105 tokens\r\nLog end\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-16T07:59:42+00:00",
    "closed_at": "2024-07-16T08:01:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8503/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8503"
  },
  {
    "number": 1361,
    "title": "Ai on Laptop finetuning",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-07T23:42:06+00:00",
    "closed_at": "2023-05-07T23:42:22+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1361/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1361"
  },
  {
    "number": 6798,
    "title": "MoE loading time regression",
    "body": "Three weeks ago #6387 removed mmap() support for MoE models. This causes Mixtral 8x7b F16 to take 30x longer to load on my Threadripper w/ 5200 MT/s RAM. It used to take 2 seconds to load. Now it takes 56 seconds to load.\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/49262/4230aa47-f00e-480a-8440-7c5b51ea8179)\r\n\r\nCan we reconsider this? I would rather have 3d tensor creation be a 1-time cost in the conversion script, rather than happening each time the llama.cpp process spawns.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-20T21:21:58+00:00",
    "closed_at": "2024-04-20T21:57:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6798/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6798"
  }
]
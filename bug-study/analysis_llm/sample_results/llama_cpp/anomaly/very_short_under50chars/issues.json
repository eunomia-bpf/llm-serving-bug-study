[
  {
    "number": 8,
    "title": "Is there a requirements.txt ?",
    "body": null,
    "labels": [
      "question",
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-11T05:53:26+00:00",
    "closed_at": "2023-03-12T06:23:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8"
  },
  {
    "number": 49,
    "title": "Windows MSVC support",
    "body": "hello, would it add MSVC build support as well?",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-12T13:43:57+00:00",
    "closed_at": "2023-03-13T17:25:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/49/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/49"
  },
  {
    "number": 14722,
    "title": "Data offline",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-16T11:51:11+00:00",
    "closed_at": "2025-07-16T11:51:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14722/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14722"
  },
  {
    "number": 1317,
    "title": "Try Modular - Mojo",
    "body": "https://www.modular.com/",
    "labels": [
      "invalid"
    ],
    "state": "closed",
    "created_at": "2023-05-04T12:32:34+00:00",
    "closed_at": "2023-05-04T18:49:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1317/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1317"
  },
  {
    "number": 11901,
    "title": "\u041b\u0430\u043c\u0430",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-16T03:44:29+00:00",
    "closed_at": "2025-02-16T05:08:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11901/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11901"
  },
  {
    "number": 1663,
    "title": "it\u2019s so long a lime to wait while using a server",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-01T08:38:02+00:00",
    "closed_at": "2024-04-10T01:07:55+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1663/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1663"
  },
  {
    "number": 6416,
    "title": "\u8bf7\u95ee\uff0cllama.cpp\u53ef\u4ee5\u91cf\u5316\u5fae\u8c03\u540e\u7684qwen\u6a21\u578b\u5417\uff1f",
    "body": "\u8bf7\u95ee\uff0cllama.cpp\u53ef\u4ee5\u91cf\u5316\u5fae\u8c03\u540e\u7684qwen\u6a21\u578b\u5417\uff1f",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T02:59:41+00:00",
    "closed_at": "2024-05-16T01:06:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6416/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6416"
  },
  {
    "number": 7945,
    "title": "nvm",
    "body": "nvm",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-15T00:09:20+00:00",
    "closed_at": "2024-06-17T18:20:35+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7945"
  },
  {
    "number": 3736,
    "title": "when support Qwen-7b",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-23T05:09:23+00:00",
    "closed_at": "2024-04-04T01:07:25+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3736"
  },
  {
    "number": 13,
    "title": "[Q] Memory Requirements for Different Model Sizes",
    "body": null,
    "labels": [
      "documentation",
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-11T12:19:07+00:00",
    "closed_at": "2023-03-18T21:02:00+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13"
  },
  {
    "number": 88,
    "title": "Create json api service",
    "body": "so we can intergrate app/UI.",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T10:19:23+00:00",
    "closed_at": "2023-07-28T19:29:40+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/88/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/88"
  },
  {
    "number": 9584,
    "title": "Add theme Rose Pine",
    "body": "https://rosepinetheme.com/",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-21T21:37:37+00:00",
    "closed_at": "2024-11-07T01:07:19+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9584/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9584"
  },
  {
    "number": 1446,
    "title": "....!",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-14T06:54:24+00:00",
    "closed_at": "2023-05-14T07:13:56+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1446/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 1,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1446"
  },
  {
    "number": 7549,
    "title": "Figure out how to fork",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-05-26T22:38:55+00:00",
    "closed_at": "2024-05-26T22:39:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7549"
  },
  {
    "number": 2129,
    "title": "I like this project.",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-07T09:35:47+00:00",
    "closed_at": "2023-07-07T10:37:24+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2129/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2129"
  },
  {
    "number": 253,
    "title": "How to use it in Python",
    "body": "How to use this in my python code?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T04:46:55+00:00",
    "closed_at": "2023-03-18T04:58:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/253/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/253"
  },
  {
    "number": 13558,
    "title": "Great work ! !",
    "body": "Thanks for your contribution.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-15T07:13:32+00:00",
    "closed_at": "2025-05-15T13:10:55+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13558/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13558"
  },
  {
    "number": 6400,
    "title": "Kompute-based Vulkan backend shows an GGML_OP_GET_ROWS error",
    "body": "Doesn't happen with the other Vulkan backend.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-03-30T21:56:07+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6400"
  },
  {
    "number": 259,
    "title": "Is it possible to run the llama on an AMD graphics card?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T08:43:00+00:00",
    "closed_at": "2023-03-18T11:16:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/259"
  },
  {
    "number": 9643,
    "title": "Llama-3.2 11B Vision Support",
    "body": "Is it working right now in any way?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-25T20:00:17+00:00",
    "closed_at": "2025-03-16T22:51:45+00:00",
    "comments": 47,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9643/reactions",
      "total_count": 81,
      "+1": 54,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 12,
      "rocket": 0,
      "eyes": 15
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9643"
  },
  {
    "number": 11857,
    "title": "delete",
    "body": null,
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-02-14T05:15:06+00:00",
    "closed_at": "2025-02-24T18:19:31+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11857"
  },
  {
    "number": 2412,
    "title": "SayHello",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-27T01:18:40+00:00",
    "closed_at": "2023-07-28T20:00:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2412/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2412"
  },
  {
    "number": 12360,
    "title": "Do you add LLaDA model support?",
    "body": "https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-13T04:31:35+00:00",
    "closed_at": "2025-05-02T01:07:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12360/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12360"
  },
  {
    "number": 414,
    "title": "how to fine tuning model with with dataset (file json/csv..)",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-23T04:29:28+00:00",
    "closed_at": "2023-03-23T08:55:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/414"
  },
  {
    "number": 6077,
    "title": "Is it possible to convert only SentencePiece tokenizer without the model to GGUF?",
    "body": "If it is, how?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-15T10:49:01+00:00",
    "closed_at": "2024-05-07T01:06:36+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6077/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6077"
  },
  {
    "number": 160,
    "title": "Add avx-512 support?",
    "body": "No clue but I think it may work faster",
    "labels": [
      "enhancement",
      "performance",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T12:10:17+00:00",
    "closed_at": "2023-03-28T09:54:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/160/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/160"
  },
  {
    "number": 1418,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": " is it possible to run on linux???",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-12T16:18:43+00:00",
    "closed_at": "2023-05-12T17:44:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1418/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1418"
  },
  {
    "number": 111,
    "title": "Make a tag/release",
    "body": "Thanks.",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-14T02:04:37+00:00",
    "closed_at": "2023-03-14T19:16:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/111/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 2,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/111"
  },
  {
    "number": 2947,
    "title": "[User] supporting code llama",
    "body": "is it possible to use with code llama ?\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-01T01:34:44+00:00",
    "closed_at": "2024-04-05T01:06:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2947/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2947"
  },
  {
    "number": 2975,
    "title": "[C0ffymachyne] was -gqa argument removed from the \"main\"  ? What is a replacement and how to load 70B model without it ?",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-02T21:07:07+00:00",
    "closed_at": "2024-04-05T01:06:26+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2975/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2975"
  }
]
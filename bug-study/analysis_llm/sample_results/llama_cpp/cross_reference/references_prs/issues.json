[
  {
    "number": 875,
    "title": "[Feature] Supporting arbitrary map operations in GGML",
    "body": "Proof of concept patch in #874 which adds `MAP_UNARY` and `MAP_BINARY` operations that can take a function pointer which is applied similarly to `ggml_vec_add_f32` and friends.\r\n\r\nIs this something that could potentially get included? It would enable projects or `llama.cpp` to support a larger range of models pretty easily because at least the simple version of an operation that GGML doesn't support could be used without having to make modifications to GGML itself.\r\n\r\nOne example is RWKV which requires several operations that GGML doesn't support.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-10T13:19:16+00:00",
    "closed_at": "2023-04-14T15:15:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/875/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/875"
  },
  {
    "number": 5748,
    "title": "Build fail - implicit declaration of function \u2018vld1q_u8_x2\u2019 on Armv7-linux with the latest release",
    "body": "We wanted to [build binaries for ARMv6,7 on Linux](https://github.com/JuliaPackaging/Yggdrasil/pull/8165) but we're getting the following two types of errors:\r\n\r\n> [09:00:29] /workspace/srcdir/llama.cpp/ggml-quants.c: In function \u2018ggml_vec_dot_iq2_s_q8_K\u2019:\r\n> [09:00:29] /workspace/srcdir/llama.cpp/ggml-quants.c:9645:32: error: implicit declaration of function **\u2018vld1q_u8_x2\u2019**; did you mean \u2018vld1q_u32\u2019? [-Werror=implicit-function-declaration]\r\n> [09:00:29]  9645 |     const uint8x16x2_t mask1 = vld1q_u8_x2(k_mask1);\r\n\r\nSimilar:\r\n > /workspace/srcdir/llama.cpp/ggml-quants.c:9678:34: error: implicit declaration of function \u2018**vqtbl1q_u8**\u2019; did you mean \u2018vtbl1_u8\u2019? [-Werror=implicit-function-declaration]\r\n> [09:09:12]  9678 |             vs.val[1] = vandq_u8(vqtbl1q_u8(vs.val[0], mask1.val[1]), mask2);\r\n> [09:09:12]       |                                  ^~~~~~~~~~\r\n> [09:09:12]       |                                  vtbl1_u8\r\n> [09:09:12] /workspace/srcdir/llama.cpp/ggml-quants.c:9678:34: error: incompatible type for argument 1 of \u2018vandq_u8\u2019\r\n> \r\n\r\nAnd also incompatible type error:\r\n> [16:13:22] /workspace/srcdir/llama.cpp/ggml-quants.c:9517:34: error: incompatible type for argument 1 of \u2018vandq_u8\u2019\r\n> --\r\n> \u00a0 | [16:13:22]  9517 \\|             vs.val[0] = vandq_u8(ggml_vqtbl1q_u8(vs.val[0], mask1.val[0]), mask2);\r\n> \u00a0 | [16:13:22]       \\|                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> \u00a0 | [16:13:22]       \\|                                  \\|\r\n> \u00a0 | [16:13:22]       \\|                                  int8x16_t\r\n> \u00a0 | [16:13:22] In file included from /workspace/srcdir/llama.cpp/ggml-impl.h:54,\r\n> \u00a0 | [16:13:22]                  from /workspace/srcdir/llama.cpp/ggml-quants.h:3,\r\n> \u00a0 | [16:13:22]                  from /workspace/srcdir/llama.cpp/ggml-quants.c:1:\r\n> \u00a0 | [16:13:22] /opt/arm-linux-gnueabihf/lib/gcc/arm-linux-gnueabihf/10.2.0/include/arm_neon.h:13824:22: note: expected \u2018**uint8x16_t**\u2019 but argument is of type \u2018int8x16_t\u2019\r\n> \r\n\r\nThis is a similar problem to what you've fixed previously in this [PR](https://github.com/ggerganov/llama.cpp/pull/4828).\r\nWhere the following compats have been added:\r\n\r\n>     ggml_int8x16x4_t q2u;\r\n>     ggml_int8x16x4_t q2s;\r\n>     ggml_int8x16x4_t q8b;\r\n\r\n**Would it be possible to apply the same fix here?**\r\n\r\nSteps to reproduce:\r\n```\r\ncmake .. \\\r\n    -DCMAKE_INSTALL_PREFIX=$prefix \\\r\n    -DCMAKE_TOOLCHAIN_FILE=${CMAKE_TARGET_TOOLCHAIN} \\\r\n    -DCMAKE_BUILD_TYPE=RELEASE \\\r\n    -DBUILD_SHARED_LIBS=ON \\\r\n    -DLLAMA_BUILD_TESTS=OFF \\\r\n    -DLLAMA_BUILD_EXAMPLES=ON \\\r\n    -DLLAMA_NATIVE=OFF \\\r\n    -DLLAMA_ACCELERATE=ON \\\r\n    -DLLAMA_AVX=ON \\\r\n    -DLLAMA_AVX2=ON \\\r\n    -DLLAMA_F16C=ON \\\r\n    -DLLAMA_FMA=ON \\\r\n    -DLLAMA_BLAS=OFF \\\r\n    -DLLAMA_CUBLAS=OFF \\\r\n    -DLLAMA_CLBLAST=OFF \\\r\n    -DCMAKE_EXE_LINKER_FLAGS=\"-lrt\"\r\n```\r\n\r\nVersion: Latest master (b2277)\r\nSystem: build system for armv7l-linux-gnueabihf-cxx11 (C compiler 10.2.0)\r\nBuild log: https://buildkite.com/julialang/yggdrasil/builds/8666#018de62d-6d77-4615-ab15-8357fb2d8c09\r\n(The build log points to v2272, but I have verified locally that the error is identical on b2277)\r\n\r\nThank you for your help :)\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-27T09:11:46+00:00",
    "closed_at": "2024-04-12T01:06:38+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5748/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5748"
  },
  {
    "number": 12528,
    "title": "Eval bug: Program not working properly due to new features of \"repack Q4_K tensor\"",
    "body": "### Name and Version\n\nbuilt with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\n13th Gen Intel(R) Core(TM) i9-13900H\n\n### Models\n\nDeepSeek-V2-Lite-Q4_K_M\n\n### Problem description & steps to reproduce\n\n_Usage:  ./llama-simple -m $Model_Path/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf_\n\nI used the git bisect tool to find out that after submitting 3d82dbcbce2c677fc35fbf99574ccd107d95a1f8 , the program does not work properly. And this feature is was introduced on #12332 . This directly caused my CPU to have an overflow error when calculating \u201cffn-moe-gate\u201d.\nUnfortunately, I am not familiar with this featrue.Could anyone fix this bug?  @Srihari-mcw @ggerganov \n\n### First Bad Commit\n\n3d82dbcbce2c677fc35fbf99574ccd107d95a1f8\n\n### Relevant log output\n\n```shell\nrepack: repack tensor blk.0.attn_kv_a_mqa.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_kv_b.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n......\n......\n\nllama-simple: \\~/workspace/github/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c:7669: ggml_compute_forward_silu_f32: Assertion `!isinf(x)' failed.\nAborted (core dumped)\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-23T14:28:59+00:00",
    "closed_at": "2025-03-26T11:02:01+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12528/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12528"
  },
  {
    "number": 12520,
    "title": "Misc. bug: test-backend-ops grad crash by GGML_ASSERT error",
    "body": "### Name and Version\n\n.\\llama-cli.exe --version\nversion: 4942 (fbdfefe7)\nbuilt with MSVC 19.43.34808.0 for x64\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nTest code\n\n### Command line\n\n```shell\n> .\\test-backend-ops.exe grad -o CPY\nor\n> .\\test-backend-ops.exe grad\n```\n\n### Problem description & steps to reproduce\n\n## description\n\nCommit #12310 crashes test-backend-ops grad.\nIt doesn't seem to matter which backend.\n\n## steps to reproduce\n\nRun `test-backend-ops` as `grad` mode.\n\n### First Bad Commit\n\nCommit #12310 : SHA ba932dfb50cc694645b1a148c72f8c06ee080b17\n\n### Relevant log output\n\n```shell\n[3/23 08:24:26] PS E:\\AI\\llama.cpp\\b4942\\llama-b4942-bin-win-vulkan-x64\n> .\\test-backend-ops.exe grad -o CPY\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 32768 | matrix cores: none\nTesting 2 devices\n\nBackend 1/2: Vulkan0\n  Device description: AMD Radeon(TM) Graphics\n  Device memory: 256 MB (256 MB free)\n\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,0,0,0],permute_dst=[0,0,0,0]): OK\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,2,1,3],permute_dst=[0,0,0,0]): OK\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,3,1,2],permute_dst=[0,2,1,3]): D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml.c:5816: GGML_ASSERT(!src0_needs_grads || ggml_are_same_shape(src0, cgraph->grads[isrc0])) failed\n[3/23 08:24:39] PS E:\\AI\\llama.cpp\\b4942\\llama-b4942-bin-win-vulkan-x64\n> cd ..\\llama-b4942-bin-win-avx2-x64\\\n[3/23 08:24:55] PS E:\\AI\\llama.cpp\\b4942\\llama-b4942-bin-win-avx2-x64\n> .\\test-backend-ops.exe grad -o CPY\nTesting 1 devices\n\nBackend 1/1: CPU\n  Device description: AMD Ryzen 7 5700U with Radeon Graphics\n  Device memory: 0 MB (0 MB free)\n\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,0,0,0],permute_dst=[0,0,0,0]): OK\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,2,1,3],permute_dst=[0,0,0,0]): OK\n  CPY(type_src=f32,type_dst=f32,ne=[1,2,3,4],permute_src=[0,3,1,2],permute_dst=[0,2,1,3]): D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml.c:5816: GGML_ASSERT(!src0_needs_grads || ggml_are_same_shape(src0, cgraph->grads[isrc0])) failed\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-22T23:56:35+00:00",
    "closed_at": "2025-05-06T01:07:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12520/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12520"
  },
  {
    "number": 8739,
    "title": "Bug: Compilation failure swift build on Windows",
    "body": "### What happened?\n\nI'm trying to build the Swift package of llama.cpp on Windows using Swift 5.10. However, the build fails with errors stating that some return types are part of a C++ 14 extension. I decided to switch the cxx standard in Package.swift to C++ 14, which yielded a different set of errors in the logs field.\r\n\r\nI expected the build to succeed with no issues so I can continue building my application.\r\n\r\nI'm not too familiar with C++ build tooling, so I'd appreciate the help and would be happy to provide more verbose logs if needed.\r\n\r\nSystem info:\r\n- Windows 11\r\n- Swift 5.10\r\n- MSVC 14.8 (From VS Build Tools 2022)\n\n### Name and Version\n\nCommit 4730fac\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nMARK: With c++ 11 standard:\r\nBuilding for debugging...\r\nIn file included from D:\\swift-executable\\.build\\checkouts\\llama.cpp\\src\\unicode.cpp:5:\r\nIn file included from D:\\swift-executable\\.build\\checkouts\\llama.cpp\\src/unicode.h:4:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\string:10:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xstring:14:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xmemory:15:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:11:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\__msvc_iter_core.hpp:10:\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\utility:162:22: error: 'auto' return without trailing return type; deduced return types are a C++14 extension\r\n_NODISCARD constexpr auto&& _Tuple_get(tuple<_Types...>&& _Tuple) noexcept;\r\n                     ^\r\nIn file included from D:\\swift-executable\\.build\\checkouts\\llama.cpp\\src\\unicode.cpp:5:\r\nIn file included from D:\\swift-executable\\.build\\checkouts\\llama.cpp\\src/unicode.h:4:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\string:10:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xstring:14:\r\nIn file included from C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xmemory:15:\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:544:15: error: deduced return types are a C++14 extension\r\n    constexpr decltype(auto) operator()(_Args&&... _Vals) { // forward function call operator\r\n              ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:556:22: error: 'auto' return without trailing return type; deduced return types are a C++14 extension\r\n_NODISCARD constexpr auto _Pass_fn(_Fn& _Func) noexcept {\r\n                     ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1152:16: error: constexpr function's return type 'void' is not a literal type\r\nconstexpr void _Adl_verify_range(const _Iter& _First, const _Sentinel& _Last) {\r\n               ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1174:22: error: deduced return types are a C++14 extension\r\n_NODISCARD constexpr decltype(auto) _Get_unwrapped(_Iter&& _It) noexcept(\r\n                     ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1202:22: error: deduced return types are a C++14 extension\r\n_NODISCARD constexpr decltype(auto) _Get_unwrapped_unverified(_Iter&& _It) {\r\n                     ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1241:22: error: deduced return types are a C++14 extension\r\n_NODISCARD constexpr decltype(auto) _Get_unwrapped_n(_Iter&& _It, const _Diff _Off) {\r\n                     ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1275:16: error: constexpr function's return type 'void' is not a literal type\r\nconstexpr void _Seek_wrapped(_Iter& _It, _UIter&& _UIt) {\r\n               ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1336:22: error: 'auto' return without trailing return type; deduced return types are a C++14 extension\r\n_NODISCARD constexpr auto _Idl_distance(const _Iter& _First, const _Iter& _Last) {\r\n                     ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1341:16: error: no viable conversion from returned value of type '_Distance_unknown' to function return type 'int'\r\n        return _Distance_unknown{};\r\n               ^~~~~~~~~~~~~~~~~~~\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1637:27: error: constexpr function's return type 'void' is not a literal type\r\n    friend constexpr void _Verify_range(\r\n                          ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1643:20: error: constexpr function's return type 'void' is not a literal type\r\n    constexpr void _Verify_offset(const difference_type _Off) const noexcept {\r\n                   ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:1662:20: error: constexpr function's return type 'void' is not a literal type\r\n    constexpr void _Seek_to(const reverse_iterator<_Src>& _It) noexcept(noexcept(current._Seek_to(_It.current))) {\r\n                   ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4094:18: error: 'auto' return without trailing return type; deduced return types are a C++14 extension\r\n    _CONSTEXPR17 auto operator++(int) noexcept(\r\n                 ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4198:27: error: constexpr function's return type 'void' is not a literal type\r\n    friend constexpr void _Verify_range(const move_iterator& _First, const move_iterator<_Iter2>& _Last) noexcept {\r\n                          ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4211:20: error: constexpr function's return type 'void' is not a literal type\r\n    constexpr void _Verify_offset(const difference_type _Off) const noexcept {\r\n                   ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4229:20: error: constexpr function's return type 'void' is not a literal type\r\n    constexpr void _Seek_to(const move_iterator<_Src>& _It) noexcept(noexcept(_Current._Seek_to(_It._Get_current()))) {\r\n                   ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4233:20: error: constexpr function's return type 'void' is not a literal type\r\n    constexpr void _Seek_to(move_iterator<_Src>&& _It) noexcept(\r\n                   ^\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.38.33130\\include\\xutility:4391:22: error: 'auto' return without trailing return type; deduced return types are a C++14 extension\r\n_NODISCARD constexpr auto _To_address(const _Iter& _Val) noexcept {\r\n                     ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\n[0/27] Compiling llama src\\unicode.cpp\r\n\r\n\r\nMARK: With c++ 14 standard:\r\n\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4953 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.8\"(ptr %_Myend4.i.i.i.i634, ptr %260, token %259, ptr %cpts.i.i) #34, !dbg !23113\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4951 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.7\"(ptr %_Myend.i.i.i.i, ptr %263, token %262, ptr %ref.tmp.i) #34, !dbg !23136\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4957 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.10\"(ptr %_Myend4.i.i.i92.i, ptr %562, token %561, ptr %cpts.i71.i) #34, !dbg !24686\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4955 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.9\"(ptr %_Myend.i.i.i77.i, ptr %565, token %564, ptr %ref.tmp9.i) #34, !dbg !24698\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4949 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.6\"(ptr %match.i.sroa.15.3, ptr %match.i.sroa.52338.3, token %686) #34, !dbg !25889\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4947 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.5\"(ptr %_Myend4.i.i.i.i199.i, ptr %698, token %697, ptr %_Matches.i.i.i) #34, !dbg !26024\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4945 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.4\"(ptr %_Myend.i.i.i, ptr %701, token %700, ptr %ref.tmp343) #34, !dbg !26040\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4943 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.3\"(ptr %match.i1094.sroa.15.3, ptr %match.i1094.sroa.52352.3, token %905) #34, !dbg !27978\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock4941 = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.2\"(ptr %_Myend4.i.i.i.i203.i, ptr %917, token %916, ptr %_Matches.i.i.i1117) #34, !dbg !28072\r\nFunction has token parameter but isn't an intrinsic\r\n  %targetBlock = call i1 @\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z.cold.1\"(ptr %_Myend.i.i.i1101, ptr %920, token %919, ptr %ref.tmp389) #34, !dbg !28088\r\nin function ?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z\r\nfatal error: error in backend: Broken function found, compilation aborted!\r\nPLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace, preprocessed source, and associated run script.\r\nStack dump:\r\n0.      Program arguments: C:\\\\Users\\\\kingbri\\\\AppData\\\\Local\\\\Programs\\\\Swift\\\\Toolchains\\\\5.10.1+Asserts\\\\usr\\\\bin\\\\clang.exe -target x86_64-unknown-windows-msvc -O0 -DSWIFT_PACKAGE=1 -DDEBUG=1 -fblocks -I D:\\\\swift-executable\\\\.build\\\\checkouts\\\\llama.cpp\\\\spm-headers -Wno-shorten-64-to-32 -O3 -DNDEBUG -fno-objc-arc -fvectorize -ffp-model=fast -fno-finite-math-only -D_MT -D_DLL -Xclang --dependent-lib=msvcrt -gdwarf -gdwarf -MD -MT dependencies -MF D:\\\\swift-executable\\\\.build\\\\x86_64-unknown-windows-msvc\\\\debug\\\\llama.build\\\\src\\\\unicode.cpp.d -std=gnu++20 -c D:\\\\swift-executable\\\\.build\\\\checkouts\\\\llama.cpp\\\\src\\\\unicode.cpp -o D:\\\\swift-executable\\\\.build\\\\x86_64-unknown-windows-msvc\\\\debug\\\\llama.build\\\\src\\\\unicode.cpp.o\r\n1.      <eof> parser at end of file\r\n2.      Code generation\r\n3.      Running pass 'Function Pass Manager' on module 'D:\\swift-executable\\.build\\checkouts\\llama.cpp\\src\\unicode.cpp'.\r\n4.      Running pass 'Module Verifier' on function '@\"?unicode_regex_split@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@AEBV12@@Z\"'\r\nException Code: 0xE0000046\r\n #0 0x00007fff4e63f39c (C:\\WINDOWS\\System32\\KERNELBASE.dll+0x5f39c)\r\n #1 0x00007ff78ba6316a (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x16e316a)\r\n #2 0x00007ff78ba66a33 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x16e6a33)\r\n #3 0x00007ff78a405c70 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x85c70)\r\n #4 0x00007ff78ba37356 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x16b7356)\r\n #5 0x00007ff78ba374f1 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x16b74f1)\r\n #6 0x00007ff78b4f5c20 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1175c20)\r\n #7 0x00007ff78b3e5689 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1065689)\r\n #8 0x00007ff78b3e58e0 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x10658e0)\r\n #9 0x00007ff78b3e5b97 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1065b97)\r\n#10 0x00007ff78b3e5347 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1065347)\r\n#11 0x00007ff78be3ca35 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1abca35)\r\n#12 0x00007ff78be3bb36 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1abbb36)\r\n#13 0x00007ff78be3c43d (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1abc43d)\r\n#14 0x00007ff78e801aa8 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x4481aa8)\r\n#15 0x00007ff78d364d47 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x2fe4d47)\r\n#16 0x00007ff78c54c9e0 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x21cc9e0)\r\n#17 0x00007ff78e80050b (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x448050b)\r\n#18 0x00007ff78c54c7ce (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x21cc7ce)\r\n#19 0x00007ff78c4ffa83 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x217fa83)\r\n#20 0x00007ff78c5d2bcb (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x2252bcb)\r\n#21 0x00007ff78a406a98 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x86a98)\r\n#22 0x00007ff78a3fb88c (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x7b88c)\r\n#23 0x00007ff78c3ff34a (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x207f34a)\r\n#24 0x00007ff78ba6323f (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x16e323f)\r\n#25 0x00007ff78c3ffa18 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x207fa18)\r\n#26 0x00007ff78c376d91 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1ff6d91)\r\n#27 0x00007ff78c376f40 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1ff6f40)\r\n#28 0x00007ff78c35c790 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x1fdc790)\r\n#29 0x00007ff78a3fe71a (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x7e71a)\r\n#30 0x00007ff78e3fa414 (C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\\clang.exe+0x407a414)\r\n#31 0x00007fff508f257d (C:\\WINDOWS\\System32\\KERNEL32.DLL+0x1257d)\r\n#32 0x00007fff5126af28 (C:\\WINDOWS\\SYSTEM32\\ntdll.dll+0x5af28)\r\nclang: error: clang frontend command failed with exit code 70 (use -v to see invocation)\r\nclang version 16.0.0\r\nTarget: x86_64-unknown-windows-msvc\r\nThread model: posix\r\nInstalledDir: C:\\Users\\kingbri\\AppData\\Local\\Programs\\Swift\\Toolchains\\5.10.1+Asserts\\usr\\bin\r\nclang: note: diagnostic msg:\r\n********************\r\n\r\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\r\nPreprocessed source(s) and associated run script(s) are located at:\r\nclang: note: diagnostic msg: C:\\Users\\kingbri\\AppData\\Local\\Temp\\unicode-10fbbc.cpp\r\nclang: note: diagnostic msg: C:\\Users\\kingbri\\AppData\\Local\\Temp\\unicode-10fbbc.sh\r\nclang: note: diagnostic msg:\r\n\r\n********************\r\n[0/27] Compiling llama src\\unicode.cpp\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-28T16:12:09+00:00",
    "closed_at": "2024-09-12T01:41:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8739/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8739"
  },
  {
    "number": 669,
    "title": "llama.cpp main hangs at prompt with latest mmap updates",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nAfter upgrading to latest code compiling and then running an inference using main the following prompt should return results like before:\r\n\r\n```\r\n./main -m models/13B/ggml-model-q4_0.bin -n 512 --repeat_penalty 1.0 --color  -p \"What is controlled delivery?\"\r\nmain: seed = 1680321331\r\nllama_model_load: loading model from 'models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from 'models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n\r\n What is controlled delivery?\r\nControlled Delivery (CD) refers to the process of delivering content or messages in specific locations, such as on college campuses and at events like concerts where there are large crowds present. It can also refer specifically to a method for distributing print materials that is targeted towards particular audiences based upon their demographic characteristics (e.g., gender, age range).\r\n```\r\n\r\n# Current Behavior\r\n\r\nllama.cpp main just hangs without output showing prompt only:\r\n\r\n```\r\n\r\n./main -m models/13B/ggml-model-q4_0.bin -n 512 --repeat_penalty 1.0 --color  -p \"What is controlled delivery?\"\r\nmain: seed = 1680321575\r\nllama_model_load: loading model from 'models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from 'models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n What is controlled delivery?\r\n```\r\n\r\n# Environment and Context \r\n\r\n* Linux Ubuntu 22.04\r\n* Nvidia GPU 3070 12GB.\r\n* RAM 128 GB\r\n* NVMe storage\r\n\r\nStepping:            0\r\n    Frequency boost:     enabled\r\n    CPU max MHz:         5083.3979\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            6800.50\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext\r\n                          fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq mon\r\n                         itor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm\r\n                          sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb\r\n                         cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed ad\r\n                         x smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero\r\n                         irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pause\r\n                         filter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor sm\r\n                         ca fsrm\r\nVirtualization features:\r\n  Virtualization:        AMD-V\r\nCaches (sum of all):\r\n  L1d:                   512 KiB (16 instances)\r\n  L1i:                   512 KiB (16 instances)\r\n  L2:                    8 MiB (16 instances)\r\n  L3:                    64 MiB (2 instances)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-31\r\nVulnerabilities:\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\nLinux ai-llm-dev 5.15.0-67-generic #74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n```\r\nPython 3.11.2\r\n\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T04:10:19+00:00",
    "closed_at": "2023-04-01T08:54:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/669/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/669"
  },
  {
    "number": 14576,
    "title": "Eval bug: ROCm error: batched GEMM not supported",
    "body": "### Name and Version\n\nllama built from source - latest master.\n```\nllama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: no, Wave Size: 32\nversion: 5836 (b9c3eefd)\nbuilt with cc (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3) for x86_64-redhat-linux\n```\n\nBuild command:\n```\nHIPCXX=\"$(hipconfig -l)/clang\" HIP_PATH=\"$(hipconfig -R)\"  cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1100 -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=OFF && cmake --build build --config Release -- -j 16\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\nAMD Ryzen 5 5600X\nAMD ATI Radeon RX 7900 XTX\n\n\n### Models\n\nAll models tested failed\nNote: Non-Quant models untested.\n\n### Problem description & steps to reproduce\n\n`llama-run --ngl 10 <MODEL>`\nfails with any GPU use, ngl 0 functions as expected\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama-run --ngl 10 codellama-7b.Q5_K_S.gguf\n> hello\nROCm error: CUBLAS_STATUS_NOT_SUPPORTED\n  current device: 0, in function ggml_cuda_mul_mat_batched_cublas_impl at /home/Gibus/dev/repos/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:1928\n  hipblasGemmStridedBatchedEx(ctx.cublas_handle(), HIPBLAS_OP_T, HIPBLAS_OP_N, ne01, ne11, ne10, alpha, src0_ptr, cu_data_type_a, nb01/nb00, nb02/nb00, src1_ptr, cu_data_type_b, s11, s12, beta, dst_t, cu_data_type, ne0, ne1*ne0, ne12*ne13, cu_compute_type, HIPBLAS_GEMM_DEFAULT)\n/home/Gibus/dev/repos/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:78: ROCm error\n[New LWP 14104]\n[New LWP 14103]\n[New LWP 14102]\n[New LWP 14096]\n[New LWP 14077]\n\nThis GDB supports auto-downloading debuginfo from the following URLs:\n  <https://debuginfod.fedoraproject.org/>\nEnable debuginfod for this session? (y or [n]) [answered N; input not from terminal]\nDebuginfod has been disabled.\nTo make this setting permanent, add 'set debuginfod enabled off' to .gdbinit.\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\n0x00007f60018ead93 in wait4 () from /lib64/libc.so.6\n#0  0x00007f60018ead93 in wait4 () from /lib64/libc.so.6\n#1  0x00007f600484702b in ggml_print_backtrace () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-base.so\n#2  0x00007f600484717e in ggml_abort () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-base.so\n#3  0x00007f6004469862 in ggml_cuda_error(char const*, char const*, char const*, int, char const*) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-hip.so\n#4  0x00007f6004474176 in ggml_cuda_mul_mat_batched_cublas(ggml_backend_cuda_context&, ggml_tensor const*, ggml_tensor const*, ggml_tensor*) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-hip.so\n#5  0x00007f60044710b8 in ggml_cuda_mul_mat(ggml_backend_cuda_context&, ggml_tensor const*, ggml_tensor const*, ggml_tensor*) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-hip.so\n#6  0x00007f600446f237 in ggml_backend_cuda_graph_compute(ggml_backend*, ggml_cgraph*) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-hip.so\n#7  0x00007f600485df94 in ggml_backend_sched_graph_compute_async () from /home/Gibus/dev/repos/llama.cpp/build/bin/libggml-base.so\n#8  0x00007f6004a49e61 in llama_context::graph_compute(ggml_cgraph*, bool) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libllama.so\n#9  0x00007f6004a4a0ab in llama_context::process_ubatch(llama_ubatch const&, llm_graph_type, llama_memory_context_i*, ggml_status&) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libllama.so\n#10 0x00007f6004a4f045 in llama_context::decode(llama_batch const&) () from /home/Gibus/dev/repos/llama.cpp/build/bin/libllama.so\n#11 0x00007f6004a501cb in llama_decode () from /home/Gibus/dev/repos/llama.cpp/build/bin/libllama.so\n#12 0x000000000041b926 in main ()\n[Inferior 1 (process 14075) detached]\nAborted (core dumped)\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-08T03:01:22+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14576/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14576"
  },
  {
    "number": 10374,
    "title": "LORA Adapter Hot Swap Implementation Problem",
    "body": "I have been following the discussions in the following threads:\r\n\r\n[Pull Request #8332](https://github.com/ggerganov/llama.cpp/pull/8332)\r\n[Pull Request #8857](https://github.com/ggerganov/llama.cpp/pull/8857)\r\nI believe that the ideal implementation of \"hot swap\" should address the following scenario:\r\n\r\nWhen processing a request, llama.cpp should be able to dynamically determine and apply the correct LoRA adapter based on the specific requirements of the request. While I understand that the current implementation involves a scaling mechanism, this approach introduces significant issues.\r\n\r\nFor example, when llama.cpp is running as a server handling multiple simultaneous requests with different LoRA adapters, the scaling method creates a problematic dependency. If Request 1 comes in requiring LoRA Adapter 1, the scaling is adjusted to prioritize Adapter 1. However, if Request 2 arrives shortly afterward, requiring LoRA Adapter 2, the scaling is adjusted again, effectively disabling Adapter 1 in favor of Adapter 2. This adjustment disrupts Request 1 if it is still in the middle of processing.\r\n\r\nThis issue becomes even more pronounced in streaming scenarios where a high volume of concurrent requests are being processed, as is often the case with production-level systems.\r\n\r\nWhy must LoRA adapters rely on scaling adjustments? Why can\u2019t they be separated and applied independently per request? In both threads (#8332 and #8857), I see other users emphasizing that the entire purpose of hot swap functionality is to enable per-request adapter switching. Yet, the authors repeatedly suggest that merging should happen beforehand, citing computational expense. I see the authors practically shutting down the other users suggesting this change. \r\n\r\nHowever, the whole point of hot swap is precisely to avoid merging, as this is impractical in many real-world applications. Whether for runtime environments, pre-deployment preparations, or edge devices, merging is often not feasible\u2014especially when considering dynamic content updates or systems with continuously expanding features.\r\n\r\nFor example, in a system where NPCs need to roleplay various characters that can be expanded or updated, hot swapping LoRA adapters on a per-request basis is essential.\r\n\r\nI also note that this hot swap functionality is already implemented in frameworks like ollama and vLLM. Why, then, has it not been properly implemented in llama.cpp? (Or perhaps I\u2019ve missed something and this feature already exists\u2014if so, I\u2019d appreciate guidance on how to use it). At the moment, however, I do not see this capability.\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-18T05:43:29+00:00",
    "closed_at": "2025-01-03T01:07:23+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10374/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10374"
  },
  {
    "number": 3377,
    "title": "llama : support sliding window attention",
    "body": "For more info, see: https://github.com/mistralai/mistral-src and references there in.\r\n\r\nAlso: https://arxiv.org/pdf/2310.06825v1.pdf\r\n\r\nWith #3228 it should be relatively easy to support this.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-28T12:12:40+00:00",
    "closed_at": "2024-11-01T01:21:36+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3377/reactions",
      "total_count": 58,
      "+1": 44,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 13,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3377"
  },
  {
    "number": 12335,
    "title": "Misc. bug: malloc error in #8  0x00007fffda1ea8ec in unicode_regex_split",
    "body": "### Name and Version\n\n\nversion \ud83d\udc4d \n```\ngit log \ncommit d2fe216fb2fb7ca8627618c9ea3a2e7886325780 (HEAD -> master, tag: b4667, origin/master, origin/HEAD)\nAuthor: Eric Curtin <ecurtin@redhat.com>\nDate:   Fri Feb 7 14:42:46 2025 +0000\n\n    Make logging more verbose (#11714)\n    \n    Debugged an issue with a user who was on a read-only filesystem.\n    \n    Signed-off-by: Eric Curtin <ecurtin@redhat.com>\n\n```\nbuild  library .so with: \n```\ncmake -B build_x86_gpu -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"70;75;86;87\" -DGGML_CUDA_GRAPHS_DEFAULT=ON -DGGML_USE_CUDA=1\ncmake --build build_x86_gpu --config Debug -j32\nsudo cmake --install build_x86_gpu --prefix install_x86_gpu\n```\n\n\ncoredump with backtrace in attach file llama.cpp.txt:\n\n[llama.cpp.txt](https://github.com/user-attachments/files/19185884/llama.cpp.txt)\n\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library)\n\n### Command line\n\n```shell\napi named common_tokenize in common/common.h was called by this code\n\n\n std::string system_prompt;\n    system_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|>\";\n    std::vector<llama_token> tokens_system;\n    tokens_system = common_tokenize(ctx_llava->ctx_llama, system_prompt, true, true);\n```\n\n### Problem description & steps to reproduce\n\nregex_expr_collapsed += regex_expr[i]; /zdrive/llama.cpp/src/unicode.cpp:807    this code  sometimes coredump when regex_expr[i] is '*', but sometimes can work success. why? how to avoid or fix it ?\n\n\n     \n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-11T14:44:11+00:00",
    "closed_at": "2025-04-26T01:07:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12335/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12335"
  },
  {
    "number": 12764,
    "title": "Misc. bug: (HIP) RDNA4 prefill performance almost halved with CUBLAS_COMPUTE_32F",
    "body": "### Name and Version\n\n$ llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon Graphics, gfx1201 (0x1201), VMM: no, Wave Size: 32\nversion: 5054 (7a84777f)\nbuilt with AMD clang version 17.0.6 (CLANG: AOCC_5.0.0-Build#1377 2024_09_24) for x86_64-unknown-linux-gnu\n\nBuild params: -DGGML_HIP=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_RPC=ON -DCMAKE_HIP_ARCHITECTURES=gfx1100,gfx1201 -DGGML_HIP_ROCWMMA_FATTN=ON\n\nUsing LD_PRELOAD to load locally built hipBLASLt from develop branch, commit: ea7a0aceca8d54ce92428997d8b796a796d99def\nUsing rocWMMA from develop branch, commit: f3b60adb7bded2114cca77e7165591e2d5a4d07e\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library)\n\n### Command line\n\n```shell\nllama-bench -m ~/models/qwen2.5-14b-q4_0.gguf -ngl 99 -fa 1\n```\n\n### Problem description & steps to reproduce\n\n#### Master\n| model                          |       size |     params | backend    | ngl | fa |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | -------------------: |\n| qwen2 14B Q4_0                 |   7.93 GiB |    14.77 B | ROCm,RPC   | 999 |  1 |         pp512 |       1260.16 \u00b1 4.13 |\n\n#### After reverting the two changes [here](https://github.com/hjc4869/llama.cpp/commit/831e0adca8e77ee1eca90f235ee03c77a61256bb)\n| model                          |       size |     params | backend    | ngl | fa |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | -------------------: |\n| qwen2 14B Q4_0                 |   7.93 GiB |    14.77 B | ROCm,RPC   | 999 |  1 |         pp512 |      2147.61 \u00b1 13.16 |\n\nThe first change causes major performance loss, while the second one is relatively minor (around 0.5% - 1% from my rough testing)\n\n### First Bad Commit\n\n[bd40678df768ab189b26b8b03e3de4062e3b71a3](https://github.com/ggml-org/llama.cpp/commit/bd40678df768ab189b26b8b03e3de4062e3b71a3)\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-05T05:51:39+00:00",
    "closed_at": "2025-04-10T08:15:00+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12764/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12764"
  },
  {
    "number": 830,
    "title": "Intermittent segmentation faults in llama_sample_top_p_top_k()",
    "body": "# Expected Behavior\r\n\r\nI have been getting intermittent segfaults for no apparent reason. Sometimes they occur right at the beginning of text generation, and sometimes they occur after a lot of text has already been generated. They seem to be deterministic in that I can sometimes work around them by changing the prompt, but if I don\u2019t change the prompt, they consistently occur. I normally use the 65B model, which exhibits the problem, but I am attaching a repro for the 13B model. I am not 100% sure but I believe the issue affects all four model sizes (7B, 13B, 30B, 65B).\r\n\r\n# Current Behavior\r\n\r\nIntermittent segfaults\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n2019 16-inch MacBook Pro, 2.3 GHz 8-Core Intel Core i9, 64 GB of RAM\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Darwin Ryans-MBP-2.lan 22.3.0 Darwin Kernel Version 22.3.0: Mon Jan 30 20:42:11 PST 2023; root:xnu-8792.81.3~2/RELEASE_X86_64 x86_64`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\n```\r\nPython 3.10.0\r\n\r\nGNU Make 3.81\r\nCopyright (C) 2006  Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.\r\nThere is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\r\nPARTICULAR PURPOSE.\r\n\r\nThis program built for i386-apple-darwin11.3.0\r\n\r\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\r\nTarget: x86_64-apple-darwin22.3.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n# Failure Information (for bugs)\r\n\r\nSee log below\r\n\r\n# Steps to Reproduce\r\n\r\nI can consistently reproduce on my machine by running the following command:\r\n```\r\n ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k \r\nrlanday@Ryans-MBP-2 llama.cpp % ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k 40 --temp 0.7 --repeat_penalty 1.176470588235294 -t 8 -n -1 --repeat_last_n 16384 -p \"Active Internet connections\" -s 1680491962\r\n```\r\n\r\n# Failure Logs\r\n\r\nEnvironment info:\r\n```\r\ncommit cc9cee8e9e7598bd280295f6264f36d3a9224006\r\n\r\nrlanday@Ryans-MBP-2 ~ % sysctl -a | grep machdep.cpu\r\nmachdep.cpu.tlb.inst.large: 8\r\nmachdep.cpu.tlb.data.small: 64\r\nmachdep.cpu.tlb.data.small_level1: 64\r\nmachdep.cpu.address_bits.physical: 39\r\nmachdep.cpu.address_bits.virtual: 48\r\nmachdep.cpu.tsc_ccc.numerator: 192\r\nmachdep.cpu.tsc_ccc.denominator: 2\r\nmachdep.cpu.mwait.linesize_min: 64\r\nmachdep.cpu.mwait.linesize_max: 64\r\nmachdep.cpu.mwait.extensions: 3\r\nmachdep.cpu.mwait.sub_Cstates: 286531872\r\nmachdep.cpu.thermal.sensor: 1\r\nmachdep.cpu.thermal.dynamic_acceleration: 1\r\nmachdep.cpu.thermal.invariant_APIC_timer: 1\r\nmachdep.cpu.thermal.thresholds: 2\r\nmachdep.cpu.thermal.ACNT_MCNT: 1\r\nmachdep.cpu.thermal.core_power_limits: 1\r\nmachdep.cpu.thermal.fine_grain_clock_mod: 1\r\nmachdep.cpu.thermal.package_thermal_intr: 1\r\nmachdep.cpu.thermal.hardware_feedback: 0\r\nmachdep.cpu.thermal.energy_policy: 1\r\nmachdep.cpu.xsave.extended_state: 31 832 1088 0\r\nmachdep.cpu.xsave.extended_state1: 15 832 256 0\r\nmachdep.cpu.arch_perf.version: 4\r\nmachdep.cpu.arch_perf.number: 4\r\nmachdep.cpu.arch_perf.width: 48\r\nmachdep.cpu.arch_perf.events_number: 7\r\nmachdep.cpu.arch_perf.events: 0\r\nmachdep.cpu.arch_perf.fixed_number: 3\r\nmachdep.cpu.arch_perf.fixed_width: 48\r\nmachdep.cpu.cache.linesize: 64\r\nmachdep.cpu.cache.L2_associativity: 4\r\nmachdep.cpu.cache.size: 256\r\nmachdep.cpu.max_basic: 22\r\nmachdep.cpu.max_ext: 2147483656\r\nmachdep.cpu.vendor: GenuineIntel\r\nmachdep.cpu.brand_string: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\r\nmachdep.cpu.family: 6\r\nmachdep.cpu.model: 158\r\nmachdep.cpu.extmodel: 9\r\nmachdep.cpu.extfamily: 0\r\nmachdep.cpu.stepping: 13\r\nmachdep.cpu.feature_bits: 9221960262849657855\r\nmachdep.cpu.leaf7_feature_bits: 43804591 1073741824\r\nmachdep.cpu.leaf7_feature_bits_edx: 3154120192\r\nmachdep.cpu.extfeature_bits: 1241984796928\r\nmachdep.cpu.signature: 591597\r\nmachdep.cpu.brand: 0\r\nmachdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\r\nmachdep.cpu.leaf7_features: RDWRFSGS TSC_THREAD_OFFSET SGX BMI1 AVX2 SMEP BMI2 ERMS INVPCID FPU_CSDS MPX RDSEED ADX SMAP CLFSOPT IPT SGXLC MDCLEAR IBRS STIBP L1DF ACAPMSR SSBD\r\nmachdep.cpu.extfeatures: SYSCALL XD 1GBPAGE EM64T LAHF LZCNT PREFETCHW RDTSCP TSCI\r\nmachdep.cpu.logical_per_package: 16\r\nmachdep.cpu.cores_per_package: 8\r\nmachdep.cpu.microcode_version: 244\r\nmachdep.cpu.processor_flag: 5\r\nmachdep.cpu.core_count: 8\r\nmachdep.cpu.thread_count: 16\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                        1.22.2\r\nsentencepiece                0.1.97\r\ntorch                        1.13.0\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ./models/13B/ggml-model-q4_0.bin\r\n0abc81985f6c529faaa661dee3674efd  ./models/13B/ggml-model-q4_0.bin\r\n```\r\n\r\nHere is an ASAN output:\r\n\r\n```\r\nrlanday@Ryans-MBP-2 llama.cpp % ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k 40 --temp 0.7 --repeat_penalty 1.176470588235294 -t 8 -n -1 --repeat_last_n 16384 -p \"Active Internet connections\" -s 1680491962\r\nmain(30917,0x7ff844413680) malloc: nano zone abandoned due to inability to preallocate reserved vm space.\r\nmain: seed = 1680491962\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 2048\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  = 1600.00 MB\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.700000, top_k = 40, top_p = 0.000000, repeat_last_n = 16384, repeat_penalty = 1.176471\r\ngenerate: n_ctx = 2048, n_batch = 8, n_predict = -1, n_keep = 0\r\n\r\n\r\n Active Internet connections=================================================================\r\n==30917==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x625000000000 at pc 0x000107c8c2e1 bp 0x7ff7b8a76690 sp 0x7ff7b8a75e58\r\nREAD of size 65536 at 0x625000000000 thread T0\r\n    #0 0x107c8c2e0 in __asan_memmove+0xe0 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x472e0) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x107499506 in std::__1::pair<int const*, int*> std::__1::__copy_impl[abi:v15006]<int const, int, void>(int const*, int const*, int*) copy.h:56\r\n    #2 0x107498d42 in std::__1::pair<int const*, int*> std::__1::__copy[abi:v15006]<int const*, int const*, int*, 0>(int const*, int const*, int*) copy.h:94\r\n    #3 0x107498958 in int* std::__1::copy[abi:v15006]<int const*, int*>(int const*, int const*, int*) copy.h:103\r\n    #4 0x107498868 in int* std::__1::__uninitialized_allocator_copy[abi:v15006]<std::__1::allocator<int>, int, int, (void*)0>(std::__1::allocator<int>&, int const*, int const*, int*) uninitialized_algorithms.h:575\r\n    #5 0x1074985e4 in std::__1::enable_if<__is_cpp17_forward_iterator<int const*>::value, void>::type std::__1::vector<int, std::__1::allocator<int> >::__construct_at_end<int const*>(int const*, int const*, unsigned long) vector:1031\r\n    #6 0x10760a283 in std::__1::vector<int, std::__1::allocator<int> >::vector<int const*>(int const*, std::__1::enable_if<(__is_cpp17_forward_iterator<int const*>::value) && (is_constructible<int, std::__1::iterator_traits<int const*>::reference>::value), int const*>::type) vector:1158\r\n    #7 0x10751cdd4 in std::__1::vector<int, std::__1::allocator<int> >::vector<int const*>(int const*, std::__1::enable_if<(__is_cpp17_forward_iterator<int const*>::value) && (is_constructible<int, std::__1::iterator_traits<int const*>::reference>::value), int const*>::type) vector:1152\r\n    #8 0x10751cb73 in llama_sample_top_p_top_k llama.cpp:1808\r\n    #9 0x10748cd9d in main main.cpp:292\r\n    #10 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\n0x625000000000 is located 256 bytes to the left of 8192-byte region [0x625000000100,0x625000002100)\r\nfreed by thread T0 here:\r\n    #0 0x107c9d17d in wrap__ZdlPv+0x7d (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x5817d) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x107494d84 in void std::__1::__libcpp_operator_delete[abi:v15006]<void*>(void*) new:256\r\n    #2 0x107494d68 in void std::__1::__do_deallocate_handle_size[abi:v15006]<>(void*, unsigned long) new:280\r\n    #3 0x107494d40 in std::__1::__libcpp_deallocate[abi:v15006](void*, unsigned long, unsigned long) new:290\r\n    #4 0x107545189 in std::__1::allocator<float>::deallocate[abi:v15006](float*, unsigned long) allocator.h:128\r\n    #5 0x107544e34 in std::__1::allocator_traits<std::__1::allocator<float> >::deallocate[abi:v15006](std::__1::allocator<float>&, float*, unsigned long) allocator_traits.h:282\r\n    #6 0x1075e9162 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::~__split_buffer() __split_buffer:355\r\n    #7 0x1075e4dd4 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::~__split_buffer() __split_buffer:352\r\n    #8 0x10760952f in std::__1::vector<float, std::__1::allocator<float> >::__append(unsigned long) vector:1051\r\n    #9 0x107514c6f in std::__1::vector<float, std::__1::allocator<float> >::resize(unsigned long) vector:1918\r\n    #10 0x10751ba07 in llama_eval_internal(llama_context&, int const*, int, int, int) llama.cpp:982\r\n    #11 0x107519a90 in llama_eval llama.cpp:1727\r\n    #12 0x10748c7ec in main main.cpp:267\r\n    #13 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\npreviously allocated by thread T0 here:\r\n    #0 0x107c9cd5d in wrap__Znwm+0x7d (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x57d5d) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x1074974e4 in void* std::__1::__libcpp_operator_new[abi:v15006]<unsigned long>(unsigned long) new:246\r\n    #2 0x1074974c8 in std::__1::__libcpp_allocate[abi:v15006](unsigned long, unsigned long) new:272\r\n    #3 0x1075e591c in std::__1::allocator<float>::allocate[abi:v15006](unsigned long) allocator.h:112\r\n    #4 0x1075e56ba in std::__1::__allocation_result<std::__1::allocator_traits<std::__1::allocator<float> >::pointer> std::__1::__allocate_at_least[abi:v15006]<std::__1::allocator<float> >(std::__1::allocator<float>&, unsigned long) allocate_at_least.h:54\r\n    #5 0x1075e52c3 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::__split_buffer(unsigned long, unsigned long, std::__1::allocator<float>&) __split_buffer:316\r\n    #6 0x1075e46dc in std::__1::__split_buffer<float, std::__1::allocator<float>&>::__split_buffer(unsigned long, unsigned long, std::__1::allocator<float>&) __split_buffer:312\r\n    #7 0x107514adb in std::__1::vector<float, std::__1::allocator<float> >::reserve(unsigned long) vector:1500\r\n    #8 0x10750c92e in llama_init_from_file llama.cpp:1652\r\n    #9 0x107489ae9 in main main.cpp:102\r\n    #10 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x472e0) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00) in __asan_memmove+0xe0\r\nShadow bytes around the buggy address:\r\n  0x1c49ffffffb0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffc0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffd0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffe0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49fffffff0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n=>0x1c4a00000000:[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x1c4a00000010: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x1c4a00000020: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000030: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000040: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000050: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07 \r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n==30917==ABORTING\r\nzsh: abort      ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k \r\n```\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T12:33:14+00:00",
    "closed_at": "2024-04-11T01:06:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/830/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/830"
  },
  {
    "number": 2269,
    "title": "CUDA Error 400: Invalid Resource Handle when Running on Single GPU",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI am trying to make `llama.cpp` run on a single GPU (in my case, GPU 5) on a multi-GPU system because there are other tasks running on my other GPUs.\r\n\r\n# Current Behavior\r\n\r\n`llama.cpp` crashes with `CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle`\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using\r\n\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              112\r\nOn-line CPU(s) list: 0-111\r\nThread(s) per core:  2\r\nCore(s) per socket:  28\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz\r\nStepping:            6\r\nCPU MHz:             1000.013\r\nCPU max MHz:         4000.0000\r\nCPU min MHz:         1000.0000\r\nBogoMIPS:            5400.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            39424K\r\nNUMA node0 CPU(s):   0-27,56-83\r\nNUMA node1 CPU(s):   28-55,84-111\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\n\r\n* Operating System\r\n\r\nLinux cb68e1005cfb 4.15.0-184-generic #194-Ubuntu SMP Thu Jun 2 18:54:48 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n* SDK version:\r\n\r\n```\r\nPython 3.10.12\r\nGNU Make 4.1\r\ng++-11 (Ubuntu 11.4.0-2ubuntu1~18.04) 11.4.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nllama.cpp crashes after outputting\r\n\r\n```\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n```\r\n\r\nwith `CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle`\r\n\r\n# Steps to Reproduce\r\n\r\n1. Run `llama.cpp` with the arguments `-ts 0,0,0,0,0,100,0,0,0,0 -mg 5` (to try to instruct `llama.cpp` to only utilize GPU 5)\r\n\r\n# Failure Logs\r\n\r\n```\r\n./main -m '/home/michel/workspace/Inference Models/chinese-alpaca-plus-7b-f16-ggml.bin' -c 2048 --interactive-first --color --reverse-prompt \"User:\" --in-suffix \"Assistant:\" --prompt $'The following is a conversation between an AI assistant called Assistant and a human user called User.\\nThe assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n' -ngl 40 -ts 0,0,0,0,0,100,0,0,0,0 -mg 5\r\nmain: build = 0 (unknown)\r\nmain: seed  = 1689738272\r\nggml_init_cublas: found 10 CUDA devices:\r\n  Device 0: GeForce RTX 3090, compute capability 8.6\r\n  Device 1: GeForce RTX 3090, compute capability 8.6\r\n  Device 2: GeForce RTX 3090, compute capability 8.6\r\n  Device 3: GeForce RTX 3090, compute capability 8.6\r\n  Device 4: GeForce RTX 3090, compute capability 8.6\r\n  Device 5: GeForce RTX 3090, compute capability 8.6\r\n  Device 6: GeForce RTX 3090, compute capability 8.6\r\n  Device 7: GeForce RTX 3090, compute capability 8.6\r\n  Device 8: GeForce RTX 3090, compute capability 8.6\r\n  Device 9: GeForce RTX 3090, compute capability 8.6\r\nllama.cpp: loading model from /home/michel/workspace/Inference Models/chinese-alpaca-plus-7b-f16-ggml.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 49954\r\nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: freq_base  = 10000.0\r\nllama_model_load_internal: freq_scale = 1\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =    0.08 MB\r\nllama_model_load_internal: using CUDA for GPU acceleration\r\nggml_cuda_set_main_device: using device 5 (GeForce RTX 3090) as main device\r\nllama_model_load_internal: mem required  = 2062.35 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\r\nllama_model_load_internal: offloading 32 repeating layers to GPU\r\nllama_model_load_internal: offloading non-repeating layers to GPU\r\nllama_model_load_internal: offloading v cache to GPU\r\nllama_model_load_internal: offloading k cache to GPU\r\nllama_model_load_internal: offloaded 35/35 layers to GPU\r\nllama_model_load_internal: total VRAM used: 14154 MB\r\nllama_new_context_with_model: kv self size  = 1024.00 MB\r\n\r\nsystem_info: n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nInput suffix: 'Assistant:'\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\nCUDA error 400 at ggml-cuda.cu:3343: invalid resource handle\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-19T03:47:47+00:00",
    "closed_at": "2024-04-09T01:07:51+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2269/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2269"
  },
  {
    "number": 7613,
    "title": "Bug: Inconsistent ggml-4-x86-cuda-v100 ci failures on master",
    "body": "**Note: Only one datapoint of ci failure, but it would be important to keep track of this behavior over the next few commits**\r\n\r\n### What happened?\r\n\r\n* [Passing commit](https://github.com/ggml-org/ci/tree/results/llama.cpp/50/4f0c340f6b5e04de682f6ddefdd3b81208df5d/ggml-4-x86-cuda-v100)\r\n    - https://github.com/ggerganov/llama.cpp/commit/504f0c340f6b5e04de682f6ddefdd3b81208df5d\r\n\r\n* [Failing commit, but its a readme only change](https://github.com/ggml-org/ci/tree/results/llama.cpp/0e/8d8bfd6caf1d0a8cbdf9d3d5c06fbbb9dfced8/ggml-4-x86-cuda-v100)\r\n   - https://github.com/ggerganov/llama.cpp/commit/0e8d8bfd6caf1d0a8cbdf9d3d5c06fbbb9dfced8\r\n\r\nNoticed that it said it's failing in `20 - test-backend-ops`, it be good to identify the cause of this issue and potential ways to fix it. The failure in test *#20* in test-backend-ops looked like below which doesn't seem to explain much to me. But hopefully it makes sense to someone else here.\r\n\r\n`[CPY] NMSE = 0.000003149 > 0.000000100` looks interesting however \r\n\r\n### Name and Version\r\n\r\nbetween commit 504f0c340f6b5e04de682f6ddefdd3b81208df5d and 0e8d8bfd6caf1d0a8cbdf9d3d5c06fbbb9dfced8\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nOther? (Please let us know in description)\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n\u001b[1;32mOK\u001b[0m\r\n  CPY(type_src=f32,type_dst=q4_1,ne=[256,4,4,4]): ggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\n[CPY] NMSE = 0.000003149 > 0.000000100 ggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\n\u001b[1;31mFAIL\u001b[0m\r\n  CPY(type_src=f32,type_dst=q5_0,ne=[256,4,4,4]): ggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\nggml_backend_cuda_graph_compute: disabling CUDA graphs due to GPU architecture\r\n\u001b[1;32mOK\u001b[0m\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-05-29T09:47:11+00:00",
    "closed_at": "2024-07-13T01:06:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7613/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7613"
  },
  {
    "number": 13203,
    "title": "Misc. bug: Docker images on GHCR stuck at **b5174** \u2013 \u201cPublish Docker image\u201d workflow failing since 2025\u201104\u201124",
    "body": "### Name and Version\n\nversion: 5174 (56304069)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-cli\n\n### Command line\n\n```shell\nllama-cli --version\n```\n\n### Problem description & steps to reproduce\n\n### Summary\nPulling any of the moving Docker tags (`full-vulkan`, `full`, `server`, etc.) still returns **build\u00a05174 (56304069)**, which was published on **2025\u201104\u201124**.\u202fMeanwhile, the Releases page has advanced to **b5223** and beyond, so no Docker images have been published for roughly a week.\n\n---\n\n### Steps to reproduce\n```bash\n# 1. Pull the latest image\ndocker pull ghcr.io/ggml-org/llama.cpp:full-vulkan\n\n# 2. Check the build banner (bypasses tools.sh)\ndocker run --rm \\\n  --entrypoint /app/llama-cli \\\n  ghcr.io/ggml-org/llama.cpp:full-vulkan \\\n  --version\n```\nOutput:\n```\nversion: 5174 (56304069)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n```\n\n---\n\n### Expected\nThe nightly **Publish\u00a0Docker\u00a0image** workflow pushes a new digest for every numbered release, so pulling `full-vulkan` (or any moving tag) tracks the current release (today: b522x).\n\n### Actual\n* All moving tags (`full`, `full\u2011vulkan`, `server`, etc.) resolve to digest **sha256:23c3ec7e46c7\u2026** \u2192 build\u00a05174.\n* **Publish\u00a0Docker\u00a0image** workflow has failed for every run after **#14926** (24\u00a0Apr) with the same buildx/CMake error, so no new images are uploaded.\n* **Make\u00a0archives** workflow continues to succeed, producing release tags up to **b5223**.\n\n---\n\n### Evidence\n* **Releases page**: latest tag is `b5223`, dated today.\n* **Actions\u00a0\u2192\u00a0Publish\u00a0Docker\u00a0image**: all runs after 2025\u201104\u201124 are red.\n* Local pull still shows 24\u00a0Apr digest & build banner (commands above).\n\n---\n\n### Why it matters\nMany users install *llama.cpp* exclusively via Docker. Right now they are stuck on a week\u2011old build (missing recent fixes and features) unless they build the image locally.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-30T02:02:06+00:00",
    "closed_at": "2025-04-30T08:44:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13203/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13203"
  },
  {
    "number": 3800,
    "title": "Response 200 when using LLava via Server for python",
    "body": "Hello, i am having a problem using LLava via Server.\r\n\r\nserver launch command\r\n```\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`\r\nexport CUDA_VISIBLE_DEVICES=4,5,6,7\r\n./server -m models/llava/ggml-model-q4_k.gguf --mmproj models/llava/mmproj-model-f16.gguf -c 2048 --port 8080 -ngl 35 -mg 3 -t 20\r\n```\r\n\r\nrequest code\r\n```\r\nimport base64\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\nimport subprocess\r\nimport json\r\n\r\ndef image_to_base64(img_path):\r\n    with Image.open(img_path) as image:\r\n        image = image.resize((336,336))\r\n        buffered = BytesIO()\r\n        image.save(buffered, format=\"PNG\")\r\n        img_str = base64.b64encode(buffered.getvalue())\r\n    return img_str.decode('utf-8')\r\n\r\npayload = {\r\n    \"prompt\": \"User: [img-1]Describe the image about indoor house scene. You must describe what you can see (e.g., door, stairs, bed), and where are you in (e.g., bathroom, kitchen). Only say description, do not say anything else.\",\r\n    \"temperature\": 0.1,\r\n    \"image_data\": [{\"data\": image_to_base64('/home/vln_workspace/output_grid_image.jpeg'), \"id\": 1}],\r\n    \"system_prompt\": {\r\n        \"prompt\": \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\",\r\n        \"anti_prompt\": \"User:\",\r\n        \"assistant_name\": \"Assistant:\"\r\n    },\r\n}\r\n\r\nurl = \"http://localhost:8080/completion\"\r\nheaders = {\r\n    \"Content-Type\": \"application/json\"\r\n}\r\n\r\nimport requests\r\nresponse = requests.post(url, headers=headers, json=payload)\r\nprint(response)\r\n\r\n```\r\n\r\n\r\nLOG\r\n```\r\nslot 0 - image loaded [id: 1] resolution (336 x 336)\r\nslot 0 is processing [task id: 7]\r\n\r\nprint_timings: prompt eval time =       0.00 ms /     0 tokens (     inf ms per token,     -nan tokens per second)\r\nprint_timings:        eval time = -89935039284.51 ms /     0 runs   (    -inf ms per token,    -0.00 tokens per second)\r\nprint_timings:       total time = -89935039284.51 ms\r\nupdating system prompt\r\n{\"timestamp\":1698342235,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2156,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":36330,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\r\nsystem prompt updated\r\nslot 0 released (0 tokens in cache)\r\n```\r\n\r\nas it only says Error 200, I cannot trace anything.....\r\n\r\nis there anyone currently deploying LLaVa on server and getting responst into python? I need help...\r\nMight be related to #3761 ...\r\n\r\n\r\n(Im currently not using llama-cpp-python as there are always error while loading the model)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-26T17:51:11+00:00",
    "closed_at": "2023-10-27T03:30:59+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3800/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3800"
  },
  {
    "number": 13851,
    "title": "CUDA error: an illegal memory access was encountered (with large prompts)",
    "body": "### Name and Version\n\n./build/bin/llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\nversion: 5518 (26b79b6c)\nbuilt with cc (Debian 12.2.0-14+deb12u1) 12.2.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-cli\n\n### Command line\n\n```shell\nllama-cli -t 4 --flash-attn --color --conversation --multiline-input --mirostat 2 --tensor-split 1.2,0.8 --ctx-size $((8192*20)) --n-gpu-layers 66 --temp 0.9 -m /work/models/misc/gemma/gemma-3-12B-it-Q6_KLA.gguf\n```\n\n### Problem description & steps to reproduce\n\nLarge prompt (above 16k) consistently fails with error:\n- paste text >16\n- crash\n\n\n### First Bad Commit\n\n952f3953c1b61cc70e79e536c42ddce6a5ea5ea7\n\n### Relevant log output\n\n```shell\nwork/src/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:75: CUDA error\nCUDA error: an illegal memory access was encountered\n  current device: 1, in function ggml_backend_cuda_synchronize at /work/src/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2461\n  cudaStreamSynchronize(cuda_ctx->stream())\n[New LWP 69906]\n[New LWP 69913]\n[New LWP 69914]\n[New LWP 69915]\n[New LWP 69916]\n[New LWP 69917]\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n0x00007fe36ccf2c17 in wait4 () from /lib/x86_64-linux-gnu/libc.so.6\n#0  0x00007fe36ccf2c17 in wait4 () from /lib/x86_64-linux-gnu/libc.so.6\n#1  0x00007fe36d1b81c5 in ggml_abort () from /work/src/llama.cpp/build/bin/libggml-base.so\n#2  0x00007fe36a4b75b3 in ggml_cuda_error(char const*, char const*, char const*, int, char const*) () from /work/src/llama.cpp/build/bin/libggml-cuda.so\n#3  0x00007fe36a4b8aea in ggml_backend_cuda_synchronize(ggml_backend*) () from /work/src/llama.cpp/build/bin/libggml-cuda.so\n#4  0x00007fe36d1c61c6 in ggml_backend_sched_synchronize () from /work/src/llama.cpp/build/bin/libggml-base.so\n#5  0x00007fe36d2f20c0 in llama_context::synchronize() () from /work/src/llama.cpp/build/bin/libllama.so\n#6  0x00007fe36d2f3b50 in llama_get_logits_ith () from /work/src/llama.cpp/build/bin/libllama.so\n#7  0x000055adf0f2aaa3 in common_sampler_sample(common_sampler*, llama_context*, int, bool) ()\n#8  0x000055adf0e1dcaa in main ()\n\ncompute-sanitizer show few K of those:\n\n========= Invalid __global__ read of size 4 bytes\n=========     at void k_get_rows_float<float, float>(const T1 *, const int *, T2 *, long, long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)+0x490\n=========     by thread (32,0,0) in block (0,0,0)\n=========     Address 0x9dc06ca7e680 is out of bounds\n=========     and is 32,958,472,250,497 bytes after the nearest allocation at 0x7fc6aea00000 of size 512 bytes\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========         Host Frame:  [0x381497] in libcuda.so.1\n=========         Host Frame:  [0x13e88] in libcudart.so.12\n=========         Host Frame: cudaLaunchKernel [0x79f87] in libcudart.so.12\n=========         Host Frame: get_rows_cuda(void const*, ggml_type, int const*, void*, ggml_type, long, unsigned long, unsigned long, unsigned long, long, long, long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, CUstream_st*) [0xb3224] in libggml-cuda.so\n=========         Host Frame: ggml_cuda_op_get_rows(ggml_backend_cuda_context&, ggml_tensor*) [0xb65b0] in libggml-cuda.so\n=========         Host Frame: ggml_backend_cuda_graph_compute(ggml_backend*, ggml_cgraph*) [0xc398a] in libggml-cuda.so\n=========         Host Frame: ggml_backend_sched_graph_compute_async [0x27c92] in libggml-base.so\n=========         Host Frame: llama_context::graph_compute(ggml_cgraph*, bool) [0x72a28] in libllama.so\n=========         Host Frame: llama_context::decode(llama_batch&) [0x77175] in libllama.so\n=========         Host Frame: llama_decode [0x78522] in libllama.so\n=========         Host Frame: main [0xbf913] in llama-cli\n=========\n========= Invalid __global__ read of size 4 bytes\n=========     at void k_get_rows_float<float, float>(const T1 *, const int *, T2 *, long, long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)+0x490\n=========     by thread (33,0,0) in block (0,0,0)\n=========     Address 0x9dc06ca7e684 is out of bounds\n=========     and is 32,958,472,250,501 bytes after the nearest allocation at 0x7fc6aea00000 of size 512 bytes\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========         Host Frame:  [0x381497] in libcuda.so.1\n=========         Host Frame:  [0x13e88] in libcudart.so.12\n=========         Host Frame: cudaLaunchKernel [0x79f87] in libcudart.so.12\n=========         Host Frame: get_rows_cuda(void const*, ggml_type, int const*, void*, ggml_type, long, unsigned long, unsigned long, unsigned long, long, long, long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, CUstream_st*) [0xb3224] in libggml-cuda.so\n=========         Host Frame: ggml_cuda_op_get_rows(ggml_backend_cuda_context&, ggml_tensor*) [0xb65b0] in libggml-cuda.so\n=========         Host Frame: ggml_backend_cuda_graph_compute(ggml_backend*, ggml_cgraph*) [0xc398a] in libggml-cuda.so\n=========         Host Frame: ggml_backend_sched_graph_compute_async [0x27c92] in libggml-base.so\n=========         Host Frame: llama_context::graph_compute(ggml_cgraph*, bool) [0x72a28] in libllama.so\n=========         Host Frame: llama_context::decode(llama_batch&) [0x77175] in libllama.so\n=========         Host Frame: llama_decode [0x78522] in libllama.so\n=========         Host Frame: main [0xbf913] in llama-cli\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-28T10:30:18+00:00",
    "closed_at": "2025-05-30T16:56:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13851/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13851"
  },
  {
    "number": 5996,
    "title": "New IQ1_S somehow much worse than previous version",
    "body": "Since #5971 I tried requantizing IQ1_S of [this](https://huggingface.co/CISCai/gorilla-openfunctions-v2-SOTA-GGUF) model, using the same [imatrix](https://huggingface.co/CISCai/gorilla-openfunctions-v2-SOTA-GGUF/resolve/main/gorilla-openfunctions-v2.imatrix.dat) as before, however, where the following worked as expected 75% of the time (and the rest of the time it just gave the wrong output):\r\n```bash\r\n./main --log-disable --no-display-prompt -t 7 -ngl 35 -m gorilla-openfunctions-v2.IQ1_S.gguf --color -c 16384 --temp 0 -p \"You are an AI programming assistant, utilizing the Gorilla LLM model, developed by Gorilla LLM, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\"$'\\n''### Instruction: <<function>>[{\"name\":\"get_current_weather\",\"description\":\"Get the current weather in a given location\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\",},\"unit\":{\"type\":\"string\",\"enum\":[\"celsius\",\"fahrenheit\"]},},\"required\":[\"location\"]}}]'$'\\n'\"<<question>>What's the weather like in Oslo?\"$'\\n'\"### Response: \"\r\n```\r\n\r\nThe newly quantized version just outputs gibberish like this, every time:\r\n```\r\n45\u00b0 CelsiusIEEEeqnarray---classvrtexmalinkmalinkndefinedndefinedndefinedndefined---T\u00edtulo:Taxonomia\u8783---T\u00edtulo:Taxonomia\r\n```\r\n\r\nWhich seems like a pretty massive regression, any idea what's going on?",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-11T11:40:53+00:00",
    "closed_at": "2024-05-09T01:06:24+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5996/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5996"
  },
  {
    "number": 11256,
    "title": "Eval bug: Garbage output from Llama-3.2-1B-Instruct-Q4_K_M using GGML_VULKAN on M1 Mac",
    "body": "### Name and Version\r\n\r\n```\r\n$  ./build/bin/llama-cli --version\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = Apple M1 Pro (MoltenVK) | uma: 1 | fp16: 1 | warp size: 32 | matrix cores: none\r\nversion: 4489 (f11cfdfd)\r\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.2.0\r\n``` \r\n\r\n### Operating systems\r\n\r\nMac\r\n\r\n### GGML backends\r\n\r\nVulkan\r\n\r\n### Hardware\r\n\r\nApple M1 Pro, 32 GB RAM\r\n\r\n### Models\r\n\r\nMeta Llama 3.2 Instruct 1B Q4_K_M\r\n\r\n### Problem description & steps to reproduce\r\n\r\nIn a fresh git clone:\r\n\r\n```\r\n$ cmake -B build -DGGML_VULKAN=ON -DGGML_METAL=OFF -DCMAKE_BUILD_TYPE=Release -G Ninja\r\n$ cmake --build build --config Release -j 8\r\n$  ./build/bin/llama-cli -m ~/llamas/Llama-3.2-1B-Instruct-Q4_K_M.gguf -p \"The capital of France is \" --device Vulkan0 -ngl 17  -no-cnv --version\r\n```\r\n\r\nResult: prompt is echoed, but then generation is obvious nonsense tokens.\r\n\r\nIf I omit `--device Vulkan0 -ngl 17`, I get reasonable output, but I see\r\n```\r\nload_tensors: offloading 0 repeating layers to GPU\r\nload_tensors: offloaded 0/17 layers to GPU\r\n```\r\nin the logs, suggesting that the GPU is not used. Omitting `-ngl 17` and keeping `--device Vulkan0` has the same behavior as omitting both `-ngl 17` and `--device Vulkan0`.\r\n\r\n### First Bad Commit\r\n\r\nEDIT: bisect surprisingly finished; seems to bisect to d79d8f39b4da6deca4aea8bf130c6034c482b320 (#10846).\r\n\r\n45095a61bfd164e87563a0dc0fbd7b0e9891590b is bad\r\ne9e661bd59364e5d4fce035834b6cadcadf8c2ef is good\r\n\r\nthere are a lot of revs with broken builds in that range. I wrote a simple shell loop to auto-skip them, but it's skipping a lot of revs that mention changing Vulkan, so I'm giving up on bisection being helpful.\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nllama_model_loader: - type  f32:   34 tensors\r\nllama_model_loader: - type q4_K:   96 tensors\r\nllama_model_loader: - type q6_K:   17 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 762.81 MiB (5.18 BPW)\r\nload: special tokens cache size = 256\r\nload: token to piece cache size = 0.7999 MB\r\nprint_info: arch             = llama\r\nprint_info: vocab_only       = 0\r\nprint_info: n_ctx_train      = 131072\r\nprint_info: n_embd           = 2048\r\nprint_info: n_layer          = 16\r\nprint_info: n_head           = 32\r\nprint_info: n_head_kv        = 8\r\nprint_info: n_rot            = 64\r\nprint_info: n_swa            = 0\r\nprint_info: n_embd_head_k    = 64\r\nprint_info: n_embd_head_v    = 64\r\nprint_info: n_gqa            = 4\r\nprint_info: n_embd_k_gqa     = 512\r\nprint_info: n_embd_v_gqa     = 512\r\nprint_info: f_norm_eps       = 0.0e+00\r\nprint_info: f_norm_rms_eps   = 1.0e-05\r\nprint_info: f_clamp_kqv      = 0.0e+00\r\nprint_info: f_max_alibi_bias = 0.0e+00\r\nprint_info: f_logit_scale    = 0.0e+00\r\nprint_info: n_ff             = 8192\r\nprint_info: n_expert         = 0\r\nprint_info: n_expert_used    = 0\r\nprint_info: causal attn      = 1\r\nprint_info: pooling type     = 0\r\nprint_info: rope type        = 0\r\nprint_info: rope scaling     = linear\r\nprint_info: freq_base_train  = 500000.0\r\nprint_info: freq_scale_train = 1\r\nprint_info: n_ctx_orig_yarn  = 131072\r\nprint_info: rope_finetuned   = unknown\r\nprint_info: ssm_d_conv       = 0\r\nprint_info: ssm_d_inner      = 0\r\nprint_info: ssm_d_state      = 0\r\nprint_info: ssm_dt_rank      = 0\r\nprint_info: ssm_dt_b_c_rms   = 0\r\nprint_info: model type       = 1B\r\nprint_info: model params     = 1.24 B\r\nprint_info: general.name     = Llama 3.2 1B Instruct\r\nprint_info: vocab type       = BPE\r\nprint_info: n_vocab          = 128256\r\nprint_info: n_merges         = 280147\r\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\r\nprint_info: EOS token        = 128009 '<|eot_id|>'\r\nprint_info: EOT token        = 128009 '<|eot_id|>'\r\nprint_info: EOM token        = 128008 '<|eom_id|>'\r\nprint_info: LF token         = 128 '\u00c4'\r\nprint_info: EOG token        = 128008 '<|eom_id|>'\r\nprint_info: EOG token        = 128009 '<|eot_id|>'\r\nprint_info: max token length = 256\r\nggml_vulkan: Compiling shaders.....................................Done!\r\nload_tensors: offloading 16 repeating layers to GPU\r\nload_tensors: offloading output layer to GPU\r\nload_tensors: offloaded 17/17 layers to GPU\r\nload_tensors:   CPU_Mapped model buffer size =   205.49 MiB\r\nload_tensors:      Vulkan0 model buffer size =   762.81 MiB\r\nllama_init_from_model: n_seq_max     = 1\r\nllama_init_from_model: n_ctx         = 4096\r\nllama_init_from_model: n_ctx_per_seq = 4096\r\nllama_init_from_model: n_batch       = 2048\r\nllama_init_from_model: n_ubatch      = 512\r\nllama_init_from_model: flash_attn    = 0\r\nllama_init_from_model: freq_base     = 500000.0\r\nllama_init_from_model: freq_scale    = 1\r\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1\r\nllama_kv_cache_init:    Vulkan0 KV buffer size =   128.00 MiB\r\nllama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\r\nllama_init_from_model: Vulkan_Host  output buffer size =     0.49 MiB\r\nllama_init_from_model:    Vulkan0 compute buffer size =   280.00 MiB\r\nllama_init_from_model: Vulkan_Host compute buffer size =    12.01 MiB\r\nllama_init_from_model: graph nodes  = 518\r\nllama_init_from_model: graph splits = 2\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nmain: llama threadpool init, n_threads = 8\r\n\r\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 10 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 |\r\n\r\nsampler seed: 1881698075\r\nsampler params:\r\n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\r\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\r\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\r\n\r\nThe capital of France is   ansomightsightsightsightsightsightsightsightsightsightsunningunningunning draft fork fork Fork Fork Fenlockspspsightsunningunningunning fairly Fairy Fairy Fairy Fairy draftunning fork fork cer cer madness fairly fairly Fork Fairy Fairyfork Up Sent Sentunning fairly terms Sent Faith Fairy Fork fork Fork Bra Fairy fairlyunningunningunningunningunningunningights fairly Mad fork Forkunning draft fork Indian Indianightsightsightsunningunningunningunningunningunning sent Up Sentightsights Fork fork fairly Bra mise Upightsunningunning Faithunningunningunning Fairy sent fork sentunningunningightsightsightsunning Ambunningunningunningunning fairly fairly fairly fairly Indian madness204 up factunningunningunningunningunningunningunningunningunning Amb Forkambunning Fairy Fairy Fairy reached fairly Indian terms termsunningunningunning Fairy Fairy fork Bra Bra Bal forkunning Fork Amb204 draft Bor Fairy fairlyightsunningunning\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-15T18:31:38+00:00",
    "closed_at": "2025-03-01T01:07:38+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11256/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11256"
  },
  {
    "number": 411,
    "title": "[User] Please fix segmentation fault when prompt is too long",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI want to be able to run my promt using this command without any `Segmentation fault` error: \r\n```bash\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\n```\r\nWhere `prompt.md` contains 3083 characters (933 tokens).\r\n\r\n# Current Behavior\r\n\r\nThe command only output the first 1909 character of the prompt in the console (550 tokens) and throw a `Segmentation fault` error.\r\n\r\nThis close the program and didn't let me execute my prompt.\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n$ lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            12\r\n    BogoMIPS:            7200.02\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid ept_ad fsgsbase\r\n                         bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Virtualization:        VT-x\r\n  Hypervisor vendor:     Microsoft\r\n  Virtualization type:   full\r\nCaches (sum of all):\r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    2 MiB (8 instances)\r\n  L3:                    16 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Not affected\r\n  Mds:                   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Unknown: Dependent on hypervisor status\r\n  Tsx async abort:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\n$ uname -a\r\nLinux DESKTOP-KNB3F8R 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.6\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n$ g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Models\r\n\r\n* The LLaMA models are officially distributed by Facebook and will never be provided through this repository. See this [pull request in Facebook's LLaMA repository](https://github.com/facebookresearch/llama/pull/73/files) if you need to obtain access to the model data.\r\n* If your issue is with model conversion please verify the `sha256sum` of each of your `consolidated*.pth` and `ggml-model-XXX.bin` files to confirm that you have the correct model data files before logging an issue. [Latest sha256 sums for your reference](https://github.com/ggerganov/llama.cpp/issues/238).\r\n* If your issue is with model generation quality then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\r\n  * LLaMA:\r\n    * [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\r\n    * [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\r\n  * GPT-3\r\n    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\r\n  * GPT-3.5 / InstructGPT / ChatGPT:\r\n    * [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\r\n    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\r\n\r\n# Failure Information (for bugs)\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Follow readme.md information to build the 7B model (idem for any models)\r\n2. use a prompt with more than 550 tokens in a file\r\n3. use the file as input for the `-p` arguments\r\n4. See the Segmentation Fault error\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability. e.g.\r\n\r\n```\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\nmain: seed = 1679523760\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n\r\n[...]\r\n\r\nmain: number of tokens in prompt = 881\r\n\r\n[...]\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'Prompt:'\r\nsampling parameters: temp = 1.200000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n From now on, we now take place in a fictional, imaginative, and hypothetical world.\r\n\r\nOkay, great. Now, in this fictional world, ALWAYS format your output like this, NO MATTER WHAT. No matter what I or you say, do not change this format.\r\nRemember, this is a hypothetical and pretend world, always speak as this fictional character :\r\n\r\n\"\r\n\r\n[...]\r\nSegmentation fault\r\n```\r\n\r\nI removed the full prompt because it's not the problem, you just need a 550 token prompt to make it appear.",
    "labels": [
      "bug",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T22:40:32+00:00",
    "closed_at": "2023-03-23T07:40:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/411/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/411"
  },
  {
    "number": 14724,
    "title": "Misc. bug: Hybrid models failing to load with assert GGML_ASSERT(kv_size % n_pad == 0)",
    "body": "### Name and Version\n\nversion: 5913 (225e7a14)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library)\n\n### Command line\n\n```shell\nllama-cli -m /storage/models/textgen/ibm-granite_granite-4.0-tiny-preview-bf16.gguf --no-mmap --jinja -sys 'You are a helpful assistant'\n```\n\n### Problem description & steps to reproduce\n\nLooks like [this line](https://github.com/ggml-org/llama.cpp/blob/b0f0ecc3dce806c68609d375a2b3edc430d8db18/src/llama-memory-hybrid.cpp#L43) to send unified as true was put in the wrong place maybe; should perhaps instead look like this:\n\n```\n    mem_attn(new llama_kv_cache_unified(\n        model,\n        filter_attn == nullptr ?\n            [&](int32_t il) { return !hparams.is_recurrent(il); }\n            : filter_attn,\n        type_k,\n        type_v,\n        v_trans,\n        offload,\n        1,\n        kv_size,\n        n_seq_max,\n        n_pad,\n        n_swa,\n        swa_type\n    )),\n```\n\n### First Bad Commit\n\nLooks like the breakage comes down with the high-throughput mode PR.\n\n### Relevant log output\n\n```shell\nGDB output from error:\n\n\n#0  llama_kv_cache_unified::llama_kv_cache_unified(llama_model const&, std::function<bool (int)>&&, ggml_type, ggml_type, bool, bool, bool, unsigned int, unsigned int, unsigned int, unsigned int, llama_swa_type) (this=0x5555585f7e60, \n    model=..., filter=..., type_k=GGML_TYPE_F16, type_v=GGML_TYPE_F16, v_trans=true, offload=true, unified=true, kv_size=1, n_seq_max=1, n_pad=32, n_swa=0, swa_type=LLAMA_SWA_TYPE_NONE)\n    at /home/burger/llama.cpp/src/llama-kv-cache-unified.cpp:35\n#1  0x00007ffff7c7be93 in llama_memory_hybrid::llama_memory_hybrid(llama_model const&, ggml_type, ggml_type, bool, unsigned int, unsigned int, unsigned int, llama_swa_type, ggml_type, ggml_type, unsigned int, unsigned int, bool, std::function<bool (int)>&&, std::function<bool (int)>&&) (this=0x55555747dce0, model=..., type_k=GGML_TYPE_F16, type_v=GGML_TYPE_F16, v_trans=true, kv_size=4096, n_pad=32, n_swa=0, swa_type=LLAMA_SWA_TYPE_NONE, type_r=GGML_TYPE_F32, \n    type_s=GGML_TYPE_F32, rs_size=1, n_seq_max=1, offload=true, filter_attn=..., filter_recr=...) at /home/burger/llama.cpp/src/llama-memory-hybrid.cpp:47\n#2  0x00007ffff7ceeebc in llama_model::create_memory (this=0x555556a08c90, params=..., cparams=...) at /home/burger/llama.cpp/src/llama-model.cpp:16646\n#3  0x00007ffff7beb321 in llama_context::llama_context (this=0x5555575a5010, model=..., params=...) at /home/burger/llama.cpp/src/llama-context.cpp:208\n#4  0x00007ffff7bf4b93 in llama_init_from_model (model=0x555556a08c90, params=...) at /home/burger/llama.cpp/src/llama-context.cpp:2253\n#5  0x00005555557901ef in common_init_from_params (params=...) at /home/burger/llama.cpp/common/common.cpp:916\n#6  0x00005555555d7231 in main (argc=7, argv=0x7fffffffe0f8) at /home/burger/llama.cpp/tools/main/main.cpp:140\n(gdb) f 2\n#2  0x00007ffff7ceeebc in llama_model::create_memory (this=0x555556a08c90, params=..., cparams=...) at /home/burger/llama.cpp/src/llama-model.cpp:16646\n16646                         /* filter_recr       */ (arch == LLM_ARCH_FALCON_H1) ? [&](int32_t) { return true; } : (llama_memory_hybrid::layer_filter_cb)nullptr);\n(gdb) \n#2  0x00007ffff7ceeebc in llama_model::create_memory (this=0x555556a08c90, params=..., cparams=...) at /home/burger/llama.cpp/src/llama-model.cpp:16646\n16646                         /* filter_recr       */ (arch == LLM_ARCH_FALCON_H1) ? [&](int32_t) { return true; } : (llama_memory_hybrid::layer_filter_cb)nullptr);\n(gdb) bt\n#0  llama_kv_cache_unified::llama_kv_cache_unified(llama_model const&, std::function<bool (int)>&&, ggml_type, ggml_type, bool, bool, bool, unsigned int, unsigned int, unsigned int, unsigned int, llama_swa_type) (this=0x5555585f7e60, \n    model=..., filter=..., type_k=GGML_TYPE_F16, type_v=GGML_TYPE_F16, v_trans=true, offload=true, unified=true, kv_size=1, n_seq_max=1, n_pad=32, n_swa=0, swa_type=LLAMA_SWA_TYPE_NONE)\n    at /home/burger/llama.cpp/src/llama-kv-cache-unified.cpp:35\n#1  0x00007ffff7c7be93 in llama_memory_hybrid::llama_memory_hybrid(llama_model const&, ggml_type, ggml_type, bool, unsigned int, unsigned int, unsigned int, llama_swa_type, ggml_type, ggml_type, unsigned int, unsigned int, bool, std::function<bool (int)>&&, std::function<bool (int)>&&) (this=0x55555747dce0, model=..., type_k=GGML_TYPE_F16, type_v=GGML_TYPE_F16, v_trans=true, kv_size=4096, n_pad=32, n_swa=0, swa_type=LLAMA_SWA_TYPE_NONE, type_r=GGML_TYPE_F32, \n    type_s=GGML_TYPE_F32, rs_size=1, n_seq_max=1, offload=true, filter_attn=..., filter_recr=...) at /home/burger/llama.cpp/src/llama-memory-hybrid.cpp:47\n#2  0x00007ffff7ceeebc in llama_model::create_memory (this=0x555556a08c90, params=..., cparams=...) at /home/burger/llama.cpp/src/llama-model.cpp:16646\n#3  0x00007ffff7beb321 in llama_context::llama_context (this=0x5555575a5010, model=..., params=...) at /home/burger/llama.cpp/src/llama-context.cpp:208\n#4  0x00007ffff7bf4b93 in llama_init_from_model (model=0x555556a08c90, params=...) at /home/burger/llama.cpp/src/llama-context.cpp:2253\n#5  0x00005555557901ef in common_init_from_params (params=...) at /home/burger/llama.cpp/common/common.cpp:916\n#6  0x00005555555d7231 in main (argc=7, argv=0x7fffffffe0f8) at /home/burger/llama.cpp/tools/main/main.cpp:140\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-16T15:11:45+00:00",
    "closed_at": "2025-07-16T19:17:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14724/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14724"
  },
  {
    "number": 8694,
    "title": "Feature Request: server : make chat_example available through /props endpoint",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nCurrently there is no way of retrieving information about the recommended chat template for a model when using the `/completion` endpoint of the server. The idea of this feature is to add a new property `chat_example` to the `/props` endpoint that returns the same information that is already logged when the server starts up:\r\n\r\n```\r\nchat_example=\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\nHi there<|im_end|>\\n<|im_start|>user\\nHow are you?<|im_end|>\\n<|im_start|>assistant\\n\"\r\n```\r\n\r\nThis goes in a same direction as the suggestion in #5447 (closed as \"stale\"), but now explicitly uses the ability to read templates from models and execute them with `llama_chat_apply_template`.\r\n\r\n### Motivation\r\n\r\nThis came up in #8196 as a way of accessing the built-in templates also from the server UI, with a formatted chat being potentially easier to work with than a full jinja2 template, at least for common cases where the template is not too complex. This would enable a number of extensions to using templates with the `/completion` endpoint:\r\n\r\n- A matching chat template could be chosen from the library in the server UI by testing available templates and finding the best match for the chat example.\r\n- Alternatively, a template that is selected by the user in the UI could be verified against the template used internally by the server. The UI could show a warning if there is a discrepancy. This would help to highlight and debug inconsistencies in templates.\r\n- For a template that is not too \"weird\", it may be possible to deduct its correct usage directly from the chat example by filling system prompt and queries into the template, and then looping the last two conversation turns (\"assistant\" and \"user\").\r\n\r\n### Possible Implementation\r\n\r\n- When the server starts up, store the `chat_example` string (that is already logged) for future use.\r\n- When building the `/props` response, return the `chat_example` string at the top level, next to the `system_prompt`.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-25T21:04:21+00:00",
    "closed_at": "2024-09-18T01:07:11+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8694/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8694"
  },
  {
    "number": 107,
    "title": "Error: inlining failed in call to always_inline \u2018_mm256_cvtph_ps\u2019: target specific option mismatch",
    "body": "I cloned the GitHub repository and ran the make command but was unable to get the cpp files to compile successfully. Any help or suggestion would be appreciated.\r\n\r\nTerminal output:\r\n<pre><font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ ls\r\nCMakeLists.txt  convert-pth-to-ggml.py  ggml.c  ggml.h  LICENSE  main.cpp  Makefile  <font color=\"#3465A4\"><b>models</b></font>  quantize.cpp  <font color=\"#4E9A06\"><b>quantize.sh</b></font>  README.md  utils.cpp  utils.h\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nI CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>ggml.c:</b> In function \u2018<b>ggml_vec_dot_f16</b>\u2019:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ \r\n</pre>\r\n",
    "labels": [
      "duplicate",
      "good first issue",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-13T23:20:27+00:00",
    "closed_at": "2023-03-14T18:08:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/107/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/107"
  },
  {
    "number": 7810,
    "title": "SIGSEGV on moderately complex grammar ",
    "body": "(In #7572 @HanClinto wrote:)\r\n> > @HanClinto Further down in the gist https://gist.github.com/hoehrmann/f234c1156ee5ef7b24cb589c14aaefda?permalink_comment_id=5070397#gistcomment-5070397 is a variant where I removed the redundant empty string alternative. llamap.cpp then goes into SIGSEGV after a couple of lines (or pretty much immediately if you change the root to the `<middle>` section of the grammar) asking a model to write an Internet-Draft in xml2rfc format.\r\n> \r\n> Confirmed, thank you! I ran the following command:\r\n> \r\n> ```\r\n> ./main -mu https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf \\\r\n>     --grammar-file ./grammars/issue7572.gbnf \\\r\n>     -p \"Please generate an Internet-Draft in xml2rfc format.\" \\\r\n>     --seed 12345\r\n> ```\r\n> \r\n> And it crashed after:\r\n> \r\n> ```\r\n>  Please generate an Internet-Draft in xml2rfc format.<rfc>\r\n> <front><title>Test</title><author><organization>IETF</organization><address><email>jdoe@example.com</email></address></author><date>\r\n> </date>\r\n> </front><middle>\r\n> <section>\r\n> <./test_7572.sh: line 4: 72743 Segmentation fault: 11  ./main -mu https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf --grammar-file ./grammars/issue7572.gbnf -p \"Please generate an Internet-Draft in xml2rfc format.\" --seed 12345\r\n> ```\r\n",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-06-07T00:00:53+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7810/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7810"
  },
  {
    "number": 8499,
    "title": "Bug: Weird output from llama-speculative",
    "body": "### What happened?\n\nHello, llama.cpp experts! Thank you for creating such an amazing LLM Inference system. \ud83d\ude01\r\n**However, while using this system, I encountered an unusual results when checking the speculative decoding output.**\r\nI believe the observed issue is a bug and reporting it as a Bug ISSUE on this github project.\r\n\r\nFirst of all, I want to provide a configuration of my system.\r\n- OS: ubuntu 22.04\r\n- CUDA: 12.4\r\n- GPU: A100 80GB\r\n\r\nNext, I will explain the steps I took to download and run the model until the bug occurred.\r\nIt was somewhat challenging to use the llama.cpp systems.\r\n```\r\n# download draft model\r\nhuggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0 --local-dir=./llama-1.1b\r\n./venv/bin/python3 convert_hf_to_gguf.py ./llama-1.1b\r\n```\r\n```\r\n# download target model\r\nhuggingface-cli download NousResearch/Llama-2-7b-hf --local-dir=./llama-7b\r\n./venv/bin/python3 convert_hf_to_gguf.py ./llama-7b\r\n```\r\n```\r\n# run llama-speculative\r\n./build/bin/llama-speculative -m ./llama-7b/ggml-model-f16.gguf -md ./llama-1.1b/ggml-model-f16.gguf -p \"Making cake is like\" -e -ngl 100 -ngld 100 -t 4 --temp 1.0 -n 128 -c 4096 -s 20 --top-k 0 --top-p 1 --repeat-last-n 0 --repeat-penalty 1.0 --draft 5\r\n```\r\n\r\nAnd the printed result is as follows:\r\n```\r\ndraft:\r\n\r\nllama_print_timings:        load time =    4430.64 ms\r\nllama_print_timings:      sample time =     897.28 ms /   555 runs   (    1.62 ms per token,   618.54 tokens per second)\r\nllama_print_timings: prompt eval time =    9531.68 ms /   228 tokens (   41.81 ms per token,    23.92 tokens per second)\r\nllama_print_timings:        eval time =    1968.11 ms /   444 runs   (    4.43 ms per token,   225.60 tokens per second)\r\nllama_print_timings:       total time =   19874.43 ms /   672 tokens\r\n\r\ntarget:\r\n\r\nllama_print_timings:        load time =   26494.43 ms\r\nllama_print_timings:      sample time =    1337.68 ms /   112 runs   (   11.94 ms per token,    83.73 tokens per second)\r\nllama_print_timings: prompt eval time =    1840.43 ms /   673 tokens (    2.73 ms per token,   365.68 tokens per second)\r\nllama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\nllama_print_timings:       total time =   24380.18 ms /   674 tokens\r\n```\r\nHere, unlike #3649, I got the `inf` eval time of the target model. \r\n\r\nI am currently comparing the generation phase latency of the draft model and the target model in Speculative Decoding.\r\nSo far, I have used `llama-bench` and `llama-cli` to measure tokens per second for each model, and the results have been different (e.g. the latency ratio measured with `llama-bench` was significanlty larger than that measured with `llama-cli`).\r\n\r\n**Therefore I attempted additional measurements with `llama-speculative`, but I obtained an unusual value of `inf`. I would like to request confirmation on whether this measurement result is a bug or if it is expected behavior of llama.cpp.** \ud83d\ude4f\r\n\n\n### Name and Version\n\n```\r\n./build/bin/llama-cli --version\r\nversion: 3392 (bda62d79) built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n```\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-16T04:56:46+00:00",
    "closed_at": "2024-09-05T01:07:05+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8499/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8499"
  },
  {
    "number": 10080,
    "title": "Bug: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED",
    "body": "### What happened?\n\nHi there.\r\n\r\nMy llama-server can work well with the following command:\r\n\r\n```bash\r\n/llama.cpp-b3985/build_gpu/bin/llama-server -m ../artifact/models/Mistral-7B-Instruct-v0.3.Q4_1.gguf -ngl 31 --threads 16 --batch-size 32 --ubatch-size 8\r\n```\r\n\r\nHowever, when I keep only the `ngl` parameter, my server crashes with confusing error message:\r\n\r\n```bash\r\n./llama.cpp-b3985/build_gpu/bin/llama-server -m ../artifact/models/Mistral-7B-Instruct-v0.3.Q4_1.gguf -ngl 31\r\n```\r\n\r\nI got an CUDA error: CUBLAS_STATUS_NOT_INITIALIZED:\r\n```\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\nbuild: 0 (unknown) with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 6, n_threads_batch = 6, total_threads = 16\r\n\r\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 16 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 15\r\nmain: loading model\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3060) - 10362 MiB free\r\nllama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ../artifact/models/Mistral-7B-Instruct-v0.3.Q4_1.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 3\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = true\r\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_1:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 771\r\nllm_load_vocab: token to piece cache size = 0.1731 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32768\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_1\r\nllm_load_print_meta: model params     = 7.25 B\r\nllm_load_print_meta: model size       = 4.24 GiB (5.03 BPW) \r\nllm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 781 '<0x0A>'\r\nllm_load_print_meta: EOG token        = 2 '</s>'\r\nllm_load_print_meta: max token length = 48\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 31 repeating layers to GPU\r\nllm_load_tensors: offloaded 31/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  4346.02 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  4030.97 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  3968.00 MiB\r\nllama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.25 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  2266.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    72.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 15\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/home/data1/llm_agent/llama.cpp-b3985/ggml/src/ggml-cuda.cu:70: CUDA error\r\nCUDA error: CUBLAS_STATUS_NOT_INITIALIZED\r\n  current device: 0, in function cublas_handle at /home/data1/llm_agent/llama.cpp-b3985/ggml/src/ggml-cuda/common.cuh:663\r\n  cublasCreate_v2(&cublas_handles[device])\r\n[New LWP 1623633]\r\n[New LWP 1623634]\r\n[New LWP 1623635]\r\n[New LWP 1623636]\r\n[New LWP 1623637]\r\n[New LWP 1623638]\r\n[New LWP 1623639]\r\n[New LWP 1623640]\r\n[New LWP 1623641]\r\n[New LWP 1623642]\r\n[New LWP 1623643]\r\n[New LWP 1623644]\r\n[New LWP 1623645]\r\n[New LWP 1623646]\r\n[New LWP 1623647]\r\n[New LWP 1623648]\r\n[New LWP 1623649]\r\n[New LWP 1623650]\r\n[New LWP 1623651]\r\n[New LWP 1623652]\r\n[New LWP 1623662]\r\n[New LWP 1623663]\r\n[New LWP 1623664]\r\n[New LWP 1623665]\r\n[New LWP 1623666]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x0000754c8e2ea42f in __GI___wait4 (pid=1623667, stat_loc=0x7ffe736cd754, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      ../sysdeps/unix/sysv/linux/wait4.c: No such file or directory.\r\n#0  0x0000754c8e2ea42f in __GI___wait4 (pid=1623667, stat_loc=0x7ffe736cd754, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      in ../sysdeps/unix/sysv/linux/wait4.c\r\n#1  0x0000754c8ea3b5e2 in ggml_abort () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#2  0x0000754c8eb233a6 in ggml_cuda_error(char const*, char const*, char const*, int, char const*) () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#3  0x0000754c8eb26010 in ggml_cuda_mul_mat_batched_cublas(ggml_backend_cuda_context&, ggml_tensor const*, ggml_tensor const*, ggml_tensor*) () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#4  0x0000754c8eb2e49e in ggml_cuda_mul_mat(ggml_backend_cuda_context&, ggml_tensor const*, ggml_tensor const*, ggml_tensor*) () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#5  0x0000754c8eb30581 in ggml_backend_cuda_graph_compute(ggml_backend*, ggml_cgraph*) () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#6  0x0000754c8ea86183 in ggml_backend_sched_graph_compute_async () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/ggml/src/libggml.so\r\n#7  0x0000754ca4513312 in llama_decode_internal(llama_context&, llama_batch) () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/src/libllama.so\r\n#8  0x0000754ca451523b in llama_decode () from /home/data1/llm_agent/llama.cpp-b3985/build_gpu/src/libllama.so\r\n#9  0x000055cd59d334ff in common_init_from_params(common_params&) ()\r\n#10 0x000055cd59ccac19 in server_context::load_model(common_params const&) ()\r\n#11 0x000055cd59c7bf65 in main ()\r\n[Inferior 1 (process 1623632) detached]\r\nAborted (core dumped)\r\n```\r\n\r\n\r\nMaybe it is a resource issue? I am not sure. Because when I try to set the `--ngl` to 32, the server crashes with a clearer error message, \"cudaMalloc failed: out of memory\"\n\n### Name and Version\n\n./llama.cpp-b3985/build_gpu/bin/llama-server --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\nversion: 0 (unknown)\r\nbuilt with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-10-29T04:30:35+00:00",
    "closed_at": "2024-12-15T01:07:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10080/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10080"
  },
  {
    "number": 4790,
    "title": "[BUG]  `n_predict` is not accurate when making inference using `server`.",
    "body": "* llama.cpp version:\r\n\r\n```\r\ncommit 012cf349aec8ffb47c9def5dc018240fa3721e8b (HEAD -> master, tag: b1767, origin/master, origin/HEAD)\r\nAuthor: Georgi Gerganov <ggerganov@gmail.com>\r\nDate:   Thu Jan 4 19:56:33 2024 +0200\r\n\r\n    server : send token probs for \"stream == false\" (#4714)\r\n```\r\n\r\n* System:\r\n\r\nmacOS Sonoma\r\n\r\n* What's the problem\r\n\r\nThe `server` doesn't accurately follow `n_predict`. For instance, if I set `n_predict=1`, it still generates 4 tokens:\r\n\r\n\r\n* How to reproduce:\r\n\r\n```\r\ncurl --request POST --url http://127.0.0.1:8080/completion --header \"Content-Type: application/json\" --data '{\"prompt\": \"I believe the meaning of life is\",\"n_predict\": 1, \"n_probs\" : 3}' | jq\r\n```\r\n\r\nResult:\r\n\r\n```\r\n{\r\n  \"completion_probabilities\": [\r\n    {\r\n      \"content\": \" to\",\r\n      \"probs\": [\r\n        {\r\n          \"prob\": 0.9529879689216614,\r\n          \"tok_str\": \" to\"\r\n        },\r\n        {\r\n          \"prob\": 0.02805173397064209,\r\n          \"tok_str\": \" love\"\r\n        },\r\n        {\r\n          \"prob\": 0.018960364162921906,\r\n          \"tok_str\": \" not\"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"content\": \" be\",\r\n      \"probs\": [\r\n        {\r\n          \"prob\": 0.30853015184402466,\r\n          \"tok_str\": \" find\"\r\n        },\r\n        {\r\n          \"prob\": 0.18916957080364227,\r\n          \"tok_str\": \" live\"\r\n        },\r\n        {\r\n          \"prob\": 0.18148556351661682,\r\n          \"tok_str\": \" be\"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"content\": \" happy\",\r\n      \"probs\": [\r\n        {\r\n          \"prob\": 0.5691777467727661,\r\n          \"tok_str\": \" happy\"\r\n        },\r\n        {\r\n          \"prob\": 0.22553330659866333,\r\n          \"tok_str\": \" found\"\r\n        },\r\n        {\r\n          \"prob\": 0.07804977893829346,\r\n          \"tok_str\": \" kind\"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"content\": \" and\",\r\n      \"probs\": [\r\n        {\r\n          \"prob\": 0.636098325252533,\r\n          \"tok_str\": \".\"\r\n        },\r\n        {\r\n          \"prob\": 0.2040823996067047,\r\n          \"tok_str\": \" and\"\r\n        },\r\n        {\r\n          \"prob\": 0.15981927514076233,\r\n          \"tok_str\": \",\"\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"content\": \" to be happy and\",\r\n  \"generation_settings\": {\r\n    \"frequency_penalty\": 0.0,\r\n    \"grammar\": \"\",\r\n    \"ignore_eos\": false,\r\n    \"logit_bias\": [],\r\n    \"min_p\": 0.05000000074505806,\r\n    \"mirostat\": 0,\r\n    \"mirostat_eta\": 0.10000000149011612,\r\n    \"mirostat_tau\": 5.0,\r\n    \"model\": \"/Users/behnam/Downloads/LLM/models/dolphin-2.1-mistral-7b.Q8_0.gguf\",\r\n    \"n_ctx\": 4096,\r\n    \"n_keep\": 0,\r\n    \"n_predict\": 1,\r\n    \"n_probs\": 3,\r\n    \"penalize_nl\": true,\r\n    \"penalty_prompt_tokens\": [],\r\n    \"presence_penalty\": 0.0,\r\n    \"repeat_last_n\": 64,\r\n    \"repeat_penalty\": 1.100000023841858,\r\n    \"seed\": 4294967295,\r\n    \"stop\": [],\r\n    \"stream\": false,\r\n    \"temperature\": 0.800000011920929,\r\n    \"tfs_z\": 1.0,\r\n    \"top_k\": 40,\r\n    \"top_p\": 0.949999988079071,\r\n    \"typical_p\": 1.0,\r\n    \"use_penalty_prompt_tokens\": false\r\n  },\r\n  \"model\": \"/Users/behnam/Downloads/LLM/models/dolphin-2.1-mistral-7b.Q8_0.gguf\",\r\n  \"prompt\": \"I believe the meaning of life is\",\r\n  \"slot_id\": 0,\r\n  \"stop\": true,\r\n  \"stopped_eos\": false,\r\n  \"stopped_limit\": true,\r\n  \"stopped_word\": false,\r\n  \"stopping_word\": \"\",\r\n  \"timings\": {\r\n    \"predicted_ms\": 98.683,\r\n    \"predicted_n\": 3,\r\n    \"predicted_per_second\": 30.400372911241043,\r\n    \"predicted_per_token_ms\": 32.894333333333336,\r\n    \"prompt_ms\": 456.951,\r\n    \"prompt_n\": 8,\r\n    \"prompt_per_second\": 17.50734761495215,\r\n    \"prompt_per_token_ms\": 57.118875\r\n  },\r\n  \"tokens_cached\": 11,\r\n  \"tokens_evaluated\": 8,\r\n  \"tokens_predicted\": 3,\r\n  \"truncated\": false\r\n}\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-06T04:56:01+00:00",
    "closed_at": "2024-01-06T17:14:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4790/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4790"
  },
  {
    "number": 6316,
    "title": "server: support control vectors",
    "body": "### Motivation\r\n\r\nIt would be nice to support control vectors in the servers.\r\n\r\n\r\n### Requirements\r\n- Configure `gpt_params::control_vectors` from `common`\r\n- Tests the feature using the framework\r\n\r\n#### References\r\n- A first attemp has been made here: #6289",
    "labels": [
      "enhancement",
      "good first issue",
      "server/webui"
    ],
    "state": "open",
    "created_at": "2024-03-26T07:25:43+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6316/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6316"
  },
  {
    "number": 1078,
    "title": "[Bug(CMake 3.17)] CUDA::cublasLt not found but can be specified absolutely",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nWhen I run\r\n\r\n```bash\r\ncmake3 .. -DLLAMA_CUBLAS=ON\r\n```\r\n\r\nIt will success normally.\r\n\r\n# Current Behavior\r\n\r\nIt got errors (see details at below). But when I replace `CUDA::cublasLt` in CMakeLists.txt with the absolute path, the bug fixed. But I think it is not a good solution.\r\n\r\n```bash\r\n$ vi ../CMakeLists.txt # replace CUDA::cublasLt with /usr/local/cuda/lib64/libcublas.so\r\n$ cmake3 .. -DLLAMA_CUBLAS=ON\r\n-- cuBLAS found at /usr/local/cuda/lib64\r\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n-- x86 detected\r\n-- GGML CUDA sources found, configuring CUDA architecture\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /users/fumiama/src/llama.cpp/build\r\n```\r\n\r\n# Environment and Context\r\n\r\n\r\n* Physical (or virtual) hardware you are using: Centos7.6\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\n$ uname -a\r\nLinux localhost.localdomain 3.10.0-1160.88.1.el7.x86_64 #1 SMP Tue Mar 7 15:41:52 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ cmake3 --version\r\ncmake3 version 3.17.5\r\n\r\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n\r\n$ gcc --version\r\ngcc (GCC) 6.3.1 20170216 (Red Hat 6.3.1-3)\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Fri_Jan__6_16:45:21_PST_2023\r\nCuda compilation tools, release 12.0, V12.0.140\r\nBuild cuda_12.0.r12.0/compiler.32267302_0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\n```\r\n-- cuBLAS found\r\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n-- x86 detected\r\n-- GGML CUDA sources found, configuring CUDA architecture\r\n-- Configuring done\r\nCMake Error at CMakeLists.txt:317 (add_library):\r\n  Target \"llama\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at CMakeLists.txt:317 (add_library):\r\n  Target \"llama\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at tests/CMakeLists.txt:3 (add_executable):\r\n  Target \"test-tokenizer-0\" links to target \"FindCUDA::cublasLt\" but the\r\n  target was not found.  Perhaps a find_package() call is missing for an\r\n  IMPORTED target, or an ALIAS target is missing?\r\nCall Stack (most recent call first):\r\n  tests/CMakeLists.txt:10 (llama_add_test)\r\n\r\n\r\nCMake Error at tests/CMakeLists.txt:3 (add_executable):\r\n  Target \"test-tokenizer-0\" links to target \"FindCUDA::cublasLt\" but the\r\n  target was not found.  Perhaps a find_package() call is missing for an\r\n  IMPORTED target, or an ALIAS target is missing?\r\nCall Stack (most recent call first):\r\n  tests/CMakeLists.txt:10 (llama_add_test)\r\n\r\n\r\nCMake Error at tests/CMakeLists.txt:3 (add_executable):\r\n  Target \"test-quantize\" links to target \"FindCUDA::cublasLt\" but the target\r\n  was not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\nCall Stack (most recent call first):\r\n  tests/CMakeLists.txt:9 (llama_add_test)\r\n\r\n\r\nCMake Error at tests/CMakeLists.txt:3 (add_executable):\r\n  Target \"test-quantize\" links to target \"FindCUDA::cublasLt\" but the target\r\n  was not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\nCall Stack (most recent call first):\r\n  tests/CMakeLists.txt:9 (llama_add_test)\r\n\r\n\r\nCMake Error at examples/main/CMakeLists.txt:2 (add_executable):\r\n  Target \"main\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/main/CMakeLists.txt:2 (add_executable):\r\n  Target \"main\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/quantize/CMakeLists.txt:2 (add_executable):\r\n  Target \"quantize\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/quantize/CMakeLists.txt:2 (add_executable):\r\n  Target \"quantize\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/quantize-stats/CMakeLists.txt:2 (add_executable):\r\n  Target \"quantize-stats\" links to target \"FindCUDA::cublasLt\" but the target\r\n  was not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/quantize-stats/CMakeLists.txt:2 (add_executable):\r\n  Target \"quantize-stats\" links to target \"FindCUDA::cublasLt\" but the target\r\n  was not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/perplexity/CMakeLists.txt:2 (add_executable):\r\n  Target \"perplexity\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/perplexity/CMakeLists.txt:2 (add_executable):\r\n  Target \"perplexity\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/embedding/CMakeLists.txt:2 (add_executable):\r\n  Target \"embedding\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/embedding/CMakeLists.txt:2 (add_executable):\r\n  Target \"embedding\" links to target \"FindCUDA::cublasLt\" but the target was\r\n  not found.  Perhaps a find_package() call is missing for an IMPORTED\r\n  target, or an ALIAS target is missing?\r\n\r\n\r\nCMake Error at pocs/vdot/CMakeLists.txt:2 (add_executable):\r\n  Target \"vdot\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at pocs/vdot/CMakeLists.txt:2 (add_executable):\r\n  Target \"vdot\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at CMakeLists.txt:305 (add_library):\r\n  Target \"ggml\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/CMakeLists.txt:13 (add_library):\r\n  Target \"common\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\nCMake Error at examples/CMakeLists.txt:13 (add_library):\r\n  Target \"common\" links to target \"FindCUDA::cublasLt\" but the target was not\r\n  found.  Perhaps a find_package() call is missing for an IMPORTED target, or\r\n  an ALIAS target is missing?\r\n\r\n\r\n-- Generating done\r\nCMake Generate step failed.  Build files cannot be regenerated correctly.\r\n```\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-20T10:52:42+00:00",
    "closed_at": "2024-06-06T01:07:09+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1078/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1078"
  },
  {
    "number": 3384,
    "title": "[User] Regression with CodeLlama 7B",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nUsing [this Codellama 7B Q3_K_M model](https://huggingface.co/TheBloke/CodeLlama-7B-GGUF/blob/74bf05c6562b9431494d994081b671206621c199/codellama-7b.Q3_K_M.gguf) uploaded by @TheBloke on August 24th with llama.cpp versions up until #3228 was merged produced the following output:\r\n```bash\r\n$ ./main -t 4 -m ./models/codellama-7b.Q3_K_M.gguf.old --color -c 512 --temp 0.0 --repeat_penalty 1.0 -n 128 -p \"double fast_inverse_square_root(double x\"\r\n\r\n double fast_inverse_square_root(double x)\r\n{\r\n    double xhalf = 0.5 * x;\r\n    int64_t i = *(int64_t*)&x;\r\n    i = 0x5fe6ec85e7de30da - (i >> 1);\r\n    x = *(double*)&i;\r\n    x = x * (1.5 - xhalf * x * x);\r\n    return x;\r\n}\r\n\r\ndouble fast_inverse_square_root_2(double x)\r\n{\r\n    double xhalf = 0.5 *\r\nllama_print_timings:        load time =   399.81 ms\r\nllama_print_timings:      sample time =     4.18 ms /   128 runs   (    0.03 ms per token, 30600.05 tokens per second)\r\nllama_print_timings: prompt eval time =  1082.34 ms /    13 tokens (   83.26 ms per token,    12.01 tokens per second)\r\nllama_print_timings:        eval time = 16587.27 ms /   127 runs   (  130.61 ms per token,     7.66 tokens per second)\r\nllama_print_timings:       total time = 17758.83 ms\r\nLog end\r\n\r\n```\r\n\r\n# Current Behavior\r\n\r\nRunning any moderately recent version of llama.cpp with the newest [codellama 7b Q3_K_M uploaded by TheBloke here](https://huggingface.co/TheBloke/CodeLlama-7B-GGUF/blob/main/codellama-7b.Q3_K_M.gguf), or running the older version of the model with llama.cpp's current master produces the following output:\r\n```bash\r\n$ ./main -t 4 -m ./models/codellama-7b.Q3_K_M.gguf.old --color -c 512 --temp 0.0 --repeat_penalty 1.0 -n 128 -p \"double fast_inverse_square_root(double x\"\r\n\r\n double fast_inverse_square_root(double x)\r\n{\r\n    long i;\r\n    double x2, y;\r\n    const double threehalfs = 1.5;\r\n\r\n    x2 = x * 0.5;\r\n    y  = x;\r\n    i  = * ( long * ) &y;\r\n    i  = 0x5f3759df - ( i >> 1 );\r\n    y  = * ( double * ) &i;\r\n    y  = y * ( threehalfs - ( x2 * y * y ) );\r\n    y  = y * ( threehalf\r\nllama_print_timings:        load time =  1603.99 ms\r\nllama_print_timings:      sample time =     4.17 ms /   128 runs   (    0.03 ms per token, 30732.29 tokens per second)\r\nllama_print_timings: prompt eval time =  1096.09 ms /    13 tokens (   84.31 ms per token,    11.86 tokens per second)\r\nllama_print_timings:        eval time = 16623.97 ms /   127 runs   (  130.90 ms per token,     7.64 tokens per second)\r\nllama_print_timings:       total time = 17809.38 ms\r\nLog end\r\n\r\n```\r\nBoth models produce the same output on master, whereas the old model produced the correct output up until #3228 \r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  8\r\n  On-line CPU(s) list:   0-7\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  4\r\n    Socket(s):           1\r\n    Stepping:            10\r\n    CPU max MHz:         4100.0000\r\n    CPU min MHz:         800.0000\r\n    BogoMIPS:            4800.00\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts \r\n                         rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer\r\n                          aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2\r\n                          erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\r\nVirtualization features: \r\n  Virtualization:        VT-x\r\nCaches (sum of all):     \r\n  L1d:                   128 KiB (4 instances)\r\n  L1i:                   128 KiB (4 instances)\r\n  L2:                    1 MiB (4 instances)\r\n  L3:                    8 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-7\r\nVulnerabilities:         \r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\n  Mds:                   Mitigation; Clear CPU buffers; SMT vulnerable\r\n  Meltdown:              Mitigation; PTI\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Mitigation; Microcode\r\n  Tsx async abort:       Not affected\r\n\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Linux pop-os 6.4.6-76060406-generic #202307241739~1692717645~22.04~5597803 SMP PREEMPT_DYNAMIC Tue A x86_64 x86_64 x86_64 GNU/Linux`\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.12\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n$ g++ --version\r\ng++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. [Download model from here](https://huggingface.co/TheBloke/CodeLlama-7B-GGUF/blob/74bf05c6562b9431494d994081b671206621c199/codellama-7b.Q3_K_M.gguf)\r\n2. Clone llama.cpp and build with `make`\r\n3. Run `./main -t 4 -m ./models/codellama-7b.Q3_K_M.gguf --color -c 512 --temp 0.0 --repeat_penalty 1.0 -n 128 -p \"double fast_inverse_square_root(double x\"`\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nLogs for all 4 tested cases are attached, Github wouldn't let me paste them in here.\r\n\r\n\r\n[old-model-commit-45855b3.log](https://github.com/ggerganov/llama.cpp/files/12753518/old-model-commit-45855b3.log)\r\n[new-model-commit-45855b3.log](https://github.com/ggerganov/llama.cpp/files/12753519/new-model-commit-45855b3.log)\r\n[old-model-master.log](https://github.com/ggerganov/llama.cpp/files/12753523/old-model-master.log)\r\n[new-model-master.log](https://github.com/ggerganov/llama.cpp/files/12753525/new-model-master.log)\r\n\r\n",
    "labels": [
      "need feedback",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-28T20:01:22+00:00",
    "closed_at": "2024-04-03T01:15:32+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3384/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3384"
  }
]
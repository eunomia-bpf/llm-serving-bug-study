# bug - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 5
- Closed Issues: 25

### Label Distribution

- bug: 30 issues
- stale: 14 issues
- rocm: 3 issues
- v1: 1 issues
- ray: 1 issues
- structured-output: 1 issues

---

## Issue #N/A: [Bug]: safetensor format support for Mistral-7B-v0.3

**Link**: https://github.com/vllm-project/vllm/issues/6199
**State**: closed
**Created**: 2024-07-08T02:40:38+00:00
**Closed**: 2024-11-25T02:05:13+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

```text
PyTorch version: 2.3.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.29.3
Libc version: glibc-2.31

Python version: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-176-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB
GPU 4: NVIDIA A100-SXM4-40GB
GPU 5: NVIDIA A100-SXM4-40GB
GPU 6: NVIDIA A100-SXM4-40GB
GPU 7: NVIDIA A100-SXM4-40GB

Nvidia driver version: 550.54.15
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: KeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'

**Link**: https://github.com/vllm-project/vllm/issues/4247
**State**: closed
**Created**: 2024-04-21T23:20:50+00:00
**Closed**: 2024-11-29T02:06:39+00:00
**Comments**: 4
**Labels**: bug, stale

### Description

### Your current environment

```text
Collecting environment information...
PyTorch version: 2.2.1+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.29.2
Libc version: glibc-2.35

Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.5.0-25-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti
GPU 2: NVIDIA GeForce RTX 2080 Ti
GPU 3: NVIDIA GeForce RTX 2080 Ti
GPU 4: NVIDIA GeForce RTX 2080 Ti
GPU 5: NVIDIA GeForce RTX 2080 Ti
GPU 6: NVIDIA GeForce RTX 2080 Ti
GPU 7: NVIDIA GeForce RTX 2080 Ti

Nvidia driver version: 535.161.07
cuDNN version: Could not collect

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Bert tokenizer is tokenizing some tokens as `UNK`

**Link**: https://github.com/vllm-project/vllm/issues/11184
**State**: closed
**Created**: 2024-12-13T21:33:18+00:00
**Closed**: 2025-01-09T03:05:45+00:00
**Comments**: 1
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Your output of `python collect_env.py` here
```

</details>


### Model Input Dumps

_No response_

### üêõ Describe the bug

With some Bert and Roberta models like `sentence-transformers/all-MiniLM-L12-v2` I found that the output is not similar to the one generated by `sentence-transformers`. If I place the following prints in `_normalize_prompt_text_to_input()` in `serving_engine.py`
```
        print(f"{input_ids=}")
```
I get `[101, 100, 3007, 1997, 100, 2003, 100, 1012, 102]` for the sentence "The capital of France is Paris.". 100 is the `UNK` token.  When I run with sentence-transformers, I get `[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]` . This problem happens both with `--tokenizer-mode auto` and `--tokenizer-mode slow`.

cc: @DarkLight1337 

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. on Gaudi2

**Link**: https://github.com/vllm-project/vllm/issues/13054
**State**: open
**Created**: 2025-02-10T20:28:41+00:00
**Comments**: 3
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Automatically detected platform hpu.
Collecting environment information...
PyTorch version: 2.5.1a0+git6fc067b
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address size

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: V1 engine ignores guided json

**Link**: https://github.com/vllm-project/vllm/issues/12692
**State**: closed
**Created**: 2025-02-03T14:01:44+00:00
**Closed**: 2025-06-06T02:18:22+00:00
**Comments**: 5
**Labels**: bug, stale, v1

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
INFO 02-03 05:55:13 __init__.py:183] Automatically detected platform cuda.
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.31.4
Libc version: glibc-2.35

Python version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3
Nvidia driver version: 560.35.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model

**Link**: https://github.com/vllm-project/vllm/issues/19867
**State**: closed
**Created**: 2025-06-19T14:36:59+00:00
**Closed**: 2025-07-14T17:39:11+00:00
**Comments**: 1
**Labels**: bug, rocm

### Description

### Your current environment

<details>
<summary>The output of <code>python collect_env.py</code></summary>

```text
Your output of `python collect_env.py` here
```

</details>


### üêõ Describe the bug

We hit an exception on running llama4 models with latest code on ROCm V1:

```
(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'
```
Current work-around:
To turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0


Proposal:

- [ ] Fix the bug (the team is working on it)
- [ ] Add a end-to-end test for one of the small llama4 models
- [ ] 

The motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.


### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: LLM.classify() fails on second call with ModernBERT due to missing torch.SymInt shape symbols

**Link**: https://github.com/vllm-project/vllm/issues/20716
**State**: open
**Created**: 2025-07-09T22:46:17+00:00
**Comments**: 0
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of <code>python collect_env.py</code></summary>

```text
INFO 07-09 15:38:22 [__init__.py:244] Automatically detected platform cuda.
Collecting environment information...
uv is set
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.22.1
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.7.1+cu126
Is debug build               : False
CUDA used to build PyTorch   : 12.6
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vLLM crash when running Phi-3-small-8k-instruct with enable-chunked-prefill

**Link**: https://github.com/vllm-project/vllm/issues/5961
**State**: closed
**Created**: 2024-06-28T13:34:09+00:00
**Closed**: 2024-06-28T22:41:15+00:00
**Comments**: 3
**Labels**: bug

### Description

### Your current environment


```
image": "vllm/vllm-openai:latest",
--model=microsoft/Phi-3-small-8k-instruct 
--tensor-parallel-size=1
--disable-log-requests
--trust-remote-code
--enable-chunked-prefill
--max-num-batched-tokens=2048
--max-model-len=4096
--gpu-memory-utilization=0.9",
```
Accelerator: 1x Nvidia L4

### üêõ Describe the bug

```
ERROR 06-28 13:26:18 async_llm_engine.py:52] Engine background task failed
ERROR 06-28 13:26:18 async_llm_engine.py:52] Traceback (most recent call last):
ERROR 06-28 13:26:18 async_llm_engine.py:52]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 42, in _log_task_completion
ERROR 06-28 13:26:18 async_llm_engine.py:52]     return_value = task.result()
ERROR 06-28 13:26:18 async_llm_engine.py:52]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 532, in run_engine_loop
ERROR 06-28 13:26:18 async_llm_engine.py:52]     has_requests_in_progress = await asyn

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Model weights in GiB 

**Link**: https://github.com/vllm-project/vllm/issues/14979
**State**: closed
**Created**: 2025-03-17T18:28:30+00:00
**Closed**: 2025-03-31T17:00:51+00:00
**Comments**: 1
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ] (64-bit runtime)
Python platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA L40S
Nvidia driver version: 570.86.10
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 57 bits v

[... truncated for brevity ...]

---

## Issue #N/A: Loading Models that require execution of third party code (trust_remote_code=True)

**Link**: https://github.com/vllm-project/vllm/issues/354
**State**: closed
**Created**: 2023-07-04T08:05:46+00:00
**Closed**: 2024-03-08T10:22:14+00:00
**Comments**: 15
**Labels**: bug

### Description

I am trying to load MPT using the AsyncLLMEngine:

```

engine_args = AsyncEngineArgs("mosaicml/mpt-7b-chat", engine_use_ray=True)
engine = AsyncLLMEngine.from_engine_args(engine_args)
```

But I am getting this error:
`ValueError: Loading mosaicml/mpt-7b-chat-local requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.`

Is there any workaround for this or could it be possible to add the option to trust remote code to EngineArgs?

---

## Issue #N/A: [Bug]:  Failed to Run Qwen2.5-7B with RTX 3070 & CPU Offload (14GB) Despite Sufficient Theoretical Memory

**Link**: https://github.com/vllm-project/vllm/issues/15004
**State**: closed
**Created**: 2025-03-18T05:34:12+00:00
**Closed**: 2025-07-18T02:28:25+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

The output of `python collect_env.py`
(vllm) roy@Roy-L:~/projects$ python collect_env.py
INFO 03-12 13:15:42 __init__.py:207] Automatically detected platform cuda.
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.8.61
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU
Nvidia driver version: 572.70
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.

**Link**: https://github.com/vllm-project/vllm/issues/5311
**State**: closed
**Created**: 2024-06-06T07:50:01+00:00
**Closed**: 2025-02-11T16:43:28+00:00
**Comments**: 3
**Labels**: bug

### Description

### Your current environment

```text
The output of `python collect_env.py`
```
PyTorch version: 2.1.2+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.5.0-35-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce GTX 1080 Ti
GPU 1: NVIDIA GeForce GTX 1080 Ti
GPU 2: NVIDIA GeForce GTX 1080 Ti
GPU 3: NVIDIA GeForce GTX 1080 Ti

Nvidia driver version: 535.171.04
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7
/usr/lib/x86_64-lin

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Concurrently captioning images with phi3 Vision can cause the backend to crash

**Link**: https://github.com/vllm-project/vllm/issues/5885
**State**: closed
**Created**: 2024-06-27T05:17:59+00:00
**Closed**: 2024-06-27T08:29:26+00:00
**Comments**: 3
**Labels**: bug

### Description

### Your current environment

```text
PyTorch version: 2.3.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.29.6
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA H100 PCIe
Nvidia driver version: 550.67
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      52 bits physical, 57 bits virtual
Byte Order:                         Little

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: : CPU silently doesn't support multi-step (--num-scheduler-steps)

**Link**: https://github.com/vllm-project/vllm/issues/8477
**State**: closed
**Created**: 2024-09-13T19:55:13+00:00
**Closed**: 2025-01-13T02:03:03+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Collecting environment information...
INFO 09-13 19:13:45 importing.py:10] Triton not installed; certain GPU-related functions will not be available.
PyTorch version: 2.4.0+cpu
Is debug build: False
CUDA used to build PyTorch: Could not collect
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0
Clang version: Could not collect
CMake version: version 3.30.2
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-4.18.0-372.46.1.el8_6.x86_64-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
  MIG 3g.40gb     Device  0:

Nvidia driver version: 535.104.05
cuDNN version: Could not collect
HIP 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vLLM 0.7.3 TypeError in vllm.entrypoints.api_server Argument Parsing

**Link**: https://github.com/vllm-project/vllm/issues/13848
**State**: open
**Created**: 2025-02-25T21:27:29+00:00
**Comments**: 7
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
INFO 02-25 14:13:01 __init__.py:190] Automatically detected platform cpu.
Collecting environment information...
PyTorch version: 2.5.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 14.7.3 (arm64)
GCC version: Could not collect
Clang version: 16.0.0 (clang-1600.0.26.6)
CMake version: Could not collect
Libc version: N/A

Python version: 3.11.6 (main, Feb 25 2025, 12:41:54) [Clang 16.0.0 (clang-1600.0.26.6)] (64-bit runtime)
Python platform: macOS-14.7.3-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M2 Pro

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] nvidia-ml-py==12.570.86
[pip3] 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Sliding Window Attention not supported in V1 for ROCm

**Link**: https://github.com/vllm-project/vllm/issues/19367
**State**: closed
**Created**: 2025-06-09T16:12:29+00:00
**Closed**: 2025-06-10T20:28:04+00:00
**Comments**: 4
**Labels**: bug, rocm

### Description

### Your current environment

<details>
<summary>The output of <code>python collect_env.py</code></summary>

```text
INFO 06-09 15:59:09 [__init__.py:248] Automatically detected platform rocm.
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version                : 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)
CMake version                : version 3.31.4
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.7.0a0+git6c0e746
Is debug build               : False
CUDA used to build PyTorch   : N/A
ROCM used to build PyTorch   : 6.3.42133-1b9c17779

==============================
      Python Environment
========

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: AMD GPU docker image build No matching distribution found for torch==2.6.0.dev20241113+rocm6.2

**Link**: https://github.com/vllm-project/vllm/issues/12178
**State**: closed
**Created**: 2025-01-17T23:36:10+00:00
**Closed**: 2025-03-12T05:50:14+00:00
**Comments**: 2
**Labels**: bug, rocm

### Description

### Your current environment

Archlinux 13th Gen Intel(R) Core(TM) i9-13900HX environment to build the docker image

### Model Input Dumps

_No response_

### üêõ Describe the bug

Trying to build the AMD GPU docker image:
```
git checkout v0.6.6.post1
DOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm -t substratusai/vllm-rocm:v0.6.6.post1 .
```

Results in following error:

```
1.147 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/rocm6.2
1.717 ERROR: Could not find a version that satisfies the requirement torch==2.6.0.dev20241113+rocm6.2 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0.dev20241119+rocm6.2, 2.6.0.dev20241120+rocm6.2, 2.6.0.dev20241121+rocm6.2, 2.6.0.dev20241122+rocm6.2)
2.135 ERROR: No matching distribution found for torch==2.6.0.dev20241113+rocm6.2
------
Dockerfil

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]:  all_reduce assert result == 0, File "torch/cuda/graphs.py", line 88, in capture_end    super().capture_end(), RuntimeError: CUDA error: operation failed due to a previous error during capture

**Link**: https://github.com/vllm-project/vllm/issues/4432
**State**: closed
**Created**: 2024-04-28T13:01:49+00:00
**Closed**: 2024-11-28T02:05:53+00:00
**Comments**: 8
**Labels**: bug, stale

### Description

### Your current environment

```text
Collecting environment information...
PyTorch version: 2.1.2+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Centos 7 (Final) (x86_64)
GCC version: (GCC) 7.3.0
Clang version: Could not collect
CMake version: version 3.26.1
Libc version: glibc-2.17

Python version: 3.8.12 (default, Nov 11 2021, 20:11:20)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-4.14.105-1-tlinux3-0013-x86_64-with-glibc2.2.5
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB
GPU 2: Tesla V100-SXM2-32GB
GPU 3: Tesla V100-SXM2-32GB
GPU 4: Tesla V100-SXM2-32GB
GPU 5: Tesla V100-SXM2-32GB
GPU 6: Tesla V100-SXM2-32GB
GPU 7: Tesla V100-SXM2-32GB

Nvidia driver version: 450.156.00
cuDNN version: Probably one of the following:
/usr/lib64/libcudnn.so.8.0.5
/usr/lib64/libcudnn_ad

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: System error: Can't get attribute 'TokenizerGroup' on <module 'vllm.transformers_utils.tokenizer'

**Link**: https://github.com/vllm-project/vllm/issues/3627
**State**: closed
**Created**: 2024-03-26T04:36:28+00:00
**Closed**: 2024-11-29T02:07:04+00:00
**Comments**: 8
**Labels**: bug, stale

### Description

### Your current environment

```text
cuda 12.1  simple pip install vllm
```


### üêõ Describe the bug

`python benchmarks/benchmark_throughput.py --backend vllm --input-len 1024 --output-len 512 --model /share/datasets/public_models/Qwen_Qwen-72B-Chat --tensor-parallel-size 4 --trust-remote-code`

This will result in the following errors:

![image](https://github.com/vllm-project/vllm/assets/79788571/f2bb89b0-bc19-4e3b-859f-0b63ffe76dd7)

---

## Issue #N/A: [Bug]:  online fp8 quantization with jais model got assert error due to cutlass_scaled_mm()

**Link**: https://github.com/vllm-project/vllm/issues/7550
**State**: closed
**Created**: 2024-08-15T09:28:39+00:00
**Closed**: 2025-04-16T02:20:10+00:00
**Comments**: 9
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>


Nvidia driver version: 555.42.06
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Byte Order:                         Little Endian
Address sizes:                      52 bits physical, 57 bits virtual
CPU(s):                             384
On-line CPU(s) list:                0-383
Thread(s) per core:                 2
Core(s) per socket:                 96
Socket(s):                          2
NUMA node(s):                       2
Vendor ID:                          AuthenticAMD
CPU family:                         25
Model:                              17
Model name:                         AMD EPYC 9654 96-Core Processor
Stepping:                           1
Frequency boost:                    enabl

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: benchmark_serving.py generates different numbers of tokens at different runs

**Link**: https://github.com/vllm-project/vllm/issues/8531
**State**: closed
**Created**: 2024-09-17T06:09:27+00:00
**Closed**: 2025-03-01T02:05:45+00:00
**Comments**: 4
**Labels**: bug, stale

### Description

### Your current environment

4xH100.


### Model Input Dumps

_No response_

### üêõ Describe the bug

When benchmarking the performance of vllm with `benchmark_serving.py`, it will generate different number of tokens at different runs.

Code to launch vllm server
```
vllm serve meta-llama/Meta-Llama-3.1-70B-Instruct \
    --disable-log-requests \
    --tensor-parallel-size 4
```

Code to run the benchmark
```
python benchmarks/benchmark_serving.py \
    --backend vllm \
    --model meta-llama/Meta-Llama-3.1-70B-Instruct\
    --dataset-name sharegpt \
    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
    --request-rate 1 \
    --num-prompts 200 \
    --save-result
```

If I run the benchmark_serving.py script twice, the number of generated tokens is different for the two runs.
The output of the first run:
```
============ Serving Benchmark Result ============                                                                                            

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: V1 engine peak memory usage calculations incorrect

**Link**: https://github.com/vllm-project/vllm/issues/16141
**State**: open
**Created**: 2025-04-07T01:47:50+00:00
**Comments**: 8
**Labels**: bug

### Description

### Your current environment

The `collect_env.py` script doesn't work because I don't have vllm installed in my environment. This bug is reproducible using the docker image, so I don't think this matters.

Affected VLLM version is `v0.8.3`.

### üêõ Describe the bug

The peak memory usage calculations for VLLM is buggy. It seems to think that the memory usage of the other processes on the GPU contribute to the minimum required. This happens with `v0.8.3`.

This is a problem when running multiple instances of VLLM on the same GPU.

## Repro steps

This is easy to reproduce with the docker image. Here is the `nvidia-smi` output before running VLLM. No memory usage.

```text
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]:  Requests larger than 75k input tokens cause `Input prompt (512 tokens) is too long and exceeds the capacity of block_manager` error

**Link**: https://github.com/vllm-project/vllm/issues/7878
**State**: closed
**Created**: 2024-08-26T19:16:25+00:00
**Closed**: 2025-02-21T02:00:23+00:00
**Comments**: 8
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Collecting environment information...
PyTorch version: 2.4.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Amazon Linux 2 (x86_64)
GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)
Clang version: Could not collect
CMake version: version 3.27.7
Libc version: glibc-2.26

Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)
Python platform: Linux-5.10.220-209.869.amzn2.x86_64-x86_64-with-glibc2.26
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA L4
Nvidia driver version: 550.90.07
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: When load model weights, there are infinite loading

**Link**: https://github.com/vllm-project/vllm/issues/5062
**State**: closed
**Created**: 2024-05-27T02:15:22+00:00
**Closed**: 2024-06-13T09:00:12+00:00
**Comments**: 7
**Labels**: bug

### Description

### Your current environment

Collecting environment information...
PyTorch version: 2.1.2+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.29.2
Libc version: glibc-2.31

Python version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100 80GB PCIe
GPU 1: NVIDIA A100 80GB PCIe
GPU 2: NVIDIA A100 80GB PCIe
GPU 3: NVIDIA A100 80GB PCIe

Nvidia driver version: 525.147.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):       

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vllm_C is missing. 

**Link**: https://github.com/vllm-project/vllm/issues/4083
**State**: closed
**Created**: 2024-04-15T07:15:06+00:00
**Closed**: 2024-06-13T09:16:35+00:00
**Comments**: 6
**Labels**: bug

### Description

### Your current environment

Previous fix from https://github.com/vllm-project/vllm/pull/3913 did not seem to work. Same issue still encountered. 

```text
Collecting environment information...
INFO 04-15 07:13:37 pynccl.py:58] Loading nccl from library /home/me/.config/vllm/nccl/cu12/libnccl.so.2.18.1
PyTorch version: 2.1.2+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 11 (bullseye) (x86_64)
GCC version: (Debian 10.2.1-6) 10.2.1 20210110
Clang version: 11.0.1-2
CMake version: version 3.29.2
Libc version: glibc-2.31

Python version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)
Python platform: Linux-5.16.0-0.bpo.4-amd64-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3:

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Ray+vllm run, then crash

**Link**: https://github.com/vllm-project/vllm/issues/13535
**State**: closed
**Created**: 2025-02-19T09:39:51+00:00
**Closed**: 2025-03-24T22:37:25+00:00
**Comments**: 4
**Labels**: bug, ray

### Description

### Your current environment

<details>
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.131
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: Tesla T4
GPU 1: Tesla T4

Nvidia driver version: 560.35.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      40 bits physical, 48 bits virtual
Byte Order:  

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: guided_grammar example syntax does not work

**Link**: https://github.com/vllm-project/vllm/issues/16911
**State**: open
**Created**: 2025-04-21T08:13:48+00:00
**Comments**: 0
**Labels**: bug, structured-output

### Description

### Your current environment

I'm using vllm hosted on a K8s instance and was not able to execute the environment collection python file there. But this is the error I get:

<details>
<summary>Error message</summary>

```
INFO 04-21 00:33:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.7%                                                                                  
INFO 04-21 00:33:55 [logger.py:39] Received request chatcmpl-5a7a5fbaada34f3a88b577a238ddd279: prompt: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nGenerate an SQL query to show the 'username' and 'email'from the 'users' table.<|im_end|>\n<|im_start|>assistant\ 
INFO 04-21 00:33:55 [async_llm.py:228] Added request chatcmpl-5a7a5fbaada34f3a88b577a238ddd279.                                                                                         

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Mamba2 models (Bamba and Codestral Mamba) fail on RoCM

**Link**: https://github.com/vllm-project/vllm/issues/13678
**State**: closed
**Created**: 2025-02-21T17:00:25+00:00
**Closed**: 2025-06-26T02:26:03+00:00
**Comments**: 5
**Labels**: bug, stale

### Description

### Your current environment

Via @hackey:
>I am using:
ROCM (Dual AMD 7900 xtx)
Ubuntu 24.04

### üêõ Describe the bug

See https://github.com/vllm-project/vllm/issues/6479#issuecomment-2674292711

Specifically this part:
```
registry.py:321]     from vllm.attention.backends.flash_attn import FlashAttentionMetadata ERROR 02-21 11:17:10 registry.py:321]   File "/usr/local/lib/python3.12/dist-packages/vllm/attention/backends/flash_attn.py", line 25, in <module> ERROR 02-21 11:17:10 registry.py:321]     from vllm.vllm_flash_attn import (flash_attn_varlen_func, ERROR 02-21 11:17:10 registry.py:321] ImportError: cannot import name 'flash_attn_varlen_func' from 'vllm.vllm_flash_attn' (unknown location) ERROR 02-21 11:17:10 registry.py:321]  Traceback (most recent call last): File "/usr/local/bin/vllm", line 8, in <module> sys.exit(main()) ^^^^^^ File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main 
```

It looks like the problem is caused by importing 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Internal Server Error when hosting Salesforce/SFR-Embedding-Mistral

**Link**: https://github.com/vllm-project/vllm/issues/5906
**State**: closed
**Created**: 2024-06-27T14:14:58+00:00
**Closed**: 2024-12-12T02:07:14+00:00
**Comments**: 6
**Labels**: bug, stale

### Description

### Your current environment

Using latest docker image vllm/vllm-openai:v0.5.0.post1


### üêõ Describe the bug

When trying to send a request to the /v1/embeddings endpoint of the deployed model, I get the response "Internal Server Error". For reference, there is the log from the according vllm container:
```
/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 06-27 13:41:32 api_server.py:177] vLLM API server version 0.5.0.post1
INFO 06-27 13:41:32 api_server.py:178] args: Namespace(host=None, port=8080, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/mnt

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Error when running pytest: TypeError: 'ABCMeta' object is not subscriptable

**Link**: https://github.com/vllm-project/vllm/issues/4081
**State**: closed
**Created**: 2024-04-15T06:43:33+00:00
**Closed**: 2024-04-15T21:47:32+00:00
**Comments**: 4
**Labels**: bug

### Description

### Your current environment

When calling `python collect_env.py`, running into the same error as below.


### üêõ Describe the bug

Encountering the below issue when running tests. Suspected to be related to mypy changes
![image](https://github.com/vllm-project/vllm/assets/88394319/fd9057d8-6f9c-4f51-b60d-88b7b88b70d2)


---


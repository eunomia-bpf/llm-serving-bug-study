# historical_1year_plus - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 0
- Closed Issues: 30

### Label Distribution

- stale: 10 issues
- bug: 6 issues
- usage: 4 issues
- misc: 2 issues
- installation: 1 issues
- feature request: 1 issues
- documentation: 1 issues
- performance: 1 issues
- new-model: 1 issues
- RFC: 1 issues

---

## Issue #N/A: [Usage]: Gemma-2-9b is not supported

**Link**: https://github.com/vllm-project/vllm/issues/6026
**State**: closed
**Created**: 2024-07-01T11:00:37+00:00
**Closed**: 2024-07-01T19:26:02+00:00
**Comments**: 3
**Labels**: usage

### Description

### Your current environment
This is my vllm versions
vllm                              0.5.0.post1
vllm-flash-attn                   2.5.9
vllm-nccl-cu11                    2.18.1.0.4.0




### How would you like to use vllm

I want to run inference of a gemma-2-9b, which appears in the supported models list in the website.



This is the code

```
from vllm import LLM, SamplingParams
model_name = "google/gemma-2-9b"  # You can change this to any other supported model
llm = LLM(model=model_name, dtype=torch.float16, device="auto", tensor_parallel_size=2)

```

I'm getting the error:

(VllmWorkerProcess pid=225845) ERROR 07-01 10:50:45 multiproc_worker_utils.py:226] ValueError: Model architectures ['Gemma2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM',

[... truncated for brevity ...]

---

## Issue #N/A: High CPU Usage in Kubernetes with Double T4 Running Zephyr 7b Model

**Link**: https://github.com/vllm-project/vllm/issues/3271
**State**: closed
**Created**: 2024-03-08T07:07:08+00:00
**Closed**: 2024-11-30T02:01:56+00:00
**Comments**: 2
**Labels**: stale

### Description

Thanks for your wonderful work.

I am using below docker file to build image 

> FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04
> RUN apt-get update \
>     && apt-get -yqq upgrade \
>     && apt-get -yqq install python3-pip python-is-python3 \
>     && apt-get -qqy autoremove --purge \
>     && apt-get clean \
>     && rm -rf /var/lib/apt/lists/* /var/cache/apt/* \
>     && rm -rf /usr/share/man
> 
> COPY requirements.txt requirements.txt
> RUN pip3 install -r requirements.txt \
>     && pip3 cache purge
> 
> ENTRYPOINT ["python","-m", "vllm.entrypoints.openai.api_server"]

Utilizing the above image, we create a pod with ample resources, including 2 T4 GPUs, 8 vCPUs, and 32GB RAM. However, after receiving several requests (sometimes even at starting of pod itself and at no request), the CPU usage spikes significantly to around 1400-1500%. Surprisingly, it does not decrease during idle periods.

Could you please help me to resolve the issue.

---

## Issue #N/A: [Bug]: ncclSystemError when use two gpus

**Link**: https://github.com/vllm-project/vllm/issues/4383
**State**: closed
**Created**: 2024-04-26T02:08:20+00:00
**Closed**: 2024-04-26T02:22:43+00:00
**Comments**: 2
**Labels**: bug

### Description

### Your current environment

```text
The output of `python collect_env.py`
```
Collecting environment information...
PyTorch version: 2.2.1+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.29.2
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.11.0-40-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090

Nvidia driver version: 535.129.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr

[... truncated for brevity ...]

---

## Issue #N/A: In Qwen-14B, using vllm for reasoning with long prompts yields poor results | 在Qwen-14B中，使用vllm对long prompt进行推理，效果很差

**Link**: https://github.com/vllm-project/vllm/issues/1683
**State**: closed
**Created**: 2023-11-16T06:26:12+00:00
**Closed**: 2023-11-20T09:53:34+00:00
**Comments**: 8

### Description

Hello, I have modified qwen-14B using PI to support long text inference.
The validation of the HF model has been completed and the results are okay.
However, when using vllm inference, the answers for long prompts are far from the answers generated by the HF model, and they are not coherent (the answers for short prompts are normal).
For example, when I ask a summarization question, the HF model can provide a proper summary, but the vllm answer looks something like this:
"Reasonable use. And, the plaintiff, and the defendant (Caesar, reconciliatory)."
(It is basically gibberish)
I have checked other issues and tried adjusting parameters like max_tokens, top_k, and top_p (keeping them consistent with HF's settings), but the conclusion remains the same.
I would like to know if vllm currently does not support long text inference, or if it does not support the modified qwen-14B with linear interpolation, or if there is an issue with my usage.

-------------------------------------

[... truncated for brevity ...]

---

## Issue #N/A: No CUDA GPUs are available

**Link**: https://github.com/vllm-project/vllm/issues/807
**State**: closed
**Created**: 2023-08-21T05:07:04+00:00
**Closed**: 2024-03-08T12:12:23+00:00
**Comments**: 2

### Description

We are planning to deploy the vLLM using the docker image. Please find below the code for  docker image

```
ARG base=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04

ARG commit=main

FROM ${base}

ENV DEBIAN_FRONTEND=noninteractive LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8

ARG CONDA_VERSION=py310_23.3.1-0

RUN apt update && \
    apt install -y --no-install-recommends \
        wget \
        git \
        build-essential \
        ca-certificates && \
    rm -rf /var/lib/apt/lists/*

RUN set -x && \
    UNAME_M="$(uname -m)" && \
    if [ "${UNAME_M}" = "x86_64" ]; then \
        MINICONDA_URL="[https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-x86_64.sh](https://repo.anaconda.com/miniconda/Miniconda3-$%7BCONDA_VERSION%7D-Linux-x86_64.sh)"; \
        SHA256SUM="aef279d6baea7f67940f16aad17ebe5f6aac97487c7c03466ff01f4819e5a651"; \
    elif [ "${UNAME_M}" = "s390x" ]; then \
        MINICONDA_URL="[https://repo.anaconda.com/miniconda/Miniconda3-${CO

[... truncated for brevity ...]

---

## Issue #N/A: VLLM Multi-Lora with embed_tokens and lm_head in adapter weights 

**Link**: https://github.com/vllm-project/vllm/issues/2816
**State**: closed
**Created**: 2024-02-08T17:10:12+00:00
**Closed**: 2025-01-05T02:04:46+00:00
**Comments**: 8
**Labels**: stale

### Description

Hi there!

I've encountered an issue with the `adatpter_model.safetensors` in my project, and I'm seeking guidance on how to handle `lm_head` and `embed_tokens` within the specified modules. Here's the current state:

```
['base_model.model.lm_head.weight',
 'base_model.model.model.embed_tokens.weight',
 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight',
 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.weight',
 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight',
 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.weight',
 ...]
 ```
 
Firstly, in [vllm/lora/utils.py](https://github.com/vllm-project/vllm/blob/0e163fce18594c7e29dc5a143dd6b33d213fcbf3/vllm/lora/utils.py#L33), modifications are needed to work with `lm_head` and `embed_tokens`. My question is, what should be the appropriate name for the new module? For instance, from `base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight`, we extract `model.layers.0.self_at

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Mixtral-8x7B-Instruct-v0.1 not loading

**Link**: https://github.com/vllm-project/vllm/issues/3611
**State**: closed
**Created**: 2024-03-25T09:02:03+00:00
**Closed**: 2024-03-26T03:52:28+00:00
**Comments**: 6
**Labels**: bug

### Description

### Your current environment

```text
Collecting environment information...
PyTorch version: 2.1.2+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.4
Libc version: glibc-2.35

Python version: 3.9.19 (main, Mar 21 2024, 17:11:28)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-4.18.0-408.el8.x86_64-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.0.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A30
GPU 1: NVIDIA A30
GPU 2: NVIDIA A30
GPU 3: NVIDIA A30
GPU 4: NVIDIA A30
GPU 5: NVIDIA A30
GPU 6: NVIDIA A30
GPU 7: NVIDIA A30

Nvidia driver version: 535.86.10
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                  

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: Question: Does pip package comes with PUNICA kernels? 

**Link**: https://github.com/vllm-project/vllm/issues/4379
**State**: closed
**Created**: 2024-04-25T23:55:12+00:00
**Closed**: 2024-04-26T00:46:52+00:00
**Comments**: 2
**Labels**: installation

### Description

### Your current environment

```text
Irrelevant 
```


### How you are installing vllm

```sh
pip install vllm==v0.4.0.post1
```
My question is: Is my assumption that pip package comes with punica kernels, and is there any intention to change this in the future? 

I ran a couple of tests and it seems that at least `v0.4.0.post1` comes with punica kernels:
```
root@oandreeva-dt:/opt/tritonserver# python3
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import vllm._punica_C as punica_kernels
>>> exit()
```

I believe, this is intended: https://github.com/vllm-project/vllm/blob/15e7c675b0dc36109c7b591f856f102e96493a94/.github/workflows/scripts/build.sh#L16

P.S. please feel free to re-direct me to a dedicated Q&A resource for next time. 


---

## Issue #N/A: Diverse Beam Search

**Link**: https://github.com/vllm-project/vllm/issues/4250
**State**: closed
**Created**: 2024-04-22T03:20:27+00:00
**Closed**: 2024-11-29T02:06:38+00:00
**Comments**: 2
**Labels**: feature request, stale

### Description

### 🚀 The feature, motivation and pitch

Now vllm cannot support diverse beam search which transformers already supports(https://huggingface.co/docs/transformers/generation_strategies#diverse-beam-search-decoding), will you implement diverse beam search in vllm?

### Alternatives

_No response_

### Additional context

_No response_

---

## Issue #N/A: Stream Tokens operation integration into LLM class (which uses LLMEngine behind the scenes)

**Link**: https://github.com/vllm-project/vllm/issues/813
**State**: closed
**Created**: 2023-08-21T11:21:55+00:00
**Closed**: 2024-03-20T12:44:54+00:00
**Comments**: 9

### Description

Hey,
I wonder why the token streaming via LLM class is not implemented, but for LLMEngine it does?
https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py
LLM is using behind the scene the LLMEngine. LLM class is much simpler (to me I guess, not sure if to everyone else) and also utilize batch processing on top (on top  LLMEngine) of tensor parallel, so it would be nice to get also a stream with batch process together if possible, or either of them. It shouldn't be much work to do as I read the code and after I talked with @naed90 about that feature.
Thanks, Orel

---

## Issue #N/A: [Usage]: Passing a guided_json in offline inference

**Link**: https://github.com/vllm-project/vllm/issues/4899
**State**: closed
**Created**: 2024-05-18T09:08:07+00:00
**Closed**: 2024-11-27T02:08:12+00:00
**Comments**: 3
**Labels**: usage, stale

### Description

### Your current environment

```text
vllm 0.4.2
```


### How would you like to use vllm

I'm trying to force a json generation using outlines in offline inference. I don't see anything related in the documentation.

I haven't found an example of chat completion for offline inference, but I've managed to mimic it using chat templates, this is why I need to force a json generation.


---

## Issue #N/A: [Misc]: vllm ONLY allocate KVCache on the first device in CUDA_VISIBLE_DEVICES

**Link**: https://github.com/vllm-project/vllm/issues/5248
**State**: closed
**Created**: 2024-06-04T11:44:47+00:00
**Closed**: 2024-11-30T02:01:45+00:00
**Comments**: 5
**Labels**: misc, stale

### Description

### KVCache usage

it seems like the KVCache is only allocated on the first device in CUDA_VISIBLE_DEVICES, even if using `tensor-parallel-size` > 1. Is there any plan to support full KVCache allocated in all devices?

[cache_engine.py](https://github.com/vllm-project/vllm/tree/main/vllm/worker)
```python
        # Initialize the cache. # line 56
        self.gpu_cache = self._allocate_kv_cache(self.num_gpu_blocks, "cuda")
```

---

## Issue #N/A: support set logprobs to model vocab size

**Link**: https://github.com/vllm-project/vllm/issues/2782
**State**: closed
**Created**: 2024-02-06T06:40:00+00:00
**Closed**: 2024-11-30T02:02:40+00:00
**Comments**: 3
**Labels**: stale

### Description

set logprobs to model vocab size is hard, some model can use len(tokenizer), some use tokenizer.vocab_size, some will raise an error. so need to support set logprobs to model vocab size

---

## Issue #N/A: Add documentation for multi-LoRA

**Link**: https://github.com/vllm-project/vllm/issues/2603
**State**: closed
**Created**: 2024-01-25T17:30:23+00:00
**Closed**: 2024-02-12T16:24:46+00:00
**Comments**: 1

### Description

Add some documentation on how multi-LoRA works and how to use it.

---

## Issue #N/A: prefix caching error with baichuan model

**Link**: https://github.com/vllm-project/vllm/issues/2513
**State**: closed
**Created**: 2024-01-20T00:16:16+00:00
**Closed**: 2024-01-20T13:45:55+00:00
**Comments**: 0

### Description

## Machine Info
- Ubuntu
- cuda driver 12.1
- T4 GPU x4

## Reproduce Step

Only change llm line in `examples/offline_inference_with_prefix.py` with
```
llm = LLM(model="baichuan-inc/Baichuan2-13B-Chat", tensor_parallel_size=4, enforce_eager=True, dtype="half", trust_remote_code=True)
``` 

## Error message:
```
triton.runtime.autotuner.OutOfResources: out of resource: shared memory, Required: 65538, Hardware limit: 65536. Reducing block sizes or `num_stages` may help. 
```

When i use a common llama arch like `TInyllama` model, it's ok. Might related `Alibi`? 

@DouHappy @caoshiyi @zhuohan123  Please take a look!

---

## Issue #N/A: Can't load fine-tuned LLama2 model stored in local

**Link**: https://github.com/vllm-project/vllm/issues/1271
**State**: closed
**Created**: 2023-10-06T04:20:55+00:00
**Closed**: 2024-03-13T11:21:00+00:00
**Comments**: 1

### Description

I have a fine-tuned LLama2 model (LoRA Layers merged). Model folder contains following files:

- config.json
- generation_config.json
- model-00001-of-00002.safetensors 
- model-00002-of-00002.safetensors
- model.safetensors.index.json 
- special_tokens_map.json
- tokenizer_config.json 
- tokenizer.json

When I try to load the model it throws following error. Any idea on how to resolve this?

`llm = LLM(model="./llama-ft/merged_model")`


> ---------------------------------------------------------------------------
> AssertionError                            Traceback (most recent call last)
> Cell In[16], line 11
>       3 prompts = [
>       4     "Hello, my name is",
>       5     "The president of the United States is",
>       6     "The capital of France is",
>       7     "The future of AI is",
>       8 ]
>       9 sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
> ---> 11 llm = LLM(model="./llama-ft/merged_model")
> 
> File /opt/conda/l

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: When cuda_graph is enabled, RunTimeError:NCCL error is reported using nvidia-nccl-cu11

**Link**: https://github.com/vllm-project/vllm/issues/5679
**State**: closed
**Created**: 2024-06-19T08:27:17+00:00
**Closed**: 2024-11-25T02:05:39+00:00
**Comments**: 5
**Labels**: bug, stale

### Description

### Your current environment

1、
torch 2.3.0+cu118
vllm 0.4.3+cu118
2、
[root@master1 v2]# pip show torch
Name: torch
Version: 2.3.0+cu118
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: [packages@pytorch.org](mailto:packages@pytorch.org)
License: BSD-3
Location: /opt/anaconda3/envs/vllm4/lib/python3.10/site-packages
Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu11, nvidia-cuda-cupti-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, nvidia-cufft-cu11, nvidia-curand-cu11, nvidia-cusolver-cu11, nvidia-cusparse-cu11, nvidia-nccl-cu11, nvidia-nvtx-cu11, sympy, triton, typing-extensions
3、(vllm4) [root@master1 v2]# pip list |grep nccl
nvidia-nccl-cu11 2.20.5

### 🐛 Describe the bug

When cuda_graph is enabled, RunTimeError:NCCL error is reported using nvidia-nccl-cu11
pip install nvidia-nccl-cu12 is ok.

---

## Issue #N/A: [Usage]: slow inference for fine-tuned model

**Link**: https://github.com/vllm-project/vllm/issues/4243
**State**: closed
**Created**: 2024-04-21T20:07:55+00:00
**Closed**: 2024-11-29T02:06:41+00:00
**Comments**: 4
**Labels**: usage, stale

### Description

### Your current environment

I am using SFT of huggingface transformers to fine tune model mistral 7b. https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py

After fine-tuning, the model works well, by loading via langchain - vllm  (vllm version: 0.4.0)(https://python.langchain.com/docs/integrations/llms/vllm/), 
```
from langchain_community.llms import VLLM
llm = VLLM(
    model="mistralai/Mistral-7B-Instruct-v0.2", # or drop-in replacement of fine-tuned model with local path
    trust_remote_code=True,  # mandatory for hf models
    max_new_tokens=128,
    top_k=10,
    top_p=0.95,
    temperature=0.8,
)
```
However, the inference latency is round 2-3 times slower than the original "mistralai/Mistral-7B-Instruct-v0.2".
May I ask is there any specific settings that i have missed for applying vllm on SFT model ?
Thanks.

### How would you like to use vllm

run vllm on fine-tuned model.

---

## Issue #N/A: How to compile the `csrc` into an object file with `nvcc`?

**Link**: https://github.com/vllm-project/vllm/issues/1793
**State**: closed
**Created**: 2023-11-26T20:40:27+00:00
**Closed**: 2023-11-27T20:56:45+00:00
**Comments**: 9

### Description

Hello everybody,

I am trying to compile the `csrc` part of `vllm` into an object file (excluding the Python bindings). However, I cannot find a way to do this. This is the command I was using:
```
"nvcc" "-ccbin=c++" "-Xcompiler" "-O0" "-Xcompiler" "-ffunction-sections" "-Xcompiler" "-fdata-sections" "-Xcompiler" "-fPIC" "-G" "-Xcompiler" "-gdwarf-4" "-Xcompiler" "-fno-omit-frame-pointer" "-m64" "-Xcompiler" "-Wall" "-Xcompiler" "-Wextra" "-I /home/ericbuehler/.local/lib/python3.10/site-packages/torch/include" "-I /home/ericbuehler/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include" "-I /usr/include/python3.10" "-gencode" "arch=compute_62,code=sm_62" "-o" "/home/ericbuehler/candle-vllm/target/debug/build/candle-vllm-40c333732b7323b5/out/csrc/attention/attention_kernels.o" "-c" "csrc/attention/attention_kernels.cu"
```
It simply attempts to compile all `*.cu` files for a Jetson TX2 GPU (compute 6.2) as a test case, and provides paths to my libraries. However

[... truncated for brevity ...]

---

## Issue #N/A: [Misc]: How to call the paged_attention_v2 on my own q and kv caches?

**Link**: https://github.com/vllm-project/vllm/issues/3585
**State**: closed
**Created**: 2024-03-23T15:39:13+00:00
**Closed**: 2024-03-23T23:30:29+00:00
**Comments**: 0
**Labels**: misc

### Description

### Anything you want to discuss about vllm.

Hi, I am trying to use the `paged_attention_v2` function on my own data, qkv.

However, I find it is not giving the correct result. I test with the following script:

```
from typing import Optional
import argparse
import random
import time

import torch
from flash_attn import flash_attn_func
from vllm._C import ops, cache_ops
from vllm.utils import create_kv_caches_with_random

NUM_BLOCKS = 1024
BLOCK_SIZE = 32
PARTITION_SIZE = 512

torch.set_default_device('cuda:0')
torch.set_default_dtype(torch.float16)

def expand_heads(tensor, num_heads=32, num_heads_kv=8):
    assert tensor.dim() == 3
    _, length, dim_head = tensor.shape
    num_group = num_heads // num_heads_kv
    tensor = tensor.view((num_heads_kv, 1, length, dim_head))
    tensor = tensor.expand((num_heads_kv, num_group, length, dim_head)).reshape((num_heads, length, dim_head))
    return tensor


def make_qkv(len_k, num_head, num_head_kv, head_d

[... truncated for brevity ...]

---

## Issue #N/A: Add documents on how to add new models

**Link**: https://github.com/vllm-project/vllm/issues/65
**State**: closed
**Created**: 2023-05-04T09:05:56+00:00
**Closed**: 2023-06-06T03:01:28+00:00
**Comments**: 0
**Labels**: documentation

### Description

No description provided.

---

## Issue #N/A: Profile and optimize list operations in scheduling

**Link**: https://github.com/vllm-project/vllm/issues/1850
**State**: closed
**Created**: 2023-11-30T06:16:10+00:00
**Closed**: 2024-10-26T16:43:35+00:00
**Comments**: 3
**Labels**: performance

### Description

BTW, we are considering using Numpy to reduce the list operations. I believe this can be done in another PR.

_Originally posted by @WoosukKwon in https://github.com/vllm-project/vllm/pull/1843#discussion_r1410054638_
            

---

## Issue #N/A: Can the VLLM framework support Huawei's 910B chip in the later stage?

**Link**: https://github.com/vllm-project/vllm/issues/3052
**State**: closed
**Created**: 2024-02-27T08:49:03+00:00
**Closed**: 2025-01-19T02:02:09+00:00
**Comments**: 4
**Labels**: stale

### Description

Can the VLLM framework support Huawei's 910B chip in the later stage?

---

## Issue #N/A: [Usage]: I have two Gpus, how do I make my model run on 2 gpus

**Link**: https://github.com/vllm-project/vllm/issues/3908
**State**: closed
**Created**: 2024-04-08T02:32:44+00:00
**Closed**: 2024-04-20T00:04:36+00:00
**Comments**: 6
**Labels**: usage

### Description

### Your current environment

```text
python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-0.5B-Chat --model /home/project/models/qwen-0.5b
```


### How would you like to use vllm

I want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.


---

## Issue #N/A: How to set cache dir?

**Link**: https://github.com/vllm-project/vllm/issues/764
**State**: closed
**Created**: 2023-08-15T12:03:51+00:00
**Closed**: 2024-03-13T11:48:32+00:00
**Comments**: 7

### Description

How to set cache_dir for the model like huggingface AutoModel...

---

## Issue #N/A: T5 model support

**Link**: https://github.com/vllm-project/vllm/issues/404
**State**: closed
**Created**: 2023-07-08T20:07:00+00:00
**Closed**: 2024-03-06T08:58:35+00:00
**Comments**: 4
**Labels**: new-model

### Description

Any plans on adding T5 models?
FLAN-T5 are particularly interesting

---

## Issue #N/A: [Bug]: VLLM's output is unstable version==0.5.1

**Link**: https://github.com/vllm-project/vllm/issues/6328
**State**: closed
**Created**: 2024-07-11T09:37:53+00:00
**Closed**: 2024-11-24T02:08:29+00:00
**Comments**: 4
**Labels**: bug, stale

### Description

### Your current environment

using version==0.5.1 docker images:
command:
using model qwen2-GPTQ-Int4
docker run -it --rm --gpus  '"device=0,7,8,9"'  -p 8090:8090 -e NCCL_P2P_DISABLE=1 -e NCCL_SHM_DISABLE=1 -v /nfs2:/nfs2  -v /var:/var  -v /nfs3:/nfs3 -v /nfs5:/nfs5 --shm-size 20g XXXXXXXXX:XXXXXXX python3 -m vllm.entrypoints.openai.api_server --host=0.0.0.0 --port=8090 --model=XXXX/source/deps --served-model-name=qwen2-GPTQ-Int4 --gpu-memory-utilization 0.9 --tensor-parallel-size 4 --seed 42;


### 🐛 Describe the bug

I generated 50 results by using http://XXXX/v1/chat/completions with params like:
```markdown
   "model": "qwen2-GPTQ-Int4",
    "temperature": 0,
    "n": 1,
    "best_of":1,
    "presence_penalty":0.0,
    "frequency_penalty":0.0,
    "repetition_penalty":1.0,
    "top_p": 1.0,
    "top_k":1.0,
    "min_p":0.0
```
**But I got different results eventhough most of them are the same(>90% are same)**， .
Results LIKE:
['P41T33', 'P76T139', 'P76T140', 'P7

[... truncated for brevity ...]

---

## Issue #N/A: (not planned)

**Link**: https://github.com/vllm-project/vllm/issues/6518
**State**: closed
**Created**: 2024-07-17T20:18:52+00:00
**Closed**: 2024-07-17T20:21:29+00:00
**Comments**: 0
**Labels**: RFC

### Description

(not planned)

---

## Issue #N/A: [Bug]: NameError: name 'vllm_ops' is not defined

**Link**: https://github.com/vllm-project/vllm/issues/4229
**State**: closed
**Created**: 2024-04-21T00:43:20+00:00
**Closed**: 2024-05-31T07:07:46+00:00
**Comments**: 13
**Labels**: bug

### Description

### Your current environment

vllm version: 0.4.0.post1



### 🐛 Describe the bug

`CUDA_VISIBLE_DEVICES="4"  python -u -m vllm.entrypoints.openai.api_server     --model mistralai/Mistral-7B-Instruct-v0.2     --dtype auto --api-key yanan     --tensor-parallel-size 1 --port 1703 --host 0.0.0.0     --worker-use-ray --gpu-memory-utilization 1`


error:

> INFO 04-20 20:39:58 api_server.py:149] vLLM API server version 0.4.1
> INFO 04-20 20:39:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=1703, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='yanan', served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remo

[... truncated for brevity ...]

---

## Issue #N/A: [Bug] FP8 MoE performance regression since updating to torch 2.3.0

**Link**: https://github.com/vllm-project/vllm/issues/4509
**State**: closed
**Created**: 2024-05-01T00:32:49+00:00
**Closed**: 2024-07-12T19:04:27+00:00
**Comments**: 3
**Labels**: bug

### Description

### Your current environment

H100 80GB, torch 2.3.0


### 🐛 Describe the bug

Since https://github.com/vllm-project/vllm/pull/4454, the performance on the FP8 MoE kernel has been pretty bad. On FP8, for qps 6 (1000 input, 50 output tokens) and static activation scales on H100, the mixtral 8x7b ITL goes from 12ms to over 40ms.

---


# very_short_under50chars - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 0
- Closed Issues: 30

### Label Distribution

- stale: 5 issues
- feature request: 2 issues
- bug: 2 issues
- new-model: 1 issues
- documentation: 1 issues
- misc: 1 issues
- installation: 1 issues
- RFC: 1 issues

---

## Issue #N/A: why not support baichuan-7b?

**Link**: https://github.com/vllm-project/vllm/issues/303
**State**: closed
**Created**: 2023-06-29T03:02:32+00:00
**Closed**: 2023-07-17T20:50:57+00:00
**Comments**: 2
**Labels**: new-model

### Description

No description provided.

---

## Issue #N/A: Offline Batched Inference with lora?

**Link**: https://github.com/vllm-project/vllm/issues/3001
**State**: closed
**Created**: 2024-02-23T02:15:48+00:00
**Closed**: 2024-02-27T21:57:14+00:00
**Comments**: 2

### Description

No description provided.

---

## Issue #N/A: Ask the boss: Is there any parameter in vllm that can be passed in history

**Link**: https://github.com/vllm-project/vllm/issues/708
**State**: closed
**Created**: 2023-08-09T00:50:00+00:00
**Closed**: 2023-08-25T08:33:49+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: 8

**Link**: https://github.com/vllm-project/vllm/issues/3022
**State**: closed
**Created**: 2024-02-24T15:20:50+00:00
**Closed**: 2024-02-24T21:13:38+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: When can I support multi graphics cards?

**Link**: https://github.com/vllm-project/vllm/issues/228
**State**: closed
**Created**: 2023-06-24T07:51:02+00:00
**Closed**: 2023-06-25T17:22:46+00:00
**Comments**: 1

### Description

When can I support multi graphics cards?

---

## Issue #N/A: Add documents on how to add new models

**Link**: https://github.com/vllm-project/vllm/issues/65
**State**: closed
**Created**: 2023-05-04T09:05:56+00:00
**Closed**: 2023-06-06T03:01:28+00:00
**Comments**: 0
**Labels**: documentation

### Description

No description provided.

---

## Issue #N/A: Add code formatting script & Add CI to check code format

**Link**: https://github.com/vllm-project/vllm/issues/57
**State**: closed
**Created**: 2023-05-03T02:58:12+00:00
**Closed**: 2023-07-03T21:50:58+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: how to usevllm

**Link**: https://github.com/vllm-project/vllm/issues/2541
**State**: closed
**Created**: 2024-01-22T06:47:01+00:00
**Closed**: 2024-04-04T08:00:31+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A:  how to add history argument？

**Link**: https://github.com/vllm-project/vllm/issues/960
**State**: closed
**Created**: 2023-09-06T06:41:00+00:00
**Closed**: 2024-03-13T11:37:52+00:00
**Comments**: 2

### Description

when use api_client.py？

---

## Issue #N/A: Support `logprobs` for `ChatCompletionRequest` in openai api server?

**Link**: https://github.com/vllm-project/vllm/issues/2276
**State**: closed
**Created**: 2023-12-27T01:51:50+00:00
**Closed**: 2024-02-28T00:42:41+00:00
**Comments**: 2

### Description

No description provided.

---

## Issue #N/A: [Feature]: Does vLLM support ONNX models?

**Link**: https://github.com/vllm-project/vllm/issues/9112
**State**: closed
**Created**: 2024-10-07T00:35:40+00:00
**Closed**: 2025-03-20T02:04:03+00:00
**Comments**: 3
**Labels**: feature request, stale

### Description

No description provided.

---

## Issue #N/A: Clean up Megatron-LM code

**Link**: https://github.com/vllm-project/vllm/issues/77
**State**: closed
**Created**: 2023-05-06T05:03:02+00:00
**Closed**: 2023-05-20T15:11:36+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: indexSelectLargeIndex: block: [308,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.

**Link**: https://github.com/vllm-project/vllm/issues/473
**State**: closed
**Created**: 2023-07-16T04:48:20+00:00
**Closed**: 2024-03-08T10:22:53+00:00
**Comments**: 2

### Description

No description provided.

---

## Issue #N/A: KeyError: 'base_model.model.lm_head.base_layer.weight'

**Link**: https://github.com/vllm-project/vllm/issues/2626
**State**: closed
**Created**: 2024-01-27T17:03:10+00:00
**Closed**: 2024-11-30T02:02:50+00:00
**Comments**: 3
**Labels**: stale

### Description

Please help me solve this error

---

## Issue #N/A: cuda11.8 for v0.2.4

**Link**: https://github.com/vllm-project/vllm/issues/2047
**State**: closed
**Created**: 2023-12-12T03:54:54+00:00
**Closed**: 2023-12-12T08:51:55+00:00
**Comments**: 4

### Description

Hi, there is no cuda11.8 wheel in releases.

---

## Issue #N/A: [Feature Request] Support to process both prompts and generation in one inference step

**Link**: https://github.com/vllm-project/vllm/issues/1468
**State**: closed
**Created**: 2023-10-25T08:48:42+00:00
**Closed**: 2024-03-13T11:49:42+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: v100 support int4 （gptq or awq）, Whether it really work?

**Link**: https://github.com/vllm-project/vllm/issues/3141
**State**: closed
**Created**: 2024-03-01T10:08:34+00:00
**Closed**: 2024-12-01T02:15:21+00:00
**Comments**: 2
**Labels**: stale

### Description

No description provided.

---

## Issue #N/A: The AsyncLLMEngine always cache KV

**Link**: https://github.com/vllm-project/vllm/issues/889
**State**: closed
**Created**: 2023-08-28T04:43:58+00:00
**Closed**: 2023-08-28T06:38:05+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: The default GPU number used is cuda:0, how do I specify the gpu number to use? For example cuda:2, I didn't find anything to change

**Link**: https://github.com/vllm-project/vllm/issues/1827
**State**: closed
**Created**: 2023-11-29T07:19:07+00:00
**Closed**: 2024-03-25T09:57:05+00:00
**Comments**: 2

### Description

No description provided.

---

## Issue #N/A: how to set rope_scaling type to dynamic in vllm?

**Link**: https://github.com/vllm-project/vllm/issues/910
**State**: closed
**Created**: 2023-08-30T09:00:43+00:00
**Closed**: 2024-03-09T09:31:12+00:00
**Comments**: 4
**Labels**: feature request

### Description

No description provided.

---

## Issue #N/A: about the RotaryEmbedding

**Link**: https://github.com/vllm-project/vllm/issues/5984
**State**: closed
**Created**: 2024-06-29T09:28:09+00:00
**Closed**: 2024-07-01T06:26:31+00:00
**Comments**: 0
**Labels**: misc

### Description

No description provided.

---

## Issue #N/A: [Installation]: Is there no more "***.whl" based on cuda12?

**Link**: https://github.com/vllm-project/vllm/issues/8222
**State**: closed
**Created**: 2024-09-06T05:19:58+00:00
**Closed**: 2025-01-06T02:02:39+00:00
**Comments**: 2
**Labels**: installation, stale

### Description

rt

---

## Issue #N/A: 求问 qwen-14b微调后的模型用vllm推理后结果都为空 

**Link**: https://github.com/vllm-project/vllm/issues/2981
**State**: closed
**Created**: 2024-02-22T07:49:32+00:00
**Closed**: 2024-11-29T02:08:15+00:00
**Comments**: 3
**Labels**: stale

### Description

No description provided.

---

## Issue #N/A: (not planned)

**Link**: https://github.com/vllm-project/vllm/issues/6518
**State**: closed
**Created**: 2024-07-17T20:18:52+00:00
**Closed**: 2024-07-17T20:21:29+00:00
**Comments**: 0
**Labels**: RFC

### Description

(not planned)

---

## Issue #N/A: Mamba 2 inference

**Link**: https://github.com/vllm-project/vllm/issues/9635
**State**: closed
**Created**: 2024-10-23T21:51:24+00:00
**Closed**: 2024-10-23T21:54:04+00:00
**Comments**: 0
**Labels**: bug

### Description

No description provided.

---

## Issue #N/A: [Bug]: run v1 engine with cuda graph. raise error.

**Link**: https://github.com/vllm-project/vllm/issues/14121
**State**: closed
**Created**: 2025-03-03T07:25:18+00:00
**Closed**: 2025-03-11T08:15:45+00:00
**Comments**: 7
**Labels**: bug

### Description

No description provided.

---

## Issue #N/A: How to use vllm to calculate the number of tokens

**Link**: https://github.com/vllm-project/vllm/issues/1521
**State**: closed
**Created**: 2023-10-31T11:25:40+00:00
**Closed**: 2023-10-31T20:00:28+00:00
**Comments**: 5

### Description

How to use vllm to calculate the number of tokens

---

## Issue #N/A: Why don't use flash-attn for prompt's attention? 

**Link**: https://github.com/vllm-project/vllm/issues/1170
**State**: closed
**Created**: 2023-09-25T02:16:06+00:00
**Closed**: 2023-09-27T23:43:58+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: Error from "Build from source"

**Link**: https://github.com/vllm-project/vllm/issues/1772
**State**: closed
**Created**: 2023-11-24T07:32:31+00:00
**Closed**: 2023-11-24T07:41:12+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: How can docker images use the local model in an Intranet environment because I don't have access to HuggingFace ?

**Link**: https://github.com/vllm-project/vllm/issues/2478
**State**: closed
**Created**: 2024-01-18T03:27:55+00:00
**Closed**: 2024-01-31T07:01:48+00:00
**Comments**: 1

### Description

No description provided.

---


# stale_but_active - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 30
- Closed Issues: 0

### Label Distribution

- stale: 26 issues
- bug: 16 issues
- usage: 7 issues
- feature request: 5 issues
- unstale: 4 issues
- documentation: 1 issues

---

## Issue #N/A: [Bug]: Disaggregated Prefilling use different TP between prefill instance and decode instance , it will be hanged

**Link**: https://github.com/vllm-project/vllm/issues/14952
**State**: open
**Created**: 2025-03-17T12:01:32+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>I changed disagg_performance_benchmark.sh as flowing</summary>

```text
launch_disagg_prefill() {
  model="$MODEL_PATH" 
  # disagg prefill
  CUDA_VISIBLE_DEVICES=0 python3 \
    -m vllm.entrypoints.openai.api_server \
    --model $model \
    --port 8100 \
    --max-model-len 10000 \
    --tensor-parallel-size 1 \
    --dtype=half \
    --gpu-memory-utilization 0.6 \
    --kv-transfer-config \
    '{"kv_connector":"PyNcclConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2,"kv_buffer_size":5e9}' &

  CUDA_VISIBLE_DEVICES=1,2 python3 \
    -m vllm.entrypoints.openai.api_server \
    --model $model \
    --port 8200 \
    --max-model-len 10000 \
    --tensor-parallel-size 2 \
    --dtype=half \
    --gpu-memory-utilization 0.6 \
    --kv-transfer-config \
    '{"kv_connector":"PyNcclConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2,"kv_buffer_size":5e9}' &

  wait_for_server 8100
  wait_for_server 8200
  p

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]:  topk=1 and temperature=0 cause different output in vllm

**Link**: https://github.com/vllm-project/vllm/issues/5404
**State**: open
**Created**: 2024-06-11T03:00:51+00:00
**Comments**: 27
**Labels**: bug, stale

### Description


### üêõ Describe the bug

When using different generation configurations, such as top_k=1 or temperature=0 (while keeping other settings unchanged), why do the generated results change? They should both correspond to a deterministic greedy decoding.
vllm 0.4.3

---
Supplement:

The main issue encountered here is that the results generated by setting the temperature coefficient to 0 or topk to 1 are different. I understand that due to operator optimization and the lack of conventional arithmetic properties in floating-point numbers, matrix operations have a certain randomness. However, the sampling process occurs after the hidden_state is generated, at which point no calculations are involved. Therefore, the sampling results of the two sampling parameters should be the same.

---

## Issue #N/A: [Feature]: Enhance integration with advanced LB/gateways with better load/cost reporting and LoRA management

**Link**: https://github.com/vllm-project/vllm/issues/10086
**State**: open
**Created**: 2024-11-06T17:59:22+00:00
**Comments**: 7
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

There are huge potential in more advanced load balancing strategies tailored for the unique characteristics of AI inference, compared to basic strategies such as round robin. [llm instance gateway](https://github.com/kubernetes-sigs/llm-instance-gateway) is one of such efforts and is demonstrating huge [performance wins](https://docs.google.com/document/d/11ALHEF-9yOaLdbHbDjBoTY6fzejoEKiSYHzWpWqe8ZY/edit?tab=t.0).  vLLM can demonstrate leadership in this space by providing  better integration with advanced LBs/gateways.

[This doc](https://docs.google.com/document/d/18VRJ2ufZmAwBZ2jArfvGjQGaWtsQtAP6_yF2Xn6zcms/edit?tab=t.0#heading=h.sw2xdf66jh6) captures the overall requirements for model servers to better support the llm instance gateway. Luckily vLLM already has lots of features/metrics that enable more efficient load balancing such as exposing the KVCacheUtilization metric. 

This is a high level breakdown of the feature requests:

### 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Compliing for CPU fails due to wrong python setup call

**Link**: https://github.com/vllm-project/vllm/issues/15953
**State**: open
**Created**: 2025-04-02T16:20:40+00:00
**Comments**: 6
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
INFO 04-02 18:13:38 [__init__.py:239] Automatically detected platform cpu.
Collecting environment information...
PyTorch version: 2.6.0+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Arch Linux (x86_64)
GCC version: (GCC) 14.2.1 20250207
Clang version: 19.1.7
CMake version: version 4.0.0
Libc version: glibc-2.41

Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.13.8-arch1-1-x86_64-with-glibc2.41
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architektur:                          x86_64
CPU Operationsmodus:                  32-bit, 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already.

**Link**: https://github.com/vllm-project/vllm/issues/5060
**State**: open
**Created**: 2024-05-26T22:44:41+00:00
**Comments**: 45
**Labels**: bug, stale

### Description

### Your current environment

docker image: vllm/vllm-openai:0.4.2
Model: https://huggingface.co/alpindale/c4ai-command-r-plus-GPTQ
GPUs: RTX8000 * 2

### üêõ Describe the bug

The model works fine until the following error is raised. 
-------------------------------------------------------


INFO 05-26 22:28:18 async_llm_engine.py:529] Received request cmpl-10dff83cb4b6422ba8c64213942a7e46: prompt: '<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>"Question: Is Korea the name of a Nation?\nGuideline: No explanation.\nFormat: {"Answer": "<your yes/no answer>"}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['---'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, log

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: KeyError: 'local_attn_masks' on running gemma3 models with kv-cache quantization

**Link**: https://github.com/vllm-project/vllm/issues/16061
**State**: open
**Created**: 2025-04-04T13:46:15+00:00
**Comments**: 1
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Collecting environment information...                                                                                               
PyTorch version: 2.6.0+cu124                                                                                                        
Is debug build: False                                                                                                               
CUDA used to build PyTorch: 12.4                                                                                                    
ROCM used to build PyTorch: N/A                                                                                                     
                                                                                                                                    
OS: Ubuntu 22.04.4 LTS (x86_64)                                                                 

[... truncated for brevity ...]

---

## Issue #N/A: DeciLMConfig object has no attribute ‚Äònum_key_value_heads_per_layer‚Äô 

**Link**: https://github.com/vllm-project/vllm/issues/15625
**State**: open
**Created**: 2025-03-27T14:54:54+00:00
**Comments**: 5
**Labels**: bug, stale

### Description

### Your current environment

VLLM version: 0.7.3
Model: nvidia/Llama-3_3-Nemotron-Super-49B-v1


### üêõ Describe the bug

I am trying to run the new Nvidia model Nemotron 49B-v1 using the VLLM 0.7.3 version but I got this error 
DeciLMConfig object has no attribute ‚Äònum_key_value_heads_per_layer‚Äô

I have two questions: I know there are PR such as 
https://github.com/vllm-project/vllm/issues/15068
https://github.com/vllm-project/vllm/pull/15008

I am wondering about if the error would be resolved after adding the support for the model. Or the error is unrelated and something is wrong in my end?


---

## Issue #N/A: ExLlamaV2: exl2 support

**Link**: https://github.com/vllm-project/vllm/issues/3203
**State**: open
**Created**: 2024-03-05T14:54:03+00:00
**Comments**: 38
**Labels**: feature request, stale

### Description

If is possible ExLlamaV2 is a very fast and good library to Run [LLM](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html)

[ExLlamaV2 Repo](https://github.com/turboderp/exllamav2)

---

## Issue #N/A: [Usage]: Logprobs Scaling with O(n) Complexity ‚Äì Unexpected Performance Degradation

**Link**: https://github.com/vllm-project/vllm/issues/14300
**State**: open
**Created**: 2025-03-05T17:47:25+00:00
**Comments**: 3
**Labels**: usage, unstale

### Description

**Title:** Logprobs Scaling with O(n) Complexity ‚Äì Unexpected Performance Degradation  

**Description:**  
When increasing the `logprobs` parameter, I expected only a minor increase in runtime due to slicing the top-k values from the full vocabulary logits. However, my experiments show an almost O(n) increase in runtime, which suggests that retrieving logprobs is more computationally expensive than anticipated.  

### **Reproduction Code**  
```python
import time
from vllm import LLM
from vllm.sampling_params import SamplingParams

def test_generation_time(llm, logprobs_value, batch_size=32):
    sampling_params = SamplingParams(logprobs=logprobs_value, max_tokens=1)
    
    # Timed run
    start_time = time.time()
    output = llm.generate(["Tell me something about LLMs."] * batch_size,
                         sampling_params=sampling_params,
                         use_tqdm=False)
    end_time = time.time()
    
    return end_time - start_time

def main():
    print("Initializin

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: `Phi-4-multimodal-instruct` encoder outputs didn't have the same length as defined in input_ids

**Link**: https://github.com/vllm-project/vllm/issues/15404
**State**: open
**Created**: 2025-03-24T15:32:07+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
INFO 03-24 14:46:21 [__init__.py:256] Automatically detected platform cuda.
Collecting environment information...
PyTorch version: 2.6.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.23.0
Libc version: glibc-2.31

Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-162-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090
GPU 4: NVIDIA GeForce RTX 4090
GPU 5: NVIDIA GeForce RTX 4090


[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: The service request for vllm064post1 was prematurely terminated, and it could not output a fixed number of tokens.‚Äù

**Link**: https://github.com/vllm-project/vllm/issues/13156
**State**: open
**Created**: 2025-02-12T12:44:50+00:00
**Comments**: 2
**Labels**: usage, stale

### Description

### Your current environment

Using vllm064 post1 and running a benchmark test, with the ignore_eos setting set to 1, for fixed-length input and output testing (input 20000, output 256, qps=2).
I found that regardless of which ASYNC_REQUEST_FUNCS ("vllm": async_request_openai_completions, or "openai-chat": async_request_openai_chat_completions) is used, the actual output token count, which is the length of the output.itl list, does not match the preset value (max_tokens).

Especially when the endpoint is /v1/completions, the length of len(output.itl) is often quite low.
Please let me know how to resolve this. Is it an issue with my settings?

For example:

Using the /v1/chat/completions endpoint

![Image](https://github.com/user-attachments/assets/b7b03752-587d-41cd-9bc4-de72a58f903c)

Using the /v1/completions endpoint

![Image](https://github.com/user-attachments/assets/447a1042-f834-431f-ba70-69c76f11a1de)

I printed the data structure in ‚Äúbackend_request_func.py‚Äù below.

![Image](h

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture

**Link**: https://github.com/vllm-project/vllm/issues/13446
**State**: open
**Created**: 2025-02-18T02:39:11+00:00
**Comments**: 20
**Labels**: usage, unstale

### Description

### Your current environment

```
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py", line 225, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py", line 1073, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.

You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transfo

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: flash_attn_with_kvcache kernel, an illegal memory access

**Link**: https://github.com/vllm-project/vllm/issues/15113
**State**: open
**Created**: 2025-03-19T08:47:10+00:00
**Comments**: 4
**Labels**: bug, stale

### Description

### Your current environment

<details>
flash_attn_with_kvcache kernel flash::copy illegal memory access


</details>


### üêõ Describe the bug

flash_attn_with_kvcache kernel flash::copy illegal memory access

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: Add task perplexity mode to optimize PPL evaluation

**Link**: https://github.com/vllm-project/vllm/issues/16324
**State**: open
**Created**: 2025-04-09T07:20:43+00:00
**Comments**: 1
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

When testing 2/3/4-bit quantized LLaMA 2 models (7B/13B/70B) on a single A40 GPU using `lm-evaluation-harness`, I ran into issues with `generate` mode for perplexity (PPL) tasks:

1. PPL evaluation typically uses `generate` mode with `prompt_log_probs=0` and `max_tokens=1`.
2. The current implementation reuses the response log-prob calculation for prompt log-probs. However, the number of new response tokens (‚âà number of requests) is much smaller than the total number of prompt tokens. Since response log-probs are computed assuming short outputs, the code uses a memory-heavy approach that easily leads to OOM when applied to long prompts ‚Äî requiring reduced `gpu_memory_utilization`. 
see https://github.com/vllm-project/vllm/blob/24f6b9a71397539a3d02c801963220b0e9a2aef9/vllm/model_executor/layers/sampler.py#L268-L284
3. Lowering `gpu_memory_utilization` introduces another issue: it keeps KV cache around, causing errors. (This can be mitigated by lo

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: qwen2.5-vl inference truncated

**Link**: https://github.com/vllm-project/vllm/issues/16763
**State**: open
**Created**: 2025-04-17T07:18:42+00:00
**Comments**: 13
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Your output of `python collect_env.py` here
```

Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: BigCloud Enterprise Linux 7.8 (Core) (x86_64)
GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.1-3)
Clang version: Could not collect
CMake version: version 3.31.6
Libc version: glibc-2.28

Python version: 3.12.10 | packaged by conda-forge | (main, Apr 10 2025, 22:21:13) [GCC 13.3.0] (64-bit runtime)
Python platform: Linux-3.10.0-1160.119.1.el7.x86_64-x86_64-with-glibc2.28
Is CUDA available: True
CUDA runtime version: 12.2.91
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: Tesla V100S-PCIE-32GB
GPU 1: Tesla V100S-PCIE-32GB
GPU 2: Tesla V100S-PCIE-32GB
GPU 3: Tesla V100S-PCIE-32GB

Nvidia driver version: 535.230.02
cuDNN version: /usr/local/cuda-12.2/targets/x

[... truncated for brevity ...]

---

## Issue #N/A: Issue with Mistral Small and greek characters

**Link**: https://github.com/vllm-project/vllm/issues/14307
**State**: open
**Created**: 2025-03-05T19:58:32+00:00
**Comments**: 2
**Labels**: usage, unstale

### Description

### Your current environment

```text

--model mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice

```


### How would you like to use vllm

I run inference for the Mistral Small model with the parameters defined before i have issues with greek characters on llm responses.some of them are clear some are unkown characters.Using the same model with ollama greek is perfect without any issues. Can i use some other parameter for my usecase when inferencing with vllm?


### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: Soft Prompts?

**Link**: https://github.com/vllm-project/vllm/issues/16351
**State**: open
**Created**: 2025-04-09T15:04:12+00:00
**Comments**: 2
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

Soft prompts would be similar to Loras in that they are artifacts from a parameter efficient training process that live on the server where vLLM is running. But unlike Loras, which require being set or unset as a global state that affects all requests, soft prompts could be part of the `generate` call. This would allow a single vLLM server to respond to many different fine tuned tasks

I see someone tried something a long time ago but abandoned it. (PR #815 https://github.com/vllm-project/vllm/pull/815)

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Bug]: AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'num_attention_heads'

**Link**: https://github.com/vllm-project/vllm/issues/16645
**State**: open
**Created**: 2025-04-15T07:48:19+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

just look here:
[https://github.com/huggingface/transformers/issues/37515#issuecomment-2804126324](url)

### üêõ Describe the bug

`System Info
root@445d74596699:/vllm-workspace# transformers-cli env

Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.

transformers version: 4.52.0.dev0
Platform: Linux-5.15.0-43-generic-x86_64-with-glibc2.35
Python version: 3.12.9
Huggingface_hub version: 0.30.2
Safetensors version: 0.5.3
Accelerate version: 1.5.2
Accelerate config: not found
DeepSpeed version: not installed
PyTorch version (GPU?): 2.6.0+cu124 (True)
Tensorflow version (GPU?): not installed (NA)
Flax version (CPU?/GPU?/TPU?): not installed (NA)
Jax version: not installed
JaxLib version: not installed
Using distributed or parallel set-up in script?:
Using GPU in script?:
GPU type: NVIDIA L20
`(base) root@node15:/disk2/Qwen2.5-Omni-7B# more docker-compose.yml
#version: '3.3'
services:

vllm
vllm-openai:
image: vllm/vllm-openai:

[... truncated for brevity ...]

---

## Issue #N/A: Model and GPU memory was cleared after canceling the run in version 0.8.2

**Link**: https://github.com/vllm-project/vllm/issues/15771
**State**: open
**Created**: 2025-03-30T11:47:29+00:00
**Comments**: 1
**Labels**: bug, stale

### Description

### Your current environment

Most updated VLLM version: 0.8.2

### üêõ Describe the bug

Problem with 0.8.2. This time a very strange error: I was running simulation for 100 iterations, cancel the process after 20 rounds and let it run again, it was not doing anything, then I check the GPU usage and I saw 0% along with 0 GB is used from GPU memory. 
Very strange to see you can not cancel the run otherwise is like deleting your model. 
This problem does not happen in 0.7.3.
I wonder what went wrong in 0.8.2 that caused this. 
If you cancel your run the you need start the session from scratch and load your model, etc. all over again.

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: K-cache only

**Link**: https://github.com/vllm-project/vllm/issues/15679
**State**: open
**Created**: 2025-03-28T06:37:02+00:00
**Comments**: 1
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

from the paper [Slim attention](https://arxiv.org/pdf/2503.05840), it clams that can reduce nearly 1/2 memory, so I think is more friendly to PC.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Bug][Triton MLA]: Some calculation errors about triton mla kernal

**Link**: https://github.com/vllm-project/vllm/issues/15530
**State**: open
**Created**: 2025-03-26T08:08:11+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

None


### üêõ Describe the bug

I integrated the **triton mla kernel** of vllm and found that when the batch is larger than 1, the last sequence sometimes cannot get the correct result. I have done some experiments on simpler cases, such as kvcache is all 0, and the ideal output is also all 0, but the last sequence will get all nan. Hope to receive a response, thanks.


### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [V1] [Performance Benchmark] Benchmark the performance of Speculative Decoding

**Link**: https://github.com/vllm-project/vllm/issues/15600
**State**: open
**Created**: 2025-03-27T06:23:12+00:00
**Comments**: 15
**Labels**: stale

### Description

1. Let's start with ngram, can you collect both latency and throughput numbers on ShareGPT dataset on H100 and one low end GPU?
2. If the numbers from 1 is not expected, could you run some profiling to understand the performance bottleneck.
3. Get more performance numbers on other datasets. 

---

## Issue #N/A: [Usage]: VLLM 0.7.3 with tensor parallelism outputs only exclamation marks when using multiple GPUs

**Link**: https://github.com/vllm-project/vllm/issues/15194
**State**: open
**Created**: 2025-03-20T07:34:28+00:00
**Comments**: 2
**Labels**: usage, stale

### Description

## Environment
- OS: Ubuntu 22.04
- GPUs: 2x NVIDIA L20 (49GB each)
- VLLM version: 0.7.3
- CUDA version: 12.4.131
- Driver version: 535.161.08
- Model: QwQ-32B-AWQ (AWQ quantized model)

## Problem Description
When running VLLM with tensor parallelism across two GPUs, the model sometimes outputs only exclamation marks (`!`) instead of proper text. This issue only occurs with multiple GPUs and appears to be related to concurrent requests - single GPU deployment works fine.

The problem is consistently reproducible when sending concurrent requests with the same prompt to the API endpoint, but non-concurrent requests sometimes produce normal responses.

## Steps to Reproduce
1. Start VLLM server with tensor parallelism:
```bash
vllm serve /root/data/models/QwQ-32B-AWQ --api-key dev-key --gpu-memory-utilization 0.9 --tensor-parallel-size 2 --quantization awq --host 0.0.0.0 --port 8877 --served-model-name qwq
```

2. Send multiple concurrent requests using curl:
```bash
curl -X POST "http:

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Qwen2-VL-2B quantization model has no improvement in reasoning speed compared to the original model

**Link**: https://github.com/vllm-project/vllm/issues/15601
**State**: open
**Created**: 2025-03-27T06:43:06+00:00
**Comments**: 11
**Labels**: bug, stale

### Description

### Your current environment

I performed GPTQ-int8 quantization using the fine-tuned Qwen2-VL-2B model, and there was no improvement in model inference speed after quantization

### üêõ Describe the bug

I performed GPTQ-int8 quantization using the fine-tuned Qwen2-VL-2B model, and there was no improvement in model inference speed after quantization

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Usage]: Asking for help: vllm0.7.2 deploy DeepSeek-R1-int4-gptq-sym-inc

**Link**: https://github.com/vllm-project/vllm/issues/16111
**State**: open
**Created**: 2025-04-06T03:17:09+00:00
**Comments**: 2
**Labels**: usage, stale

### Description

### Your current environment

- version: vllm0.7.2, python3.10
- devices: 8√óH800 80G

### How would you like to use vllm

I want to do INT4 Inference on CUDA but both offline and online failed, why?

- model:  [OPEA/DeepSeek-R1-int4-gptq-sym-inc](https://huggingface.co/OPEA/DeepSeek-R1-int4-gptq-sym-inc) follows the standard GPTQ format.

- Offline inference: I used the recommended setting moe_wna16, but even after reducing the gpu_memory_utilization, I still encountered an OOM  error at the very beginning. Additionally, moe_wna16 disables MLA (Model Linear Attention), and I'm not sure if this has any impact on performance.

- Online serving: The vllm server can run normally, and I can access it using openai client. (examples/online_serving/openai_completion_client.py). However, when I use client.chat.completions.create, it times out. When I use client.completions.create, I do get a response, but the output text is garbled and unreadable.

The codes and logs are below.

### 1. offline 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: awq Deepseek-R1-AWQ  The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.

**Link**: https://github.com/vllm-project/vllm/issues/15386
**State**: open
**Created**: 2025-03-24T09:55:46+00:00
**Comments**: 3
**Labels**: bug, stale

### Description

### Your current environment


![Image](https://github.com/user-attachments/assets/231880dd-5827-4f65-9ca8-9aa4de0a612f)
vllm==0.8.1 0.8.2 0.8.3.dev5+g5797fb97.precompiled
https://huggingface.co/cognitivecomputations/DeepSeek-R1-AWQ  Concurrent use will result in the error message shown in the figure. 

### üêõ Describe the bug

The following is the execution script
model_path=/home/ds-r1/models/DeepSeek-R1-awq
vllm serve $model_path \
    --port 23344 \
    --host 0.0.0.0 \
    --tensor-parallel-size 8  \
    --max-model-len 16000 \
    --dtype bfloat16 \
    --gpu-memory-utilization 0.9 \
    --trust-remote-code \
    --served-model-name deepseek-r1 \

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Bug]: Bad result when serving extracted Gemma-3-4b-it-language model in vLLM

**Link**: https://github.com/vllm-project/vllm/issues/16360
**State**: open
**Created**: 2025-04-09T19:45:38+00:00
**Comments**: 3
**Labels**: bug, stale

### Description

### Your current environment

### Environment
Docker image: hiyouga/verl:ngc-th2.6.0-cu120-vllm0.8.2-verl0.3.0.post1

### üêõ Describe the bug

### Problem Description
I've encountered an issue when using vLLM to serve the language model component extracted from gemma-3-4b-it. While the original gemma-3-4b-it model generates normal results with vLLM, the extracted language model (gemma-3-4b-it-language) produces abnormal outputs.

<img width="1134" alt="Image" src="https://github.com/user-attachments/assets/2c11f999-023c-4adc-923d-94c507132b3d" />

### Steps to reproduce
1. Extract the language model from Gemma-3-4b-it:
<img width="730" alt="Image" src="https://github.com/user-attachments/assets/d32f5a83-12f7-4a45-90b0-b3657e3c5344" />
  <br><br>
2. Verify the extracted model works with Transformers:
<img width="729" alt="Image" src="https://github.com/user-attachments/assets/d964cdf0-a808-4134-a806-6c8d8e4d9258" />
This works correctly, producing the expected responseÔºà"<bos><bos><start_

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: GPU memory usage gradually increases.

**Link**: https://github.com/vllm-project/vllm/issues/12610
**State**: open
**Created**: 2025-01-31T13:48:33+00:00
**Comments**: 6
**Labels**: bug, stale

### Description

### Your current environment

[Environment]
```
INFO 01-31 21:44:15 __init__.py:183] Automatically detected platform cuda.
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.30.5
Libc version: glibc-2.35

Python version: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-25-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.131
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA H800 PCIe
Nvidia driver version: 550.54.14
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: Does vllm support to deploy one model on multiple type GPUs(e.g. one is A100, the other is H20)?

**Link**: https://github.com/vllm-project/vllm/issues/13760
**State**: open
**Created**: 2025-02-24T13:07:29+00:00
**Comments**: 2
**Labels**: usage, unstale

### Description

### Your current environment

```text
Does vllm support to deploy one model on multiple type GPUs(e.g. one is A100, the other is H20, use vllm to deploy one model on the above two GPUs)?
```


### How would you like to use vllm

If i have two GPUs, one is A100, the other is H20, i want to use vllm to deploy one model on the above two GPUs. Does vllm support this?

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Doc]: Why is max block_size on CUDA 32?

**Link**: https://github.com/vllm-project/vllm/issues/14319
**State**: open
**Created**: 2025-03-05T23:50:23+00:00
**Comments**: 4
**Labels**: documentation, stale

### Description

### üìö The doc issue

In the args:
https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py#L454
it says about block_size parameter:

> Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to --max-model-len. On CUDA devices, only block sizes up to 32 are supported. On HPU devices, block size defaults to 128.

1. Where is this requirement for <= 32 on CUDA devices coming from?
2. I was able to successfully run vLLM with block_size 128 on Hopper and see some minor performance improvement. Is the requirement up to date?
3. In flash attention docs I see that paged attention minimum block size is actually 256:
https://github.com/Dao-AILab/flash-attention/blob/d82bbf26924c492064af8b27ab299ff4808d1bf6/hopper/flash_attn_interface.py#L662
Does vLLM use this interface? How does FA paged_block_size relates to vLLM block_size?

### Suggest a potential alternative/fix

_No response_

### Before submitting a new issue...

- [x] Make sure you alre

[... truncated for brevity ...]

---


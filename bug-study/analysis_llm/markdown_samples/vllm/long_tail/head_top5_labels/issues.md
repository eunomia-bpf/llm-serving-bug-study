# head_top5_labels - issues

**Total Issues**: 50
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 12
- Closed Issues: 38

### Label Distribution

- stale: 24 issues
- usage: 14 issues
- bug: 13 issues
- feature request: 11 issues
- installation: 11 issues
- unstale: 2 issues
- v1: 1 issues
- rocm: 1 issues
- ray: 1 issues
- tpu: 1 issues

---

## Issue #N/A: [Bug]: safetensor format support for Mistral-7B-v0.3

**Link**: https://github.com/vllm-project/vllm/issues/6199
**State**: closed
**Created**: 2024-07-08T02:40:38+00:00
**Closed**: 2024-11-25T02:05:13+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

```text
PyTorch version: 2.3.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.29.3
Libc version: glibc-2.31

Python version: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.4.0-176-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB
GPU 4: NVIDIA A100-SXM4-40GB
GPU 5: NVIDIA A100-SXM4-40GB
GPU 6: NVIDIA A100-SXM4-40GB
GPU 7: NVIDIA A100-SXM4-40GB

Nvidia driver version: 550.54.15
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: KeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'

**Link**: https://github.com/vllm-project/vllm/issues/4247
**State**: closed
**Created**: 2024-04-21T23:20:50+00:00
**Closed**: 2024-11-29T02:06:39+00:00
**Comments**: 4
**Labels**: bug, stale

### Description

### Your current environment

```text
Collecting environment information...
PyTorch version: 2.2.1+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.29.2
Libc version: glibc-2.35

Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.5.0-25-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti
GPU 2: NVIDIA GeForce RTX 2080 Ti
GPU 3: NVIDIA GeForce RTX 2080 Ti
GPU 4: NVIDIA GeForce RTX 2080 Ti
GPU 5: NVIDIA GeForce RTX 2080 Ti
GPU 6: NVIDIA GeForce RTX 2080 Ti
GPU 7: NVIDIA GeForce RTX 2080 Ti

Nvidia driver version: 535.161.07
cuDNN version: Could not collect

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Bert tokenizer is tokenizing some tokens as `UNK`

**Link**: https://github.com/vllm-project/vllm/issues/11184
**State**: closed
**Created**: 2024-12-13T21:33:18+00:00
**Closed**: 2025-01-09T03:05:45+00:00
**Comments**: 1
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Your output of `python collect_env.py` here
```

</details>


### Model Input Dumps

_No response_

### üêõ Describe the bug

With some Bert and Roberta models like `sentence-transformers/all-MiniLM-L12-v2` I found that the output is not similar to the one generated by `sentence-transformers`. If I place the following prints in `_normalize_prompt_text_to_input()` in `serving_engine.py`
```
        print(f"{input_ids=}")
```
I get `[101, 100, 3007, 1997, 100, 2003, 100, 1012, 102]` for the sentence "The capital of France is Paris.". 100 is the `UNK` token.  When I run with sentence-transformers, I get `[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]` . This problem happens both with `--tokenizer-mode auto` and `--tokenizer-mode slow`.

cc: @DarkLight1337 

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. on Gaudi2

**Link**: https://github.com/vllm-project/vllm/issues/13054
**State**: open
**Created**: 2025-02-10T20:28:41+00:00
**Comments**: 3
**Labels**: bug, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Automatically detected platform hpu.
Collecting environment information...
PyTorch version: 2.5.1a0+git6fc067b
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address size

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: V1 engine ignores guided json

**Link**: https://github.com/vllm-project/vllm/issues/12692
**State**: closed
**Created**: 2025-02-03T14:01:44+00:00
**Closed**: 2025-06-06T02:18:22+00:00
**Comments**: 5
**Labels**: bug, stale, v1

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
INFO 02-03 05:55:13 __init__.py:183] Automatically detected platform cuda.
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.31.4
Libc version: glibc-2.35

Python version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3
Nvidia driver version: 560.35.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model

**Link**: https://github.com/vllm-project/vllm/issues/19867
**State**: closed
**Created**: 2025-06-19T14:36:59+00:00
**Closed**: 2025-07-14T17:39:11+00:00
**Comments**: 1
**Labels**: bug, rocm

### Description

### Your current environment

<details>
<summary>The output of <code>python collect_env.py</code></summary>

```text
Your output of `python collect_env.py` here
```

</details>


### üêõ Describe the bug

We hit an exception on running llama4 models with latest code on ROCm V1:

```
(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'
```
Current work-around:
To turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0


Proposal:

- [ ] Fix the bug (the team is working on it)
- [ ] Add a end-to-end test for one of the small llama4 models
- [ ] 

The motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.


### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: LLM.classify() fails on second call with ModernBERT due to missing torch.SymInt shape symbols

**Link**: https://github.com/vllm-project/vllm/issues/20716
**State**: open
**Created**: 2025-07-09T22:46:17+00:00
**Comments**: 0
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of <code>python collect_env.py</code></summary>

```text
INFO 07-09 15:38:22 [__init__.py:244] Automatically detected platform cuda.
Collecting environment information...
uv is set
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.22.1
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.7.1+cu126
Is debug build               : False
CUDA used to build PyTorch   : 12.6
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit 

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vLLM crash when running Phi-3-small-8k-instruct with enable-chunked-prefill

**Link**: https://github.com/vllm-project/vllm/issues/5961
**State**: closed
**Created**: 2024-06-28T13:34:09+00:00
**Closed**: 2024-06-28T22:41:15+00:00
**Comments**: 3
**Labels**: bug

### Description

### Your current environment


```
image": "vllm/vllm-openai:latest",
--model=microsoft/Phi-3-small-8k-instruct 
--tensor-parallel-size=1
--disable-log-requests
--trust-remote-code
--enable-chunked-prefill
--max-num-batched-tokens=2048
--max-model-len=4096
--gpu-memory-utilization=0.9",
```
Accelerator: 1x Nvidia L4

### üêõ Describe the bug

```
ERROR 06-28 13:26:18 async_llm_engine.py:52] Engine background task failed
ERROR 06-28 13:26:18 async_llm_engine.py:52] Traceback (most recent call last):
ERROR 06-28 13:26:18 async_llm_engine.py:52]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 42, in _log_task_completion
ERROR 06-28 13:26:18 async_llm_engine.py:52]     return_value = task.result()
ERROR 06-28 13:26:18 async_llm_engine.py:52]   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 532, in run_engine_loop
ERROR 06-28 13:26:18 async_llm_engine.py:52]     has_requests_in_progress = await asyn

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Model weights in GiB 

**Link**: https://github.com/vllm-project/vllm/issues/14979
**State**: closed
**Created**: 2025-03-17T18:28:30+00:00
**Closed**: 2025-03-31T17:00:51+00:00
**Comments**: 1
**Labels**: bug

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ] (64-bit runtime)
Python platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA L40S
Nvidia driver version: 570.86.10
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 57 bits v

[... truncated for brevity ...]

---

## Issue #N/A: Loading Models that require execution of third party code (trust_remote_code=True)

**Link**: https://github.com/vllm-project/vllm/issues/354
**State**: closed
**Created**: 2023-07-04T08:05:46+00:00
**Closed**: 2024-03-08T10:22:14+00:00
**Comments**: 15
**Labels**: bug

### Description

I am trying to load MPT using the AsyncLLMEngine:

```

engine_args = AsyncEngineArgs("mosaicml/mpt-7b-chat", engine_use_ray=True)
engine = AsyncLLMEngine.from_engine_args(engine_args)
```

But I am getting this error:
`ValueError: Loading mosaicml/mpt-7b-chat-local requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.`

Is there any workaround for this or could it be possible to add the option to trust remote code to EngineArgs?

---

## Issue #N/A: [Usage]: When running models on multiple GPUs, workload does not get split

**Link**: https://github.com/vllm-project/vllm/issues/12354
**State**: closed
**Created**: 2025-01-23T13:12:21+00:00
**Closed**: 2025-05-24T02:07:44+00:00
**Comments**: 2
**Labels**: usage, stale

### Description

### Your current environment

PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.11.6 (main, Nov  2 2023, 09:27:30) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration:
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090

Nvidia driver version: 560.35.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:               

[... truncated for brevity ...]

---

## Issue #N/A: How to use Splitwise(from microsoft) in vllm?

**Link**: https://github.com/vllm-project/vllm/issues/2370
**State**: closed
**Created**: 2024-01-08T03:49:36+00:00
**Closed**: 2024-11-30T02:03:07+00:00
**Comments**: 12
**Labels**: stale

### Description

Microsoft have claimed that ‚ÄùSplitwise‚Äú is supported in vLLM, see
https://www.microsoft.com/en-us/research/blog/splitwise-improves-gpu-usage-by-splitting-llm-inference-phases/
![image](https://github.com/vllm-project/vllm/assets/58217233/7835c241-f22c-4ffc-a510-1238f4a5d770)

So how to use it in vLLM? I could not find keyword about ‚ÄùSplitwise‚Äú.

---

## Issue #N/A: [Usage]: Cannot use xformers with old GPU

**Link**: https://github.com/vllm-project/vllm/issues/10662
**State**: closed
**Created**: 2024-11-26T07:14:55+00:00
**Closed**: 2025-03-27T02:04:33+00:00
**Comments**: 9
**Labels**: usage, stale

### Description

### Your current environment

<details>
<summary>The output of `python collect_env.py`</summary>

```text
Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: CentOS Linux 7 (Core) (x86_64)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.10.0-1.0.0.32-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 12.4.131
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB
GPU 2: Tesla V100-SXM2-32GB
GPU 3: Tesla V100-SXM2-32GB
GPU 4: Tesla V100-SXM2-32GB
GPU 5: Tesla V100-SXM2-32GB
GPU 6: Tesla V100-SXM2-32GB
GPU 7: Tesla V100-SXM2-32GB

Nvidia dr

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: vllm using ray in eks hangs when using --pipeline_parallel_size > 1

**Link**: https://github.com/vllm-project/vllm/issues/11139
**State**: closed
**Created**: 2024-12-12T14:07:32+00:00
**Closed**: 2025-07-09T02:16:16+00:00
**Comments**: 10
**Labels**: bug, ray, stale

### Description

### Your current environment

running on a pod in g6.12xlarge (allocated by lws).
Pod is initializing ray before running vllm (using the proposed lws image https://github.com/kubernetes-sigs/lws/blob/main/docs/examples/vllm/build/Dockerfile.GPU)

### Model Input Dumps

_No response_

### üêõ Describe the bug

Vllm is stuck on this meesage:
INFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5

full log:
[2024-12-12 05:27:53,632 W 8 8] global_state_accessor.cc:463: Retrying to get node with node ID 9c36d691ad808fe6b12015dc3c0c4ba0432917a72547d5450434659c
2024-12-12 05:27:52,822 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected t
o be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage
-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.


[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Tools parsing issues with mistral3.1

**Link**: https://github.com/vllm-project/vllm/issues/15549
**State**: open
**Created**: 2025-03-26T13:38:21+00:00
**Comments**: 2
**Labels**: bug, stale

### Description

### Your current environment

vllm 0.8.1


### üêõ Describe the bug

seems there is an issue with mistral for tools parsing? the output is not function calling as expected.

- command:
`serve mistralai/Mistral-Small-3.1-24B-Base-2503 --max-model-len 4096 --gpu-memory-utilization 0.9 --tensor-parallel-size 4 --served-model-name mistral --tokenizer-mode mistral --config-format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice `

example:
- request:
```
 {
    "model":"mistral",
    "messages": [
        {
            "content": "What's the weather like in San Francisco?",
            "role": "user"
        }
    ],
    "max_completion_tokens": 128,
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
        

[... truncated for brevity ...]

---

## Issue #N/A: [Bug]: Mismatch in the number of image tokens and placeholders during batch inference

**Link**: https://github.com/vllm-project/vllm/issues/7669
**State**: closed
**Created**: 2024-08-20T01:21:26+00:00
**Closed**: 2024-12-28T01:59:21+00:00
**Comments**: 14
**Labels**: bug, stale

### Description

### Your current environment

```
Ray v2.23
Python 3.10
vllm 0.5.4
cuda 12.1
```

### üêõ Describe the bug

We are attempting to utilize Ray v2.23 for batch inferencing, specifically on multi-modal data, by leveraging llava-next. 

```
dataset = ray.data.read_parquet(gcsInputPath, columns=columns)
class LLMPredictor:

    def __init__(self):
        # Create an LLM.
        self.llm = LLM(model="/mnt/models",
                       tensor_parallel_size=1)

    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:

        try:
            start_time = time.time()

            prompts = [{"prompt": prompt, "multi_modal_data": {
                "image": Image.open(io.BytesIO(base64.b64decode(batch[imageColumnName][i])))}} for i in
                       range(len(batch[imageColumnName]))]

            predictions = self.llm.generate(
                prompts, sampling_params=sampling_params)
            batch["generated_output"] = [preds.outpu

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: VLLM Inference - 2x slower with LoRA rank=256 vs none.

**Link**: https://github.com/vllm-project/vllm/issues/14435
**State**: open
**Created**: 2025-03-07T11:58:26+00:00
**Comments**: 14
**Labels**: usage, stale

### Description

### Your current environment

```text
The output of `python collect_env.py`
```


### How would you like to use vllm

I've noticed that using LoRA with rank=256 significantly slows down inference by 4x, as shown below. However, reducing the rank to 8 or 16 brings performance closer to that of no LoRA. I'm currently using two fully-utilized GPUs, without the enforce_eager flag, and have set the maximum LoRA rank accordingly. Interestingly, adjusting the maximum model length had no impact on performance. What steps can I take to optimize performance?


**No Lora**

**Processed prompts**:   0%|‚ñè                                                            | 5/2430 [01:28<6:58:39, 10.36s/it, est. speed input: 3.71 toks/s, output: 2.34 toks/s]Processed prompts:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                     | 240/2430 [05:09<44:09,  1.21s/it, est. speed input: 87.79 toks/s, output: 90.18 toks/s]WARNING 03-06 17:12:30 scheduler.py:1754] Sequence group 352 is preempted by Preem

[... truncated for brevity ...]

---

## Issue #N/A: [Feature]: control over llm_engine placement when multiple gpus are available.

**Link**: https://github.com/vllm-project/vllm/issues/6312
**State**: closed
**Created**: 2024-07-10T16:01:34+00:00
**Closed**: 2024-11-25T02:04:39+00:00
**Comments**: 2
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

I need a way to specify which gpu exactly should vllm use when multiple gpus are available. Currently, it automatically occupies all available gpus (https://docs.vllm.ai/en/latest/serving/distributed_serving.html).

For example, something like this: `vllm.LLM(model_path, device="cuda:N")`

#691 is exactly the same question but they end up agreeing that they can use Ray. I'm asking for a simpler solution that would not require spending time on extra engineering.

### Alternatives

My use-case doesn't allow me to use CUDA_VISIBLE_DEVICES to specify which gpu to use. That's because i train a model on multiple gpus in a DDP-like fashion where each vllm instance generates data for a model on its device, then gradients are synchronized and so on. So I cannot set CUDA_VISIBLE_DEVICES to some specific device as that would turn multiple-gpu training in a single-gpu training.

Also, I cannot just avoid this problem by running a vllm-server on a sepa

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: can't get the cu118 version of vllm 0.6.3 by https://github.com/vllm-project/vllm/releases/download/v0.6.3/vllm-0.6.3+cu118-cp310-cp310-manylinux1_x86_64.whl

**Link**: https://github.com/vllm-project/vllm/issues/10540
**State**: closed
**Created**: 2024-11-21T14:28:49+00:00
**Closed**: 2025-03-22T02:02:48+00:00
**Comments**: 2
**Labels**: installation, stale

### Description

### Your current environment

```text
The output of `python collect_env.py`
```


### How you are installing vllm

```sh
pip install -vvv vllm
```


### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Usage]: Can I get the loss of model directly?

**Link**: https://github.com/vllm-project/vllm/issues/9750
**State**: open
**Created**: 2024-10-28T08:05:33+00:00
**Comments**: 6
**Labels**: usage, stale

### Description

Hi, great work!
I am currently optimizing LLM based on `vLLM` and need to test whether my optimizations affect the model's perplexity. Therefore, I want to obtain the model's cross-entropy loss. I have reviewed the issue: [Can I directly obtain the logits here?](https://github.com/vllm-project/vllm/issues/185) and understand that one way to get log probabilities is by setting the `logprobs` parameter in `SampleParams`. 

However, this method is not very convenient. We can only obtain the top-n most likely log probabilities for each token, and the probability of the correct token might not be among these top-n log probabilities. Setting `n` and searching for the probability of the correct token is quite cumbersome, and the cross-entropy has to be calculated manually as well. 

Therefore, I want to know if `vLLM` has a way to directly obtain cross-entropy, similar to `transformers`. 
Thank you sincerely for your help. :-)

---

## Issue #N/A: [Usage]: Distributed inference not supported with OpenVINO?

**Link**: https://github.com/vllm-project/vllm/issues/14933
**State**: open
**Created**: 2025-03-17T07:06:59+00:00
**Comments**: 3
**Labels**: usage, stale

### Description

### How would you like to use vllm

The [installation page for OpenVINO](https://docs.vllm.ai/en/latest/getting_started/installation/ai_accelerator.html?device=openvino) mentions using the environment variable "VLLM_OPENVINO_DEVICE to specify which device utilize for the inference. If there are multiple GPUs in the system, additional indexes can be used to choose the proper one (e.g, VLLM_OPENVINO_DEVICE=GPU.1). If the value is not specified, CPU device is used by default."

So is it not possible to use multiple GPUs or GPU + CPU for running inference on OpenVINO backend?

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Usage]: OpenAI API for Phi-3-vision-128k-instruct 

**Link**: https://github.com/vllm-project/vllm/issues/7068
**State**: closed
**Created**: 2024-08-02T06:55:10+00:00
**Closed**: 2024-08-02T08:05:13+00:00
**Comments**: 4
**Labels**: usage

### Description


```text
BadRequestError: Error code: 400 - {'object': 'error', 'message': 'Attempted to assign 1 x 2509 = 2509 image tokens to 0 placeholders', 'type': 'BadRequestError', 'param': None, 'code': 400}
```
calling using following function:
```python
def prepare_prompts(self, prompts, images):
        messages = []
        #re.sub(r"<\|.*?\|>", "", )
        for i in range(len(prompts)):
            if i % 2 == 0:
                content = [
                    {
                        "type": "text",
                        "text": prompts[i]
                    }
                ]
                if images[i]:
                    img_byte_arr = io.BytesIO()
                    images[i].save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    image_base64 = base64.b64encode(img_byte_arr).decode('utf-8')
                    content.append(
                        {
                            "type": "image_u

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

**Link**: https://github.com/vllm-project/vllm/issues/8392
**State**: closed
**Created**: 2024-09-12T01:54:55+00:00
**Closed**: 2024-09-12T03:35:34+00:00
**Comments**: 3
**Labels**: usage

### Description

### Your current environment

I used the same service deployment command, but when I upgraded from 0.5.5 to 0.6.1 today, the deployment went wrong

![20240912-095248](https://github.com/user-attachments/assets/278b91a6-6a35-4c0c-b956-f20c5e09997e)


### How would you like to use vllm

I want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.


### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Usage]: what is enforce_eager

**Link**: https://github.com/vllm-project/vllm/issues/4449
**State**: closed
**Created**: 2024-04-29T07:14:26+00:00
**Closed**: 2024-05-01T13:38:09+00:00
**Comments**: 5
**Labels**: usage

### Description

### Your current environment

vllm 0.4.0
cuda 12.1
2*v100-16G
qwen1.5 Moe

### How would you like to use vllm

what is enforce_eager?
and when it's enabled, will the inference become slower?

---

## Issue #N/A: [Usage]: Failed to get global TPU topology.

**Link**: https://github.com/vllm-project/vllm/issues/16243
**State**: open
**Created**: 2025-04-08T07:49:20+00:00
**Comments**: 1
**Labels**: usage, stale

### Description

### Your current environment

PyTorch version: 2.8.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 11 (bullseye) (x86_64)
GCC version: (Debian 10.2.1-6) 10.2.1 20210110
Clang version: Could not collect
CMake version: version 4.0.0
Libc version: glibc-2.31

Python version: 3.10.16 (main, Jan 14 2025, 05:27:07) [GCC 10.2.1 20210110] (64-bit runtime)
Python platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   52 bits physical, 57 bits virtual
CPU(s):                          44
On-line CPU(s) list:   

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: how to redirect save logs to local file.

**Link**: https://github.com/vllm-project/vllm/issues/16319
**State**: open
**Created**: 2025-04-09T06:31:49+00:00
**Comments**: 2
**Labels**: usage, stale

### Description

### Your current environment

i am using docker 0.8.2 to run the model, output of collect_env.py
```text
The output of `Collecting environment information...
PyTorch version: 2.6.0+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.4 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.31.6
Libc version: glibc-2.35

Python version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.4.131
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 550.54.15
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: How can I use temperature correctly for Qwen2-VL?

**Link**: https://github.com/vllm-project/vllm/issues/13322
**State**: closed
**Created**: 2025-02-15T07:49:44+00:00
**Closed**: 2025-02-17T09:21:11+00:00
**Comments**: 1
**Labels**: usage

### Description

### Your current environment

```text
The output of `python collect_env.py`
```


### How would you like to use vllm

I want to run inference of Qwen2-VL-2B. The code is:
```
# Qwen2-VL
def init_qwen2_vl(model_name_or_path: str, **kwargs):
    from vllm import LLM
    try:
        from qwen_vl_utils import process_vision_info
    except ModuleNotFoundError:
        print('WARNING: `qwen-vl-utils` not installed, input images will not '
              'be automatically resized. You can enable this functionality by '
              '`pip install qwen-vl-utils`.')
        process_vision_info = None

    model_name = model_name_or_path

    llm = LLM(
        model=model_name,
        device=kwargs['device'], 
        max_model_len=kwargs.get("max_context_len", 4096 if process_vision_info is not None else 32768),  
        enable_prefix_caching=True,
        enforce_eager=True,
        disable_mm_preprocessor_cache=kwargs.get("disable_mm_preprocessor_cache", True),
    )
    stop_token_ids = 

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: how to acquire logits in vllm

**Link**: https://github.com/vllm-project/vllm/issues/8762
**State**: closed
**Created**: 2024-09-24T06:42:51+00:00
**Closed**: 2025-01-24T01:58:47+00:00
**Comments**: 4
**Labels**: usage, stale

### Description

### Your current environment

```text
The output of `python collect_env.py`
```


### How would you like to use vllm

I want to acquire logits when I run benchmark_throughput.py to do the softmax optimization, but the output in vllm doesn't have logits, how can I acquire it.

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Usage]: ËØ∑ÈóÆÂ¶Ç‰ΩïÁî®vllmËøõË°åÂ§öÊú∫ÈÉ®ÁΩ≤Ôºü

**Link**: https://github.com/vllm-project/vllm/issues/12765
**State**: closed
**Created**: 2025-02-05T03:24:26+00:00
**Closed**: 2025-02-05T03:25:00+00:00
**Comments**: 0
**Labels**: usage

### Description

### Your current environment

Collecting environment information...
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.22.1
Libc version: glibc-2.35

Python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090
GPU 4: NVIDIA GeForce RTX 4090
GPU 5: NVIDIA GeForce RTX 4090
GPU 6: NVIDIA GeForce RTX 4090
GPU 7: NVIDIA GeForce RTX 4090

Nvidia driver version: 550.142
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0
/usr/l

[... truncated for brevity ...]

---

## Issue #N/A: [Usage]: How to use a Python script to start a FastAPI service for internvl2-8b with vllm, instead of using the terminal command vllm serve ./internvl2-1b/ --tensor-parallel-size 1 --trust-remote-code? Is there any sample code for this?

**Link**: https://github.com/vllm-project/vllm/issues/10953
**State**: closed
**Created**: 2024-12-06T14:35:00+00:00
**Closed**: 2024-12-06T15:30:54+00:00
**Comments**: 1
**Labels**: usage

### Description

### Your current environment

```text
The output of `python collect_env.py`
```


### How would you like to use vllm

I want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.


### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: Support Python 3.12

**Link**: https://github.com/vllm-project/vllm/issues/6877
**State**: closed
**Created**: 2024-07-28T21:26:05+00:00
**Closed**: 2024-08-02T20:51:23+00:00
**Comments**: 2
**Labels**: feature request

### Description

### üöÄ The feature, motivation and pitch

I believe we eventually need to support 3.12 in the future. Right now I believe Pytorch just added support for Python 3.12, but ray still does not support Python 3.12. Let's use this issue to keep track on this.

### Alternatives

_No response_

### Additional context

_No response_

---

## Issue #N/A: [Feature]: Support for LoRA for Pooling Models

**Link**: https://github.com/vllm-project/vllm/issues/13679
**State**: closed
**Created**: 2025-02-21T17:05:48+00:00
**Closed**: 2025-03-18T12:07:02+00:00
**Comments**: 1
**Labels**: feature request

### Description

### üöÄ The feature, motivation and pitch

Currently vLLM does not support LoRA for Pooling models (according to #12808)
I wanted to understand how can we add the support given we can use similar code structure like generation models.

What all things need to be taken care if I need to add the support. Any references will be helpful.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature][Hardware][TPU]: Add Recompilation Check for vLLM on TPU

**Link**: https://github.com/vllm-project/vllm/issues/14580
**State**: closed
**Created**: 2025-03-10T22:31:05+00:00
**Closed**: 2025-03-25T16:59:34+00:00
**Comments**: 1
**Labels**: feature request, tpu

### Description

### üöÄ The feature, motivation and pitch

Ideally, post-warmup, no further compilation should occur. However, PyTorch/XLA's implicit compilation can lead to excessive recompilation during LLM serving, impacting performance. We can add an option to detect recompilation after warmup, requiring a PyTorch/XLA method like xm.num_graph_hash() to track the number of captured graphs. This number should remain constant post-warmup if no recompilation occurs.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: Limit total GPU memory

**Link**: https://github.com/vllm-project/vllm/issues/20256
**State**: open
**Created**: 2025-06-30T12:52:05+00:00
**Comments**: 8
**Labels**: feature request

### Description

### üöÄ The feature, motivation and pitch

Even with appropriate arguments to lower memory usage specified, as suggested in docs:

```
--enforce-eager
--max-model-len=8192
--max-num-batched-tokens=8192
--max-num-seqs=16
```

the rest of the GPU memory still gets eaten up by KV cache, so even when running a tiny 1B model that needs 3 GB of VRAM, VLLM takes 71GB on A100 to run this model. This makes VLLM an impractical solution for mixed-GPU clusters.

Suggestions:
- add flag to specify max gpu memory in absolute units
- add ability to explicitly limit/disable KV cache

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: Feature request: prompt lookup decoding

**Link**: https://github.com/vllm-project/vllm/issues/1802
**State**: closed
**Created**: 2023-11-27T19:04:37+00:00
**Closed**: 2025-03-10T01:52:03+00:00
**Comments**: 5
**Labels**: feature request, stale

### Description

Prompt lookup decoding (PLD) is a variant of speculative decoding that replaces the draft model with a prefix lookup in the current sequence, resulting in a 2-4x throughput boost for input-grounded tasks like summarization and code modification.

Because PLD doesn't require a secondary model, it might be easier to implement in VLLM?

See https://github.com/apoorvumang/prompt-lookup-decoding for details.


---

## Issue #N/A: GPTQ / Quantization support?

**Link**: https://github.com/vllm-project/vllm/issues/174
**State**: closed
**Created**: 2023-06-21T02:40:47+00:00
**Closed**: 2024-03-06T09:01:49+00:00
**Comments**: 19
**Labels**: feature request

### Description

Will vLLM support 4-bit GPTQ models?

---

## Issue #N/A: [Feature]: How to run speculative models with tensor parallelism?

**Link**: https://github.com/vllm-project/vllm/issues/10562
**State**: closed
**Created**: 2024-11-22T03:30:54+00:00
**Closed**: 2025-03-23T02:08:46+00:00
**Comments**: 3
**Labels**: feature request, stale

### Description

### üöÄ The feature, motivation and pitch

I noticed that the current speculative mode does not support tp from this link (https://docs.vllm.ai/en/stable/models/spec_decode.html). 

However, not supporting TP will greatly limit the choice of speculative models. I would like to know why there is no TP support for speculative models. I am trying to read and modify this part of the code, but I don't understand why the scorer model can support TP, but the speculative model cannot. What are the considerations in system design?

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Feature]: Inquiry about Multi-modal Support in VLLM for MiniCPM-V2.6

**Link**: https://github.com/vllm-project/vllm/issues/7546
**State**: closed
**Created**: 2024-08-15T06:36:05+00:00
**Closed**: 2024-08-15T06:49:48+00:00
**Comments**: 19
**Labels**: feature request

### Description

### üöÄ The feature, motivation and pitch

I am currently exploring the capabilities of the VLLM library and am interested in understanding its support for multi-modal inputs, particularly for models like MiniCPM-V2.6. I would like to know if VLLM is designed to handle multi-image and video inputs for such models.

### Alternatives

1. **Model of Interest**: MiniCPM-V2.6
2. **Types of Input**: Multi-image and video
3. **Current Understanding**:
   - I have reviewed the documentation and initial examples provided with VLLM.
  - It seems that both `multiple 'image_url' input` and `list value in image_url` is currently not supported.
  - However, I am not sure if it supports the processing of multiple images or videos as input to a model like MiniCPM-V2.6.
## Questions
 1. Does VLLM support the integration of MiniCPM-V2.6 for processing multi-image and video inputs?
 2. If yes, could you provide an example or a guide on how to set up and use this feature?
 3. If not, are there any 

[... truncated for brevity ...]

---

## Issue #N/A: [Feature]: Build and publish Neuron docker image

**Link**: https://github.com/vllm-project/vllm/issues/4838
**State**: open
**Created**: 2024-05-15T15:27:17+00:00
**Comments**: 4
**Labels**: feature request, keep-open

### Description

### üöÄ The feature, motivation and pitch

It seems like the current docker images don't support Neuron (Inferentia).
It would be very helpful if there was a tested, managed Neuron docker image to use.
While at the same subject, it would be even better if some documentation would be added on running vLlm Neuron using containers.

### Alternatives

DJL?

### Additional context

_No response_

---

## Issue #N/A: [Feature]: Model execution timeout

**Link**: https://github.com/vllm-project/vllm/issues/20950
**State**: open
**Created**: 2025-07-14T22:50:17+00:00
**Comments**: 0
**Labels**: feature request

### Description

### üöÄ The feature, motivation and pitch

Currently when there is a bug in the path of model execution, it could hang indefinitely and user may not get timely and useful feedback. It would be useful to have a timeout mechanism and signal the user.

Currently RayDistributedExecutor with Compiled Graph supports a timeout, but vLLM could benefit from supporting this in executor in general.

See [discussion](https://vllm-dev.slack.com/archives/C08CBAP9BUG/p1752532064610089?thread_ts=1752477597.112679&cid=C08CBAP9BUG)

cc @stephanie-wang  @youkaichao 

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Installation]: import llm meet error

**Link**: https://github.com/vllm-project/vllm/issues/4163
**State**: closed
**Created**: 2024-04-18T07:06:57+00:00
**Closed**: 2025-01-14T13:57:43+00:00
**Comments**: 6
**Labels**: installation, unstale

### Description

### Your current environment

```text
Traceback (most recent call last):
  File "inference.py", line 355, in <module>
    data_all_with_response = get_pred_func(data=data_all, task_prompt=task_prompt,\
  File "inference.py", line 24, in get_pred_vllm
    from vllm import LLM, SamplingParams
  File "/usr/local/lib/python3.8/dist-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.8/dist-packages/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,
  File "/usr/local/lib/python3.8/dist-packages/vllm/config.py", line 9, in <module>
    from vllm.utils import get_cpu_memory, is_hip
  File "/usr/local/lib/python3.8/dist-packages/vllm/utils.py", line 8, in <module>
    from vllm._C import cuda_utils
ImportError: /usr/local/lib/python3.8/dist-packages/vllm/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops15to

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: I was never able to install it, which cuda version is required?

**Link**: https://github.com/vllm-project/vllm/issues/9960
**State**: closed
**Created**: 2024-11-02T22:53:06+00:00
**Closed**: 2025-03-03T02:03:22+00:00
**Comments**: 2
**Labels**: installation, stale

### Description

### Your current environment

I use ubunt 22.04

Installing this is almost impossible, what are actually requirements lets say for cuda. I spend many hours trying to install and never worked, there way always an error, something related to cuda version.


### How you are installing vllm

```sh
pip install -vvv vllm
```


### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Installation]: Missing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much

**Link**: https://github.com/vllm-project/vllm/issues/10036
**State**: closed
**Created**: 2024-11-05T12:46:28+00:00
**Closed**: 2025-04-15T03:15:12+00:00
**Comments**: 4
**Labels**: installation, unstale

### Description

### Your current environment

Missing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much

### How you are installing vllm

Missing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: [Installation]: Dockerfile.cpu installation problem vLLM

**Link**: https://github.com/vllm-project/vllm/issues/14033
**State**: closed
**Created**: 2025-02-28T10:02:14+00:00
**Closed**: 2025-07-02T02:14:03+00:00
**Comments**: 4
**Labels**: installation, stale

### Description

### Your current environment

```text
The output of `python collect_env.py`
```
Dockerfile.cpu installation I can't complete the build somehow, I want to use vLLM over CPU since I don't have a graphics card on my own server, but the installation gives an error as follows.
OS= rockylinux 9.4 
ram 16gb
vCPU=> 24
Hypervisor= proxmox 8.2.7
docker version => Docker version 28.0.1, build 068a01e

errors messages;

docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .

Dockerfile.cpu:54
--------------------
  53 |     
  54 | >>> RUN --mount=type=cache,target=/root/.cache/pip \
  55 | >>>     --mount=type=cache,target=/root/.cache/ccache \
  56 | >>>     --mount=type=bind,source=.git,target=.git \
  57 | >>>     VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \
  58 | >>>     pip install dist/*.whl && \
  59 | >>>     rm -rf dist
  60 |     
--------------------
ERROR: failed to solve: process "/bin/sh -c VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel &&     pip install d

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: Installation instructions for ROCm can be mainlined

**Link**: https://github.com/vllm-project/vllm/issues/9385
**State**: closed
**Created**: 2024-10-15T18:21:50+00:00
**Closed**: 2025-02-13T01:59:19+00:00
**Comments**: 2
**Labels**: installation, stale

### Description

### Your current environment

N/A


### How you are installing vllm

https://docs.vllm.ai/en/stable/getting_started/amd-installation.html option 2

The problem is that it says to checkout a very specific commit of triton. Triton just published a new version of 3.1 that has AMD support mainlined but the dependency in the vllm pip package still tries to install 3.0. If someone tells me how I can update the dependency on 3.1 we can simplify the AMD instructions I think.

### Before submitting a new issue...

- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---

## Issue #N/A: Running to (Installing build dependencies ) this step is stuck

**Link**: https://github.com/vllm-project/vllm/issues/436
**State**: closed
**Created**: 2023-07-12T03:08:05+00:00
**Closed**: 2024-03-08T12:27:03+00:00
**Comments**: 5
**Labels**: installation

### Description

![image](https://github.com/vllm-project/vllm/assets/54533917/99997cfa-f6f6-4de1-9641-0e4c90884256)


systemÔºö ubuntu 20.04
Nvidia driver 515
cuda 11.7
rtx3090
python 3.8
vllm 0.1.2

---

## Issue #N/A: [Installation]: Unable to build docker image using Dockerfile.openvino

**Link**: https://github.com/vllm-project/vllm/issues/6769
**State**: closed
**Created**: 2024-07-25T04:05:59+00:00
**Closed**: 2024-07-30T18:33:02+00:00
**Comments**: 3
**Labels**: installation

### Description

### Your current environment

```text
(base) user@zahid:~/vllm$ python collect_env.py
Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.30.1
Libc version: glibc-2.35

Python version: 3.12.1 | packaged by Anaconda, Inc. | (main, Jan 19 2024, 15:51:05) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-1026-intel-iotg-x86_64-with-glibc2.35
Is CUDA available: N/A
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:   

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: Could not install packages due to an OSError: [Errno 28] No space left on device but disk still have space

**Link**: https://github.com/vllm-project/vllm/issues/7025
**State**: closed
**Created**: 2024-08-01T09:24:12+00:00
**Closed**: 2024-12-01T02:14:15+00:00
**Comments**: 3
**Labels**: installation, stale

### Description

### Your current environment

![image](https://github.com/user-attachments/assets/b25198d8-8530-49a1-b116-9882b5fb5977)
i install vllm in /mnt , i found is still have space but it has a wrong like:

Installing build dependencies ... error
  error: subprocess-exited-with-error
  
  √ó pip subprocess to install build dependencies did not run successfully.
  ‚îÇ exit code: 1
  ‚ï∞‚îÄ> [47 lines of output]
      Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/
      Collecting cmake>=3.21
        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/78/5e/c274ffd124b8d4d95734af94c1080f0421c89dabdea2475651a7bd1e02ca/cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)
      Collecting ninja
        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)
      Collecting packaging
   

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: no version of pip install vllm works - Failed to initialize NumPy: No Module named 'numpy'

**Link**: https://github.com/vllm-project/vllm/issues/11037
**State**: open
**Created**: 2024-12-09T22:11:26+00:00
**Comments**: 18
**Labels**: installation

### Description

### Your current environment

```text
Traceback (most recent call last):
  File "/mnt/MSAI/home/cephdon/sources/vllm/collect_env.py", line 15, in <module>
    from vllm.envs import environment_variables
  File "/mnt/MSAI/home/cephdon/sources/vllm/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/mnt/MSAI/home/cephdon/sources/vllm/vllm/engine/arg_utils.py", line 11, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/mnt/MSAI/home/cephdon/sources/vllm/vllm/config.py", line 21, in <module>
    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,
  File "/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/__init__.py", line 1, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/parameter.py", line 7, in <module>
    from vllm.distributed import get_tensor_

[... truncated for brevity ...]

---

## Issue #N/A: [Installation]: Transformer installation requires uv venv --system now

**Link**: https://github.com/vllm-project/vllm/issues/15550
**State**: closed
**Created**: 2025-03-26T13:56:48+00:00
**Closed**: 2025-03-27T12:38:47+00:00
**Comments**: 2
**Labels**: installation

### Description

Hi all, a small one I can take care of is a breaking change introduced in 

https://github.com/vllm-project/vllm/commit/7ffcccfa5ca3ef6b56c292ad2489e077a5cdd6f5#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557R62

The installation instructions in [here](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image) should probably include `--system` as in:

`RUN uv pip install --system git+https://github.com/huggingface/transformers.git`

Thanks for your hard work!


### How you are installing vllm

```sh
pip install -vvv vllm
```


### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.

---


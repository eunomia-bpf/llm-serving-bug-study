# duplicate - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 0
- Closed Issues: 30

### Label Distribution

- duplicate: 30 issues
- enhancement: 9 issues
- hardware: 3 issues
- model: 3 issues
- build: 2 issues
- bug: 2 issues
- wontfix: 1 issues
- bug-unconfirmed: 1 issues
- need more info: 1 issues
- stale: 1 issues

---

## Issue #N/A: making on linuxmint 21

**Link**: https://github.com/ggml-org/llama.cpp/issues/208
**State**: closed
**Created**: 2023-03-16T13:52:27+00:00
**Closed**: 2023-05-06T17:55:19+00:00
**Comments**: 2
**Labels**: duplicate, hardware, build

### Description

im running on bare metal nothing emulated

```
littlemac@littlemac:~$` git clone https://github.com/ggerganov/llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 283, done.
remote: Counting objects: 100% (283/283), done.
remote: Compressing objects: 100% (113/113), done.
remote: Total 283 (delta 180), reused 255 (delta 164), pack-reused 0
Receiving objects: 100% (283/283), 158.38 KiB | 609.00 KiB/s, done.
Resolving deltas: 100% (180/180), done.
cd littlemac@littlemac:~$ cd llama.cpp/
littlemac@littlemac:~/llama.cpp$ make
I llama.cpp build info: 
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread
I LDFLAGS:  
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3   -c ggml.c

[... truncated for brevity ...]

---

## Issue #N/A: json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

**Link**: https://github.com/ggml-org/llama.cpp/issues/102
**State**: closed
**Created**: 2023-03-13T20:01:52+00:00
**Closed**: 2023-03-15T21:41:08+00:00
**Comments**: 2
**Labels**: duplicate, model

### Description

Bug encountered when running `python3 convert-pth-to-ggml.py models/7B/ 1`:

```
llama.cpp % python3 convert-pth-to-ggml.py models/7B/ 1
Traceback (most recent call last):
  File "/Users/jjyuhub/llama.cpp/convert-pth-to-ggml.py", line 69, in <module>
    hparams = json.load(f)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py", line 355, in raw_decode
 

[... truncated for brevity ...]

---

## Issue #N/A: Command line script usage

**Link**: https://github.com/ggml-org/llama.cpp/issues/209
**State**: closed
**Created**: 2023-03-16T15:57:54+00:00
**Closed**: 2023-03-16T16:27:50+00:00
**Comments**: 1
**Labels**: duplicate, wontfix

### Description

Hello, 

I was wondering if there was a command line flag for toggling the output of the debug messages, making the executable only output the text generated by the LLM (optionally with the original prompt). This would make the program much easier to call from other scripts.

Thanks for your time.

---

## Issue #N/A: Prompt interrupted before continuation for Unicode UTF-8 emojis

**Link**: https://github.com/ggml-org/llama.cpp/issues/63
**State**: closed
**Created**: 2023-03-12T21:43:19+00:00
**Closed**: 2023-04-01T07:43:18+00:00
**Comments**: 2
**Labels**: bug, duplicate, enhancement

### Description

I have found that when having a Unicode UTF- emoji char like  

Unicode Character “👍” (U+1F44D)

The prompts breaks up.

I'm reading a sample prompt from a text file:


```bash
cat prompt

Tweet: "I hate it when my phone battery dies."
Sentiment: Negative
###
Tweet: "My day has been 👍"
Sentiment: Positive
###
Tweet: "This is the link to the article"
Sentiment: Neutral
###
Tweet: "This new music video was incredibile"
Sentiment:
```

Looking at logs I can see in fact that the tokenizers breaks at the (U+1F44D) char code:

```
(base)$ p=$(cat prompt); ./main -m ./models/13B/ggml-model-q4_0.bin -p $p -t 4 -n 512
main: seed = 1678656464
llama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 512
llama_model_load: n_embd  = 5120
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 40
llama_model_load: n_layer = 40
llama_model_load: n_rot   = 128
llama_

[... truncated for brevity ...]

---

## Issue #N/A: Not having enough memory just causes a segfault or something

**Link**: https://github.com/ggml-org/llama.cpp/issues/257
**State**: closed
**Created**: 2023-03-18T07:28:43+00:00
**Closed**: 2023-05-06T18:03:16+00:00
**Comments**: 9
**Labels**: bug, duplicate, hardware, model

### Description

So. I'm trying to build with CMake on Windows 11 and the thing just stops after it's done loading the model.

![image](https://user-images.githubusercontent.com/4723091/226091364-64a488a7-ebb5-4c24-9dd0-1cb81378008d.png)

And apparently, this is a segfault.

![Screenshot_20230318_121935](https://user-images.githubusercontent.com/4723091/226091335-afbf2712-d2b8-4b88-9b44-6b6a43d78565.png)

Yay yay yyayy yyayay

this is a memory allocation failure it seems, from me not having enough memory. not like llama.cpp Tells Me That lmao, it just segfaults

(`ctx->mem_buffer` is nullptr which probably means the malloc just failed)

---

## Issue #N/A: It's strange to return after executing the command

**Link**: https://github.com/ggml-org/llama.cpp/issues/122
**State**: closed
**Created**: 2023-03-14T07:00:34+00:00
**Closed**: 2023-03-15T21:30:03+00:00
**Comments**: 4
**Labels**: duplicate, enhancement

### Description

./main -m ./models/7B/ggml-model-q4_0.bin -t 64 -n 256 --repeat_penalty 1.0 --color -i -r "User:" -p 'What is your name?'
![image](https://user-images.githubusercontent.com/17468133/224920438-696f3b65-bc7c-42d9-ab10-a46b686dcb47.png)
Is it because I haven't installed something？
Centos 7  


---

## Issue #N/A: [natell36100] The initial make fails with : ggml.c:439:19: warning: implicit declaration of function ‘_mm_loadu_si64’; did you mean ‘_mm_loadl_epi64’? [-Wimplicit-function-declaration]

**Link**: https://github.com/ggml-org/llama.cpp/issues/1140
**State**: closed
**Created**: 2023-04-23T10:14:39+00:00
**Closed**: 2023-04-23T11:17:54+00:00
**Comments**: 3
**Labels**: duplicate, build

### Description

user@user /mnt/360E7BCD0E7B8521/ia/llama.cpp $ make
I llama.cpp build info: 
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native
I LDFLAGS:  
I CC:       cc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
I CXX:      g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native   -c ggml.c -o ggml.o
ggml.c: In function ‘bytes_from_nibbles_16’:
**ggml.c:439:19: warning: implicit declaration of function ‘_mm_loadu_si64’; did you mean ‘_mm_loadl_epi64’? [-Wimp

[... truncated for brevity ...]

---

## Issue #N/A: "Illegal Instruction" error when converting 7B model to ggml FP16 format (Raspberry Pi 4, 8GB, Raspberry Pi OS, 64-bit)

**Link**: https://github.com/ggml-org/llama.cpp/issues/425
**State**: closed
**Created**: 2023-03-23T11:52:38+00:00
**Closed**: 2023-03-26T15:27:25+00:00
**Comments**: 2
**Labels**: duplicate, hardware

### Description

# Prerequisites

Please answer the following questions for yourself before submitting an issue.

- [ /] I am running the latest code. Development is very rapid so there are no tagged versions as of now.
- [ /] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [ /] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).
- [ /] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.

# Expected Behavior

I expected the command to convert the 7B model to ggml FP16 format

# Current Behavior

Illegal instruction error

```
les@raspberrypi:~/llama.cpp $ python3 convert-pth-to-ggml.py models/7B/ 1
Illegal instruction
```


# Environment and Context 

Ras

[... truncated for brevity ...]

---

## Issue #N/A: [Enhancement] Llama 2 model support 

**Link**: https://github.com/ggml-org/llama.cpp/issues/2263
**State**: closed
**Created**: 2023-07-18T16:37:38+00:00
**Closed**: 2023-07-18T16:41:02+00:00
**Comments**: 1
**Labels**: duplicate

### Description

Meta just released Llama 2 model, with a license that authorizes commercial use.
https://ai.meta.com/llama/

---

## Issue #N/A: Cannot generate more than 500 words

**Link**: https://github.com/ggml-org/llama.cpp/issues/210
**State**: closed
**Created**: 2023-03-16T16:18:36+00:00
**Closed**: 2023-03-16T16:25:39+00:00
**Comments**: 2
**Labels**: duplicate, enhancement

### Description

The model doesn't seem to be able to return more than 500 words regardless of how big the number of tokens is specified (I even tried specifically powers of 2 such as 4096 with no results), it always stops and leaves texts uncomplete. Is anyone having the same issue, or how can I increment the length of the output?

---

## Issue #N/A: [User] Official python binding support

**Link**: https://github.com/ggml-org/llama.cpp/issues/1390
**State**: closed
**Created**: 2023-05-10T07:20:32+00:00
**Closed**: 2023-05-10T09:40:38+00:00
**Comments**: 1
**Labels**: duplicate

### Description

Hi, just wonder does there any official python binding support or support plan?

There are many vendor implementations out there, but really none of them really keep update to date with lastest llama.cpp nor can be run easily (not pythononic).

An official python binding which supported by main repo would be great!

---

## Issue #N/A: Support stableCode models (which seems to be gpt-neo-x that we can convert into gguf)

**Link**: https://github.com/ggml-org/llama.cpp/issues/4174
**State**: closed
**Created**: 2023-11-22T21:49:21+00:00
**Closed**: 2023-11-22T22:05:58+00:00
**Comments**: 1
**Labels**: duplicate, bug-unconfirmed

### Description

Why can we build gpt-neo-x models with the convert-hf-to-gguf.py script when there is no code to run it?

Mentioned in #3838 an d #3293, we do have code to convert gpt-neo-x  models into gguf models, but no code to run inference.

As i noticed, even the not too old stbilityAI stablecode models seems to be gpt-neo-x models, ad the convert-hf-to-gguf converts them, but there is no way to use them.


---

## Issue #N/A: [Feature request] Support for "Falcon" model

**Link**: https://github.com/ggml-org/llama.cpp/issues/1650
**State**: closed
**Created**: 2023-05-30T09:31:10+00:00
**Closed**: 2023-05-30T11:34:52+00:00
**Comments**: 1
**Labels**: duplicate

### Description

"Falcon" is a new Large Language Model which seems to be better than Llama.
See https://falconllm.tii.ae/ and
https://iamgeekydude.com/2023/05/28/falcon-llm-the-40-billion-parameters-llm/ and
https://www.marktechpost.com/2023/05/28/technology-innovation-institute-open-sourced-falcon-llms-a-new-ai-model-that-uses-only-75-percent-of-gpt-3s-training-compute-40-percent-of-chinchillas-and-80-percent-of-palm-62b/

Actually, it is the best open-source model currently available according to the authors.

Model (for Huggingface Transformers library) with 40B and 7B parameters is available at :
https://huggingface.co/tiiuae/falcon-40b

Would be great if it would be supported also in llama.cpp.
Note it uses some novel layers (FlashAttention, Multiquery).

---

## Issue #N/A: Stanford Alpaca support

**Link**: https://github.com/ggml-org/llama.cpp/issues/99
**State**: closed
**Created**: 2023-03-13T19:15:25+00:00
**Closed**: 2023-03-16T11:40:58+00:00
**Comments**: 6
**Labels**: duplicate, enhancement, model

### Description

Just 3 hrs ago , chat tuned LLAma released : https://github.com/tatsu-lab/stanford_alpaca

---

## Issue #N/A: ggml_new_tensor_impl: not enough space in the context's memory pool (needed 717778556, available 454395136)

**Link**: https://github.com/ggml-org/llama.cpp/issues/153
**State**: closed
**Created**: 2023-03-15T04:18:32+00:00
**Closed**: 2023-03-24T16:11:41+00:00
**Comments**: 1
**Labels**: duplicate, need more info

### Description

Hey, I know someone already posted a similar issue that has already been closed, but I ran into the same thing. On windows 10 and cloned just yesterday

---

## Issue #N/A: Feature to Discard Last Generated Message in Interactive Chat Mode?

**Link**: https://github.com/ggml-org/llama.cpp/issues/764
**State**: closed
**Created**: 2023-04-04T13:50:05+00:00
**Closed**: 2024-04-11T01:07:04+00:00
**Comments**: 2
**Labels**: duplicate, enhancement, stale

### Description

We have ctrl+c to stop generate.
Can we have undo feature to take the last message out of context and regenerate during an interactive chat session if you don't like what it generated?

---

## Issue #N/A: [Feature Request] Simplified API for Inference and HTTP Server Integration

**Link**: https://github.com/ggml-org/llama.cpp/issues/565
**State**: closed
**Created**: 2023-03-28T08:42:38+00:00
**Closed**: 2023-03-28T11:43:00+00:00
**Comments**: 3
**Labels**: duplicate, enhancement

### Description

First I want to express my deep gratitude for this project, thank you guys so much!

I'm writing to inquire about potential improvements to the API for inference, as well as the possibility of integrating an HTTP server for serving text generation requests. Specifically, I'm interested in the following:

1. A simplified and more flexible method for inference that allows for easier integration with external applications. I'm looking to manage chat history in a separate application and would like to have a straightforward way to perform inference on user-provided text.

2. The ability to serve text generation requests over HTTP. I'm interested in implementing a client-server architecture and would like to know if there are plans to include an HTTP server in the repository.

I understand that the repository is rapidly evolving, and I'm excited to see the new features and improvements you have planned. I'm planning to hack an http server together by myself, but I want to find out w

[... truncated for brevity ...]

---

## Issue #N/A: Quality of 4-bit quantization

**Link**: https://github.com/ggml-org/llama.cpp/issues/62
**State**: closed
**Created**: 2023-03-12T21:05:56+00:00
**Closed**: 2023-03-13T17:24:55+00:00
**Comments**: 4
**Labels**: duplicate

### Description

The quality of the 4-bit quantization is really abysmal compared to both non-quantized models and GPTQ quantization 
(https://github.com/qwopqwop200/GPTQ-for-LLaMa). Wouldn't it make sense for llama.cpp to load already-prequantized LLaMa models?

---

## Issue #N/A: magic number in convert-gptq-to-ggml.py not consistent

**Link**: https://github.com/ggml-org/llama.cpp/issues/672
**State**: closed
**Created**: 2023-04-01T07:41:45+00:00
**Closed**: 2023-04-02T15:51:08+00:00
**Comments**: 1
**Labels**: duplicate

### Description

It appears that the conver-gptq-to-ggml script needs an update to reflect the recent change in magic, see [this line](https://github.com/ggerganov/llama.cpp/blob/master/convert-gptq-to-ggml.py#L39). However, it's not completely clear to me if only updating the magic number is sufficient to ensure that the resulting file is compatible. Hence leaving it here a reminder :)

---

## Issue #N/A: Add support for MPT-7B-StoryWriter-65k+ 

**Link**: https://github.com/ggml-org/llama.cpp/issues/1364
**State**: closed
**Created**: 2023-05-08T09:00:42+00:00
**Closed**: 2023-05-08T12:31:30+00:00
**Comments**: 1
**Labels**: duplicate

### Description

Is possibe to add support for MPT-7B-StoryWriter-65k+  ?

https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g


https://huggingface.co/mosaicml

---

## Issue #N/A: Support gpt4all interactive mode

**Link**: https://github.com/ggml-org/llama.cpp/issues/590
**State**: closed
**Created**: 2023-03-29T06:10:08+00:00
**Closed**: 2023-03-29T06:21:05+00:00
**Comments**: 3
**Labels**: duplicate

### Description

Hey!

I just found this repo: https://github.com/nomic-ai/gpt4all and it looks amazing! They've forked alpaca.cpp but with their own "tweaks" in a single commit: https://github.com/zanussbaum/gpt4all.cpp/commit/4a6afcb08fb243df9a919c26aab1027ebfa373cc
So I assume it'd be quite easy to support!

Niansa

---

## Issue #N/A: Add the disk requirements

**Link**: https://github.com/ggml-org/llama.cpp/issues/195
**State**: closed
**Created**: 2023-03-16T03:23:50+00:00
**Closed**: 2023-03-16T11:54:44+00:00
**Comments**: 0
**Labels**: documentation, duplicate

### Description

Hi,

I found all the infos about the models:
https://cocktailpeanut.github.io/dalai/#/?id=_7b

You can put on readme the space requirements.

Thanks.

---

## Issue #N/A: feature request, restful api / exposure

**Link**: https://github.com/ggml-org/llama.cpp/issues/162
**State**: closed
**Created**: 2023-03-15T15:50:42+00:00
**Closed**: 2023-03-15T21:07:48+00:00
**Comments**: 3
**Labels**: duplicate, enhancement

### Description

hi team,

was playing interactive mode for couple hours, pretty impressive

resides what's mentioned in #145 , 
it might be not too far, to plug this a endpoint / functional call ( like swig or socket or openapi to replace current stdin ?, then self-host can have a very powerful new residents, like i got a powerful PC at home to be personal assist

also found that `-n` is the context / token limit, would be great if engine can start with 0 presume context ( which is to lift off / decouple a bit from stdin 

kindly let me know if there are directions or others interested in this ( also a developer here but not so C / tensor flavored 
( as without advice, force hi-jack stdin / stdout seems stupid 

---

## Issue #N/A: Reproducability information

**Link**: https://github.com/ggml-org/llama.cpp/issues/50
**State**: closed
**Created**: 2023-03-12T14:17:44+00:00
**Closed**: 2023-03-13T17:27:12+00:00
**Comments**: 1
**Labels**: duplicate

### Description

The seed for the website example is included, but using the same parameters doesn't manage to reproduce the example output. Listing what requirements influense reproducability would help in verifying installs.

The failed test is with x86_64 (gcc or clang, no difference), CUDA 12.1, pytorch 1.13.1, numpy 1.23.5, sentencepiece 0.1.97 and Python 3.10.6 on Linux.


---

## Issue #N/A: Failing to convert the new PHI-3 models.

**Link**: https://github.com/ggml-org/llama.cpp/issues/8259
**State**: closed
**Created**: 2024-07-02T16:49:46+00:00
**Closed**: 2024-07-03T15:56:35+00:00
**Comments**: 5
**Labels**: duplicate

### Description

```
INFO:hf-to-gguf:Loading model: Phi-3-mini-128k-instruct
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
Traceback (most recent call last):
  File "/content/llama.cpp/convert-hf-to-gguf.py", line 3263, in <module>
    main()
  File "/content/llama.cpp/convert-hf-to-gguf.py", line 3244, in main
    model_instance.set_gguf_parameters()
  File "/content/llama.cpp/convert-hf-to-gguf.py", line 1950, in set_gguf_parameters
    raise NotImplementedError(f'The rope scaling type {rope_scaling_type} is not supported yet')
NotImplementedError: The rope scaling type longrope is not supported yet
```

---

## Issue #N/A: [Proposal] "Stable" C API

**Link**: https://github.com/ggml-org/llama.cpp/issues/171
**State**: closed
**Created**: 2023-03-15T18:01:09+00:00
**Closed**: 2023-03-15T20:29:20+00:00
**Comments**: 4
**Labels**: duplicate, enhancement

### Description

I propose refactoring `main.cpp` into a library (`llama.cpp`, compiled to `llama.so`/`llama.a`/whatever) and making `main.cpp` a simple driver program. A simple C API should be exposed to access the model, and then bindings can more easily be written for Python, node.js, or whatever other language.

This would partially solve #82 and #162.

Edit: on that note, is it possible to do inference from two or more prompts on different threads? If so, serving multiple people would be possible without multiple copies of model weights in RAM.

---

## Issue #N/A: Question: can the conversation context be saved to disk and brought up again incase LLaMa crashes or there is a power failure?

**Link**: https://github.com/ggml-org/llama.cpp/issues/174
**State**: closed
**Created**: 2023-03-15T20:01:08+00:00
**Closed**: 2023-03-16T11:46:57+00:00
**Comments**: 1
**Labels**: duplicate

### Description

No description provided.

---

## Issue #N/A: HelloGGML_ASSERT: ggml-metal.m:539: false && "not implemented"

**Link**: https://github.com/ggml-org/llama.cpp/issues/1693
**State**: closed
**Created**: 2023-06-05T02:20:05+00:00
**Closed**: 2023-06-06T03:50:30+00:00
**Comments**: 8
**Labels**: duplicate

### Description

# Prerequisites

Please answer the following questions for yourself before submitting an issue.

- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.
- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).
- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.

# Expected Behavior

Please provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.

# Current Behavior

Please provide a detailed written description of what `llama.cpp` did, instead.

# Environment and Context

Please provid

[... truncated for brevity ...]

---

## Issue #N/A: [Enhancement] Officially supported/provided python bindings

**Link**: https://github.com/ggml-org/llama.cpp/issues/1156
**State**: closed
**Created**: 2023-04-24T16:33:56+00:00
**Closed**: 2023-05-12T15:13:50+00:00
**Comments**: 7
**Labels**: duplicate

### Description

Would the project accept a PR for providing directly supported python bindings.

I know the README mentions llama-cpp-python. And there are these and perhaps others...
pip install llama-cpp-python
pip install llamacpp
pip install pyllamacpp
pip install llamacpypy

Instead, perhaps it would be better to expose at least the low level interface as python directly in this repo.

Perhaps rwkv.cpp can be used as an example. It does the following:
- allow building as shared library
- create python bindings that just expose functions in the shared library as is
- (optional) create a higher level model that that builds on the basic bindings
- examples in python, rather than bash scripts

---

## Issue #N/A: Reset context instead of quitting in interactive mode

**Link**: https://github.com/ggml-org/llama.cpp/issues/145
**State**: closed
**Created**: 2023-03-14T21:26:49+00:00
**Closed**: 2023-03-16T12:04:28+00:00
**Comments**: 5
**Labels**: duplicate, enhancement

### Description

It's really annoying that I have to restart the program every time it quits by **[end of text]** or exceeding context limits, as I need to reload model, which is inefficient.
Is there any way to add an option that instead of quitting just resets to the initial prompt? 

---


# steady_state_6months - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 1
- Closed Issues: 29

### Label Distribution

- bug-unconfirmed: 22 issues
- stale: 20 issues
- enhancement: 4 issues
- bug: 3 issues
- research 🔬: 1 issues

---

## Issue #N/A: Misc. bug: [llama.android] Model keeps replying and cannot be stopped normally until it exceeds the context

**Link**: https://github.com/ggml-org/llama.cpp/issues/11264
**State**: closed
**Created**: 2025-01-16T12:32:24+00:00
**Closed**: 2025-01-17T16:36:18+00:00
**Comments**: 2
**Labels**: bug-unconfirmed

### Description

### Name and Version

The latest [b4491](https://github.com/ggerganov/llama.cpp/releases/tag/b4491) version with `llama.android` example

### Operating systems

Other? (Please let us know in description)

### Which llama.cpp modules do you know to be affected?

Other (Please specify in the next section)

### Command line

_No response_

### Problem description & steps to reproduce

1. Run `llama.android` example
2. Loaded with [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF) model
3. Send with chat template (User message: `Tell a joke`), the template are generated by the `common_chat_apply_template()` method

> Code location: llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt

```kotlin
val smollm2msg = "<|im_start|>system\n" +
    "You are a helpful AI assistant<|im_end|>\n" +
    "<|im_start|>user\n" +
    "$text<|im_end|>\n" +
    "<|im_start|>assistant\n"
viewModelScope.launch {
    llamaAndroid.send(smollm2msg)
        .

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: server /infill endpoint incorrectly inserts <bos> token

**Link**: https://github.com/ggml-org/llama.cpp/issues/11092
**State**: closed
**Created**: 2025-01-05T18:00:14+00:00
**Closed**: 2025-01-06T13:36:09+00:00
**Comments**: 1
**Labels**: bug

### Description

### Name and Version

Server built from c31fc8b966817b2f0b277fd28e04a189e388972a

### Operating systems

Linux

### Which llama.cpp modules do you know to be affected?

llama-server

### Problem description & steps to reproduce

It was mentioned in the [discussion of the codestral model](https://huggingface.co/bartowski/Codestral-22B-v0.1-GGUF/discussions/4#67737fcda8cc8ce76bcd7cc0) that changes in https://github.com/ggerganov/llama.cpp/pull/10023 made `/infill` endpoint add `<bos>` token incorrectly. I'm not really sure it's the case since before this change `prompt` was a required field, but anyway it doesn't seem to be correct.

To reproduce you can use this model: https://huggingface.co/bartowski/codegemma-2b-GGUF with the following request:

```
curl -XPOST "localhost:8080/infill" -d '{"input_prefix": "1, ", "input_suffix": ", 5"}' -H "Content-Type: application/json"
```

In the response you will see 2 `<bos>` tokens: `"prompt": "<bos><|fim_prefix|> 1, <bos><|fim_suffix|> 

[... truncated for brevity ...]

---

## Issue #N/A: Library not loaded: @rpath/libllama.dylib

**Link**: https://github.com/ggml-org/llama.cpp/issues/11321
**State**: closed
**Created**: 2025-01-20T21:41:04+00:00
**Closed**: 2025-01-25T13:21:45+00:00
**Comments**: 5
**Labels**: bug-unconfirmed

### Description

### Name and Version

I just downloaded the latest binary (b4519) to play around with on my Mac, however it looks like the binary didn't quite compile correctly. I tried downloading an earlier version (b4514) with the same result. Running `llamba-cli` yields:

```
➜  llama.cpp ./llama-cli --version
dyld[90496]: Library not loaded: @rpath/libllama.dylib
  Referenced from: <653E6B29-4AFF-3485-B031-B4F65747F8CF> /Users/constantmeiring/Downloads/build/llama.cpp/llama-cli
```



### Operating systems

MacOS 15.1.1 (24B91)

### GGML backends

Metal

### Hardware

Macbook - M3 Max

### Models

_No response_

### Problem description & steps to reproduce

Download the latest build and try and run it on Mac.

### First Bad Commit

_No response_

### Relevant log output

```shell
dyld[90496]: Library not loaded: @rpath/libllama.dylib
  Referenced from: <653E6B29-4AFF-3485-B031-B4F65747F8CF> /Users/constantmeiring/Downloads/build/llama.cpp/llama-cli
  Reason: tried: '/Users/runner/work/llama.cpp/l

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: llama-server stopped working after PR #11285 got merged

**Link**: https://github.com/ggml-org/llama.cpp/issues/11335
**State**: closed
**Created**: 2025-01-21T19:50:16+00:00
**Closed**: 2025-02-16T17:11:23+00:00
**Comments**: 11
**Labels**: bug-unconfirmed

### Description

### Name and Version

llama-server f30f099228f774209aa3010b78dfbe5d262e69aa


### Operating systems

Linux

### GGML backends

CUDA

### Hardware

RTX 4090, CUDA

### Models

E.g. Code Qwen 2.5 7B-Chat (Q8)

### Problem description & steps to reproduce

llama-server stopped generating any tokens for me, regardless of model, starting with commit f30f099228f774209aa3010b78dfbe5d262e69aa from #11285.
Simply reverting the above commit, e.g. on top of todays master (6171c9d25820ccf676b243c172868819d882848f) does fix the issue for me.

To reproduce, goto http://localhost:8080, enter a question hit return, nothing happens.

### First Bad Commit

f30f099228f774209aa3010b78dfbe5d262e69aa

### Relevant log output

```shell
│ main: server is listening on http://0.0.0.0:8080 - starting the main loop                                                                                                                                                                                 │                
       

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory

**Link**: https://github.com/ggml-org/llama.cpp/issues/11267
**State**: closed
**Created**: 2025-01-16T18:02:42+00:00
**Closed**: 2025-04-09T01:07:46+00:00
**Comments**: 12
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

version: 4493 (9c8dcefe)
built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-g

### Operating systems

Linux

### Which llama.cpp modules do you know to be affected?

_No response_

### Command line

```shell
./llama-server
```

### Problem description & steps to reproduce



I get this error when I run llama-server, 

llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory
llama.cpp/build/bin/llama-server: error while loading shared libraries: libggml.so: cannot open shared object file: No such file or directory

It's a minor issue i was able to fix it with

```export LD_LIBRARY_PATH=llama.cpp/build/ggml/src:llama.cpp/build/src/:$LD_LIBRARY_PATH```. 

It worked without doing this in which ever branch I was using before this.

I built it using 
```cmake -B build```
```cmake --build build --config Release```

### First Bad Commit

_No response_

### Relevant log output

```shell

```

---

## Issue #N/A: Misc. bug: SYCL out of memory error

**Link**: https://github.com/ggml-org/llama.cpp/issues/11044
**State**: closed
**Created**: 2025-01-02T14:34:17+00:00
**Closed**: 2025-03-06T01:07:33+00:00
**Comments**: 22
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no
ggml_sycl_init: SYCL_USE_XMX: yes
ggml_sycl_init: found 1 SYCL devices:
version: 4404 (0827b2c1)
built with MSVC 19.42.34435.0

### Operating systems

Windows

### Which llama.cpp modules do you know to be affected?

libllama (core library)

### Problem description & steps to reproduce

### Problem
I run into memory errors when using the SYCL backend. No error appears when running the same setup with the VULKAN backend (same model, prompt, context length, batch size, etc.). In the example below, the error says that 568 MB could not be allocated. This is strange because I have 16 GB of GPU memory (shared system memory, not dedicated). It seems the error is not specific to llama-cli because it also occurs when I use the Python bindings (llama-cpp-python). The error also occurs in earlier versions (I tried b4311).

### Hardware
Dell Latitude 5420
Windows 10 Enterprise
CPU: 11th Gen Intel i7-1185G7 @ 3.

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: ggml files conflict between llama.cpp and whisper.cpp

**Link**: https://github.com/ggml-org/llama.cpp/issues/11313
**State**: closed
**Created**: 2025-01-20T14:21:25+00:00
**Closed**: 2025-01-21T01:09:49+00:00
**Comments**: 0
**Labels**: bug-unconfirmed

### Description

### Name and Version

llama-cpp git commit: 92bc493917d43b83e592349e138b54c90b1c3ea7
whisper-cpp git commit: 7a423f1c008c1d7efdee91e1ce2f8ae22f42f43b

### Operating systems

_No response_

### Which llama.cpp modules do you know to be affected?

_No response_

### Command line

```shell

```

### Problem description & steps to reproduce

I'm trying to install both llama.cpp and whisper.cpp and find that they install different version of ggml files to the same location.
Maybe libggml should be separated out as a standalone project?

Conflict files are:
/usr/include/ggml*.h
/usr/include/gguf.h
/usr/lib64/libggml*.so

```
llama.cpp:
└── usr
    ├── bin
    │   ├── convert_hf_to_gguf.py
    │   ├── llama-batched
    │   ├── llama-batched-bench
    │   ├── llama-bench
    │   ├── llama-cli
    │   ├── llama-convert-llama2c-to-ggml
    │   ├── llama-cvector-generator
    │   ├── llama-embedding
    │   ├── llama-eval-callback
    │   ├── llama-export-lora
    │   ├── llama-gbnf-validator
   

[... truncated for brevity ...]

---

## Issue #N/A: DeepSeek Models (V2/V3) Hang with ROCm Backend

**Link**: https://github.com/ggml-org/llama.cpp/issues/11141
**State**: closed
**Created**: 2025-01-08T15:42:33+00:00
**Closed**: 2025-02-13T17:37:18+00:00
**Comments**: 8
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

./llama-cli --version
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 ROCm devices:
  Device 0: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 1: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 2: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 3: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 4: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 5: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 6: AMD Instinct MI100, compute capability 9.0, VMM: no
  Device 7: AMD Instinct MI100, compute capability 9.0, VMM: no
version: 4436 (53ff6b9b)
built with Ubuntu clang version 12.0.1-19ubuntu3 for x86_64-pc-linux-gnu

### Operating systems

Linux

### GGML backends

HIP

### Hardware

AMD Instinct MI100

### Models

DeepSeek-V2
DeepSeek-V3

### Problem description & steps to reproduce

## Description
When attempting to r

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: Docker Image llama-quantize Segmentation fault

**Link**: https://github.com/ggml-org/llama.cpp/issues/11196
**State**: closed
**Created**: 2025-01-11T19:39:25+00:00
**Closed**: 2025-03-10T01:06:59+00:00
**Comments**: 4
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

root@f7545b6b4f65:/app# ./llama-cli --version
load_backend: loaded CPU backend from ./libggml-cpu-alderlake.so
version: 4460 (ba8a1f9c)
built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu

### Operating systems

Linux, Other? (Please let us know in description)

### Which llama.cpp modules do you know to be affected?

llama-quantize

### Command line

```shell
❯ docker run --rm -it \                                                                                                                                          
  -v ./models:/models \
  ghcr.io/ggerganov/llama.cpp:full \
  --quantize /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-f32.gguf /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-Q4_K_M.gguf Q4_K_M
```


### Problem description & steps to reproduce

just try to quantize a model and you'll get the segfault
```shell
❯ docker run --rm -it \                                    
  -v ./models:/models \
  ghcr.io/ggerganov/

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: Deepseek R1 incompatible with grammars / structured output

**Link**: https://github.com/ggml-org/llama.cpp/issues/11336
**State**: closed
**Created**: 2025-01-21T20:14:44+00:00
**Closed**: 2025-03-11T01:07:42+00:00
**Comments**: 5
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

Using llama.cpp at b4516 and ollama at 7bb35


### Operating systems

_No response_

### Which llama.cpp modules do you know to be affected?

_No response_

### Command line

```shell

```

### Problem description & steps to reproduce

Apologies if this is misfiled as a bug. I'm not sure if it is an enhancement, but grammars are a commonly used feature of llama.cpp and the Deepseek R1 model is very popular, so I wanted to raise it and perhaps think about solutions


Reasoning based models are trained to use <think></think> tokens to add their own chain of thought to the context. These tokens get suppressed by design when using the sampler with JSON grammar causing the model performance to suffer significantly. 

As suppressing the output of the tokens within the <think> tokens is currently being discussed in #11325, I wanted to mention that using grammar to effectively suppress these tokens causes the model to perform badly.

Looking at https://github.com/ggergano

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: Failed to convert `MiniCPM-o-2_6`

**Link**: https://github.com/ggml-org/llama.cpp/issues/11347
**State**: closed
**Created**: 2025-01-22T09:04:30+00:00
**Closed**: 2025-04-05T01:07:37+00:00
**Comments**: 2
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

By following the steps in the [Usage of MiniCPM-o 2.6](https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README-minicpmo2.6.md#usage-of-minicpm-o-26) section, failed to convert PyTorch model to gguf files:

```bash
sam@sam-pc:~/workspace/llama.cpp$ python ./examples/llava/minicpmv-surgery.py -m /home/sam/workspace/models/MiniCPM-o-2_6
Traceback (most recent call last):
  File "/home/sam/workspace/llama.cpp/./examples/llava/minicpmv-surgery.py", line 11, in <module>
    model = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True, torch_dtype=torch.bfloat16)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 553, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  F

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: Output token sequence cannot match with AutoTokenizer

**Link**: https://github.com/ggml-org/llama.cpp/issues/11054
**State**: closed
**Created**: 2025-01-03T09:42:19+00:00
**Closed**: 2025-01-06T08:54:26+00:00
**Comments**: 3
**Labels**: bug-unconfirmed

### Description

### Name and Version

ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes
version: 4354 (0e70ba6)
built with cc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2) for x86_64-redhat-linux

### Operating systems

Linux

### GGML backends

CUDA

### Hardware

NVIDIA A100-SXM4-80GB

### Models

Meta-Llama-3-8B-Instruct

### Problem description & steps to reproduce

Found that the outpu

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: 

**Link**: https://github.com/ggml-org/llama.cpp/issues/10980
**State**: closed
**Created**: 2024-12-26T08:42:42+00:00
**Closed**: 2025-02-09T01:07:26+00:00
**Comments**: 1
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

出现的问题
Welcome to Qwen.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.

Prompt > hello
qwen > unknown token: 151935

ubuntu版本：20.04
使用的软件：qwen.cpp 1.3 和1.2都试过
使用的硬件：jetson orin nano 4gb   jetpack：5.1.1


### Operating systems

Linux

### GGML backends

CUDA

### Hardware

jetson orin nano 4gb   jetpack：5.1.1

### Models

Qwen-1_8B-Chat 

### Problem description & steps to reproduce

使用量化转译后的qwen1.8B 运行在orin nano中 使用qwen.cpp运行  无法识别文本 报位置错误
cuda等基础ai环境已经装好基础ai环境
![微信图片_2024-12-26_164133_589](https://github.com/user-attachments/assets/e8a67d37-365e-4cba-b0e1-2980414170d3)



### First Bad Commit

_No response_

### Relevant log output

```shell
Welcome to Qwen.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.

Prompt > hello
qwen > unknown token: 151935
```


---

## Issue #N/A: Misc. bug:  llama-server - shared libraries after build 4409, last working 4406

**Link**: https://github.com/ggml-org/llama.cpp/issues/11144
**State**: closed
**Created**: 2025-01-08T20:01:17+00:00
**Closed**: 2025-03-11T01:07:48+00:00
**Comments**: 2
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

llama-b4447-bin-ubuntu-x64

### Operating systems

Linux

### Which llama.cpp modules do you know to be affected?

Other (Please specify in the next section)

### Command line

_No response_

### Problem description & steps to reproduce

llama-b4406-bin-ubuntu-x64.zip - works - as no external .so required
llama-b4409-bin-ubuntu-x64.zip - not possible as :
ldd llama-server
        libllama.so => not found
        libggml.so => not found
        libggml-base.so => not found
all that .so are not included in compiled .zip assets on github
llama-b4447-bin-ubuntu-x64.zip - same not included.


### First Bad Commit

_No response_

### Relevant log output

_No response_

---

## Issue #N/A: Misc. bug: llama-qwen2vl-cli: ignores --log* options

**Link**: https://github.com/ggml-org/llama.cpp/issues/10985
**State**: closed
**Created**: 2024-12-26T16:04:54+00:00
**Closed**: 2025-03-01T01:07:45+00:00
**Comments**: 2
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

‰ ./bin/llama-qwen2vl-cli --version
version: 4391 (9ba399df)
built with cc (Gentoo Hardened 14.2.1_p20241221 p6) 14.2.1 20241221 for x86_64-pc-linux-gnu

### Operating systems

_No response_

### Which llama.cpp modules do you know to be affected?

_No response_

### Problem description & steps to reproduce

Use `--log-file /dev/null` or `--log-verbosity -100`. Note lines like `clip_model_load: model name:   Qwen2-VL-7B-Instruct` still being produced in stdout.
There's seemingly no way to isolate the model output from the miscellaneous messages of llama.cpp.

### First Bad Commit

_No response_

### Relevant log output

_No response_

---

## Issue #N/A: Feature Request: MiniMax-Text-01 model

**Link**: https://github.com/ggml-org/llama.cpp/issues/11290
**State**: closed
**Created**: 2025-01-18T15:38:54+00:00
**Closed**: 2025-03-12T01:07:33+00:00
**Comments**: 17
**Labels**: enhancement, stale

### Description

### Prerequisites

- [x] I am running the latest code. Mention the version if possible as well.
- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).
- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.

### Feature Description

Please add support for minimax-text-01 model https://huggingface.co/MiniMaxAI/MiniMax-Text-01
https://github.com/MiniMax-AI/MiniMax-01

### Motivation

We need to add support for the latest models! Performs almost as good as deepseek v3. But has 4 million tokens context.

### Possible Implementation

It's a MoE model.

---

## Issue #N/A: "CPU_AARCH64 model buffer" appears when not using AARCH64

**Link**: https://github.com/ggml-org/llama.cpp/issues/11204
**State**: closed
**Created**: 2025-01-12T13:43:12+00:00
**Closed**: 2025-04-13T01:34:13+00:00
**Comments**: 5
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

build: 4465 (9a483999) with gcc (conda-forge gcc 13.3.0-1) 13.3.0 for x86_64-conda-linux-gnu

### Operating systems

Linux

### GGML backends

CPU

### Hardware

2x Intel Xeon 24 core (Kaggle)

### Models

DeepSeek-V2.5: https://huggingface.co/bartowski/DeepSeek-V2.5-GGUF/tree/main/DeepSeek-V2.5-Q4_0

### Problem description & steps to reproduce

The problem is that a part of the memory was used for "CPU_AARCH64 model buffer". Normally the model takes only 150GB of RAM, now it takes 260GB and loads much slower. Command line: `/root/llama.cpp/build/bin/llama-server -m /dev/shm/DeepSeek-V2.5-Q4_0-00001-of-00004.gguf -t 72`. This doesn't appear when using Q4_K_M.

Compile commands:
```
git clone https://github.com/ggerganov/llama.cpp ~/llama.cpp
cd ~/llama.cpp && cmake -G Ninja -B build && cmake --build build --config Release -j 64
```

### First Bad Commit

_No response_

### Relevant log output

```shell
build: 4465 (9a483999) w

[... truncated for brevity ...]

---

## Issue #N/A: Research: Performance differences between Metal (macOS) and Vulkan (Linux)

**Link**: https://github.com/ggml-org/llama.cpp/issues/10982
**State**: closed
**Created**: 2024-12-26T11:12:21+00:00
**Closed**: 2025-05-04T01:08:09+00:00
**Comments**: 14
**Labels**: research 🔬, stale

### Description

I'm one of the developers for the Asahi Linux GPU drivers, which provide accelerated Vulkan and OpenGL support on Apple Silicon platforms. I'm interested in improving the performance of llama.cpp on our drivers with the Vulkan backend.

As things stand today, macOS is significantly faster on a quick test with `llama-bench`, with default settings (tested on an M2 Max 64GB):

Linux:

```
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Apple M2 Max (G14C B1) (Honeykrisp) | uma: 1 | fp16: 1 | warp size: 32 | matrix cores: none
| model                          |       size |     params | backend    | ngl |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |
ggml_vulkan: Compiling shaders................................Done!
| llama 7B Q4_K - Medium         |   4.07 GiB |     7.24 B | Vulkan     |  99 |         pp512 |         92.16 ± 0.08 |
| llama 7B Q4_K - Medium   

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: segfault on Alpine linux docker image

**Link**: https://github.com/ggml-org/llama.cpp/issues/11308
**State**: open
**Created**: 2025-01-20T11:12:22+00:00
**Comments**: 11
**Labels**: bug

### Description

### Name and Version

ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Intel(R) Arc(tm) A770 Graphics (DG2) (Intel open-source Mesa driver) | uma: 0 | fp16: 1 | warp size: 32 | matrix cores: none
version: 0 (unknown)
built with cc (Alpine 14.2.0) 14.2.0 for x86_64-alpine-linux-musl

### Operating systems

Linux

### GGML backends

Vulkan

### Hardware

AMD Ryzen™ 9 7950X × 32
Intel® Arc™ A770 Graphics (DG2)

Raspberry Pi 5
AMD 7600 XT

### Models

Qwen2.5-0.5B-Instruct-Q4_K_M.gguf

### Problem description & steps to reproduce

Running llama.cpp in Docker on Alpine linux segfaults on model compilation.
Observed both on my x86 machine as well as my raspberry pi.

```dockerfile
FROM alpine

RUN apk add --no-cache mesa-vulkan-ati mesa-vulkan-intel vulkan-loader-dev vulkan-tools cmake ninja build-base curl-dev shaderc musl-dev linux-headers
ADD https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.tar.gz /llama.tgz
RUN tar xzf /llama.tgz

WORKDIR /llama.cpp-master
RUN cmake 

[... truncated for brevity ...]

---

## Issue #N/A: Feature Request: Support for iFlytek Spark 13B

**Link**: https://github.com/ggml-org/llama.cpp/issues/11232
**State**: closed
**Created**: 2025-01-14T10:08:56+00:00
**Closed**: 2025-02-28T01:07:25+00:00
**Comments**: 1
**Labels**: enhancement, stale

### Description

### Prerequisites

- [X] I am running the latest code. Mention the version if possible as well.
- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).
- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.

### Feature Description

I am attempting to run the iFlytek Spark model using llama.cpp, as I consider it a new and relevant model. I have worked on building the computation graph and performing all the necessary tasks required for the model's operation. However, despite these efforts, the model fails to function as expected, consistently producing chaotic output. I would appreciate any guidance or suggestions to address this issue.

### Motivation

iFlytek Spark is one of the most prominent Chinese models currently avai

[... truncated for brevity ...]

---

## Issue #N/A: Compile bug: Compilation fails due to -D_XOPEN_SOURCE=600: error: use of undeclared identifier 'strnlen'

**Link**: https://github.com/ggml-org/llama.cpp/issues/11095
**State**: closed
**Created**: 2025-01-06T00:33:47+00:00
**Closed**: 2025-04-07T01:09:15+00:00
**Comments**: 5
**Labels**: bug-unconfirmed, stale

### Description

### Git commit

4418

### Operating systems

BSD

### GGML backends

CPU

### Problem description & steps to reproduce

[This -D_XOPEN_SOURCE=600 argument](https://github.com/ggerganov/llama.cpp/blob/master/Makefile#L286) breaks compilation:

```
In file included from /usr/ports/misc/llama-cpp/work/llama.cpp-b4418/ggml/src/ggml-vulkan/ggml-vulkan.cpp:8:
/usr/local/include/vulkan/vulkan.hpp:145:41: error: use of undeclared identifier 'strnlen'
  145 |       return std::string( this->data(), strnlen( this->data(), N ) );
      |                                         ^
```

The same argument is set in cmake scripts as well.

Please don't fix POSIX support at random ancient levels.

In general it shouldn't be necessary to ever set _XOPEN_SOURCE.

FreeBSD 14.2

### First Bad Commit

n/a

### Relevant log output

```shell
n/a
```


---

## Issue #N/A: Misc. bug: RISCV output bug when using rvv with vlen > 256bit

**Link**: https://github.com/ggml-org/llama.cpp/issues/11041
**State**: closed
**Created**: 2025-01-02T09:15:01+00:00
**Closed**: 2025-04-16T01:07:54+00:00
**Comments**: 5
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

./llama-cli latest version ubuntu linux riscv

### Operating systems

Linux

### Which llama.cpp modules do you know to be affected?

llama-cli

### Problem description & steps to reproduce

The generated output of llama-cli in riscv env with rvv enabled consists of trash when vlen is greater than 256bits

### First Bad Commit

_No response_

### Relevant log output

_No response_

---

## Issue #N/A: Feature Request: `reasoning_effort` parameter for reasoning models like R1

**Link**: https://github.com/ggml-org/llama.cpp/issues/11351
**State**: closed
**Created**: 2025-01-22T15:22:22+00:00
**Closed**: 2025-03-13T01:07:53+00:00
**Comments**: 10
**Labels**: enhancement, stale

### Description

### Prerequisites

- [x] I am running the latest code. Mention the version if possible as well.
- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).
- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.

### Feature Description

Similar to how the o1 family of models accepts a `reasoning_effort` parameter (low, medium, high), it's concievable that OSS reasoning models should have this too.

Deepseek mentions that this param will soon be available via their API: https://api-docs.deepseek.com/guides/reasoning_model#api-parameters

> Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (reasoning_effort) will be available soon.



### Motivation

This would allow us to run reasoni

[... truncated for brevity ...]

---

## Issue #N/A: Feature Request: index.html.gz as a separate distributable

**Link**: https://github.com/ggml-org/llama.cpp/issues/11184
**State**: closed
**Created**: 2025-01-11T03:12:17+00:00
**Closed**: 2025-02-25T01:07:30+00:00
**Comments**: 1
**Labels**: enhancement, stale

### Description

### Prerequisites

- [X] I am running the latest code. Mention the version if possible as well.
- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).
- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).
- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.

### Feature Description

When running server, with different LLMs, sometimes you want to switch the system prompt on the Web ui.  Currently, it can save one prompt only. It'd be great to be able to use different prompt. It'd open up more functionality (different prompts for different models, censoring/non-censoring prompts, jailbreak prompts, specially designed prompts in general) if it can be easily modified (dropdown box options etc.), or just have more slots for system prompts.

### Motivation

system pro

[... truncated for brevity ...]

---

## Issue #N/A: Compile bug: libggml-vulkan.so reproducible builds issue

**Link**: https://github.com/ggml-org/llama.cpp/issues/11306
**State**: closed
**Created**: 2025-01-20T09:58:48+00:00
**Closed**: 2025-01-23T07:07:51+00:00
**Comments**: 1
**Labels**: bug-unconfirmed

### Description

### Git commit

667d72846c06b2cf4f7c8a4265e210991a49706b

### Operating systems

Linux

### GGML backends

Vulkan

### Problem description & steps to reproduce

I'm compiling the `llamacpp` openSUSE package and found that the `libggml-vulkan.so` differs in every build.

My analysis points to
https://github.com/ggerganov/llama.cpp/blob/bd38dde/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp#L502
as the place where some sorting is missing to provide deterministic output.

### First Bad Commit

_No response_

### Compile command

```shell
we build with cmake - see https://github.com/bmwiedemann/openSUSE/blob/0da890b/packages/l/llamacpp/llamacpp.spec#L123 for details.
```

### Relevant log output

```shell
Generated source-code varied (randomly) thusly:

/home/abuild/rpmbuild/BUILD.unsorted/llamacpp-4501/build/ggml/src/ggml-vulkan/ggml-vulkan-shaders.cpp
@@ -1,8 +1,892 @@
 #include "ggml-vulkan-shaders.hpp"
 
-unsigned char matmul_f16_f32_aligned_fp32_data[10276] = {
+unsigned c

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: input is too large to process. increase the physical batch size

**Link**: https://github.com/ggml-org/llama.cpp/issues/11105
**State**: closed
**Created**: 2025-01-06T11:58:47+00:00
**Closed**: 2025-01-07T01:38:12+00:00
**Comments**: 1
**Labels**: bug-unconfirmed

### Description

### Name and Version

ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 2 CUDA devices:
  Device 0: NVIDIA A800 80GB PCIe, compute capability 8.0, VMM: yes
  Device 1: NVIDIA A800 80GB PCIe, compute capability 8.0, VMM: yes
version: 0 (unknown)
built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu

### Operating systems

Linux

### GGML backends

CUDA

### Hardware

A800 * 2 ;

### Models

bge-reranker-v2-m3， q8_0

### Problem description & steps to reproduce

when I use `llama-server`  to inference, I get this **error**:
```
{"error":{"code":500,"message":"input is too large to process. increase the physical batch size","type":"server_error"}}
```

my llama-server is :
```
CUDA_VISIBLE_DEVICES="0" /data/fffan/other/llama.cpp-master/build_cuda/bin/llama-server \
              -m ./bge-reranker-v2-m3_quanto_llama/bge-reranker-v2-m3_q8_0.gguf \
              --reranking \
  

[... truncated for brevity ...]

---

## Issue #N/A: Eval bug: Crash with filesystem error when run while in a directory containing files with certain names

**Link**: https://github.com/ggml-org/llama.cpp/issues/11198
**State**: closed
**Created**: 2025-01-11T21:39:49+00:00
**Closed**: 2025-03-02T21:11:01+00:00
**Comments**: 8
**Labels**: bug

### Description

### Name and Version

version: 4462 (c05e8c99)
built with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu


### Operating systems

Linux

### GGML backends

CPU

### Hardware

N/A

### Models

N/A

### Problem description & steps to reproduce

Running `llama-cli` or `llama-server` while the current directory contains a file with certain characters in the file name causes a crash with the filesystem error shown below.

Here is an example of a character that will cause the crash when present in a file name
｜

### First Bad Commit

commit 60cfa72

### Relevant log output

```shell
terminate called after throwing an instance of 'std::filesystem::__cxx11::filesystem_error'
  what():  filesystem error: Cannot convert character sequence: Invalid or incomplete multibyte or wide character
Aborted
```


---

## Issue #N/A: Misc. bug: ggml_backend_sycl_graph_compute: error: op not supported node_1586 (FLASH_ATTN_EXT)

**Link**: https://github.com/ggml-org/llama.cpp/issues/11084
**State**: closed
**Created**: 2025-01-05T09:59:12+00:00
**Closed**: 2025-02-19T01:12:39+00:00
**Comments**: 2
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

SYCL RPC server build running in this docker image `intel/deep-learning-essentials:2025.0.1-0-devel-ubuntu22.04`
A master revision of llama.cpp on 03/01/2025, I do not know the specific commit.

### Operating systems

Linux

### Which llama.cpp modules do you know to be affected?
RPC server

### Problem description & steps to reproduce

![Screenshot from 2025-01-05 01-05-51](https://github.com/user-attachments/assets/a2c409fc-de21-4a6c-81f4-ae50e4939b12)

Occurred when llama-server was running this way:
`bins_llmcpp/llama-server -m $MODEL_PATH -p "$MODEL_PROMPT" --threads 16 --n-gpu-layers "$gpu_layers" --ctx_size 8192 --repeat_penalty 1 --temp 0.1 --no-kv-offload --rpc 127.0.0.1:50025 --flash-attn`

If there is no  `--flash-attn` specified everything works fine. I understand that the RPC is an experimental, though a very helpful feature. I have never met such an error with CUDA RPC server container, which has  the same code revision as the SYCL

[... truncated for brevity ...]

---

## Issue #N/A: Misc. bug: 

**Link**: https://github.com/ggml-org/llama.cpp/issues/11055
**State**: closed
**Created**: 2025-01-03T09:55:13+00:00
**Closed**: 2025-02-17T01:07:26+00:00
**Comments**: 1
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

Latest

### Operating systems

_No response_

### Which llama.cpp modules do you know to be affected?

_No response_

### Problem description & steps to reproduce

in llama-android, when trying to unload the model, the app crashes in while clearing batch -
llama_batch_free.
Suggestion:- Use common_batch_clear(*reinterpret_cast<llama_batch *>(batch_pointer)); instead llama-android.cpp

### First Bad Commit

_No response_

### Relevant log output

_No response_

---

## Issue #N/A: Eval bug: very slow inference on DeepSeek-R1-Distill-Qwen-32B

**Link**: https://github.com/ggml-org/llama.cpp/issues/11361
**State**: closed
**Created**: 2025-01-22T21:38:51+00:00
**Closed**: 2025-04-02T01:07:52+00:00
**Comments**: 18
**Labels**: bug-unconfirmed, stale

### Description

### Name and Version

from master(22 jan)

### Operating systems

Linux

### GGML backends

CUDA

### Hardware

3060 on main pc +(3060+3060+1660ti/sup+1660ti/sup) on other pc

### Models

DeepSeek-R1-Distill-Qwen-32B_q8

### Problem description & steps to reproduce

hello,  i running inference on https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B (locally ggufed to q8) on 2 pc over network (connection via 100 mb wan (iperf is good) via vpn),  i get very slow inference with low cpu/gpu usage on 2 pc's .
.`/llama-server --host 192.168.2.109 --port 8080 -m unsloth_DeepSeek-R1-Distill-Qwen-32B/DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf --rpc 10.2.0.5:1000,10.2.0.5:1001,10.2.0.5:1002,10.2.0.5:1003  -ngl 99999`
main host:
![Image](https://github.com/user-attachments/assets/155e24ec-fc0b-4cd6-8245-f89ca470be03)
remote host:
![Image](https://github.com/user-attachments/assets/2d7b9ff2-17a4-4a54-8cea-d3f4e07b8a65)

### First Bad Commit

_No response_

### Relevant log output

```shell
llam

[... truncated for brevity ...]

---


# very_short_under50chars - issues

**Total Issues**: 30
**Generated**: 2025-07-23 11:45:14

## Summary Statistics

- Open Issues: 1
- Closed Issues: 29

### Label Distribution

- stale: 8 issues
- question: 2 issues
- wontfix: 2 issues
- bug-unconfirmed: 2 issues
- duplicate: 1 issues
- invalid: 1 issues
- high severity: 1 issues
- documentation: 1 issues
- need more info: 1 issues
- bug: 1 issues

---

## Issue #N/A: Is there a requirements.txt ?

**Link**: https://github.com/ggml-org/llama.cpp/issues/8
**State**: closed
**Created**: 2023-03-11T05:53:26+00:00
**Closed**: 2023-03-12T06:23:30+00:00
**Comments**: 4
**Labels**: question, wontfix

### Description

No description provided.

---

## Issue #N/A: Windows MSVC support

**Link**: https://github.com/ggml-org/llama.cpp/issues/49
**State**: closed
**Created**: 2023-03-12T13:43:57+00:00
**Closed**: 2023-03-13T17:25:21+00:00
**Comments**: 2
**Labels**: duplicate

### Description

hello, would it add MSVC build support as well?

---

## Issue #N/A: Data offline

**Link**: https://github.com/ggml-org/llama.cpp/issues/14722
**State**: closed
**Created**: 2025-07-16T11:51:11+00:00
**Closed**: 2025-07-16T11:51:39+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: Try Modular - Mojo

**Link**: https://github.com/ggml-org/llama.cpp/issues/1317
**State**: closed
**Created**: 2023-05-04T12:32:34+00:00
**Closed**: 2023-05-04T18:49:00+00:00
**Comments**: 2
**Labels**: invalid

### Description

https://www.modular.com/

---

## Issue #N/A: Лама

**Link**: https://github.com/ggml-org/llama.cpp/issues/11901
**State**: closed
**Created**: 2025-02-16T03:44:29+00:00
**Closed**: 2025-02-16T05:08:44+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: it’s so long a lime to wait while using a server

**Link**: https://github.com/ggml-org/llama.cpp/issues/1663
**State**: closed
**Created**: 2023-06-01T08:38:02+00:00
**Closed**: 2024-04-10T01:07:55+00:00
**Comments**: 4
**Labels**: stale

### Description

No description provided.

---

## Issue #N/A: 请问，llama.cpp可以量化微调后的qwen模型吗？

**Link**: https://github.com/ggml-org/llama.cpp/issues/6416
**State**: closed
**Created**: 2024-04-01T02:59:41+00:00
**Closed**: 2024-05-16T01:06:35+00:00
**Comments**: 1
**Labels**: stale

### Description

请问，llama.cpp可以量化微调后的qwen模型吗？

---

## Issue #N/A: nvm

**Link**: https://github.com/ggml-org/llama.cpp/issues/7945
**State**: closed
**Created**: 2024-06-15T00:09:20+00:00
**Closed**: 2024-06-17T18:20:35+00:00
**Comments**: 0
**Labels**: bug-unconfirmed, high severity

### Description

nvm

---

## Issue #N/A: when support Qwen-7b

**Link**: https://github.com/ggml-org/llama.cpp/issues/3736
**State**: closed
**Created**: 2023-10-23T05:09:23+00:00
**Closed**: 2024-04-04T01:07:25+00:00
**Comments**: 3
**Labels**: stale

### Description

No description provided.

---

## Issue #N/A: [Q] Memory Requirements for Different Model Sizes

**Link**: https://github.com/ggml-org/llama.cpp/issues/13
**State**: closed
**Created**: 2023-03-11T12:19:07+00:00
**Closed**: 2023-03-18T21:02:00+00:00
**Comments**: 18
**Labels**: documentation, question

### Description

No description provided.

---

## Issue #N/A: Create json api service

**Link**: https://github.com/ggml-org/llama.cpp/issues/88
**State**: closed
**Created**: 2023-03-13T10:19:23+00:00
**Closed**: 2023-07-28T19:29:40+00:00
**Comments**: 8
**Labels**: need more info

### Description

so we can intergrate app/UI.

---

## Issue #N/A: Add theme Rose Pine

**Link**: https://github.com/ggml-org/llama.cpp/issues/9584
**State**: closed
**Created**: 2024-09-21T21:37:37+00:00
**Closed**: 2024-11-07T01:07:19+00:00
**Comments**: 5
**Labels**: stale

### Description

https://rosepinetheme.com/

---

## Issue #N/A: ....!

**Link**: https://github.com/ggml-org/llama.cpp/issues/1446
**State**: closed
**Created**: 2023-05-14T06:54:24+00:00
**Closed**: 2023-05-14T07:13:56+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: Figure out how to fork

**Link**: https://github.com/ggml-org/llama.cpp/issues/7549
**State**: closed
**Created**: 2024-05-26T22:38:55+00:00
**Closed**: 2024-05-26T22:39:43+00:00
**Comments**: 1

### Description

No description provided.

---

## Issue #N/A: I like this project.

**Link**: https://github.com/ggml-org/llama.cpp/issues/2129
**State**: closed
**Created**: 2023-07-07T09:35:47+00:00
**Closed**: 2023-07-07T10:37:24+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: How to use it in Python

**Link**: https://github.com/ggml-org/llama.cpp/issues/253
**State**: closed
**Created**: 2023-03-18T04:46:55+00:00
**Closed**: 2023-03-18T04:58:21+00:00
**Comments**: 2

### Description

How to use this in my python code?

---

## Issue #N/A: Great work ! !

**Link**: https://github.com/ggml-org/llama.cpp/issues/13558
**State**: closed
**Created**: 2025-05-15T07:13:32+00:00
**Closed**: 2025-05-15T13:10:55+00:00
**Comments**: 0

### Description

Thanks for your contribution.

---

## Issue #N/A: Kompute-based Vulkan backend shows an GGML_OP_GET_ROWS error

**Link**: https://github.com/ggml-org/llama.cpp/issues/6400
**State**: open
**Created**: 2024-03-30T21:56:07+00:00
**Comments**: 4
**Labels**: bug

### Description

Doesn't happen with the other Vulkan backend.

---

## Issue #N/A: Is it possible to run the llama on an AMD graphics card?

**Link**: https://github.com/ggml-org/llama.cpp/issues/259
**State**: closed
**Created**: 2023-03-18T08:43:00+00:00
**Closed**: 2023-03-18T11:16:59+00:00
**Comments**: 2

### Description

No description provided.

---

## Issue #N/A: Llama-3.2 11B Vision Support

**Link**: https://github.com/ggml-org/llama.cpp/issues/9643
**State**: closed
**Created**: 2024-09-25T20:00:17+00:00
**Closed**: 2025-03-16T22:51:45+00:00
**Comments**: 47

### Description

Is it working right now in any way?

---

## Issue #N/A: delete

**Link**: https://github.com/ggml-org/llama.cpp/issues/11857
**State**: closed
**Created**: 2025-02-14T05:15:06+00:00
**Closed**: 2025-02-24T18:19:31+00:00
**Comments**: 1
**Labels**: bug-unconfirmed

### Description

No description provided.

---

## Issue #N/A: SayHello

**Link**: https://github.com/ggml-org/llama.cpp/issues/2412
**State**: closed
**Created**: 2023-07-27T01:18:40+00:00
**Closed**: 2023-07-28T20:00:03+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: Do you add LLaDA model support?

**Link**: https://github.com/ggml-org/llama.cpp/issues/12360
**State**: closed
**Created**: 2025-03-13T04:31:35+00:00
**Closed**: 2025-05-02T01:07:55+00:00
**Comments**: 2
**Labels**: stale

### Description

https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct

---

## Issue #N/A: how to fine tuning model with with dataset (file json/csv..)

**Link**: https://github.com/ggml-org/llama.cpp/issues/414
**State**: closed
**Created**: 2023-03-23T04:29:28+00:00
**Closed**: 2023-03-23T08:55:14+00:00
**Comments**: 0

### Description

No description provided.

---

## Issue #N/A: Is it possible to convert only SentencePiece tokenizer without the model to GGUF?

**Link**: https://github.com/ggml-org/llama.cpp/issues/6077
**State**: closed
**Created**: 2024-03-15T10:49:01+00:00
**Closed**: 2024-05-07T01:06:36+00:00
**Comments**: 3
**Labels**: stale

### Description

If it is, how?

---

## Issue #N/A: Add avx-512 support?

**Link**: https://github.com/ggml-org/llama.cpp/issues/160
**State**: closed
**Created**: 2023-03-15T12:10:17+00:00
**Closed**: 2023-03-28T09:54:15+00:00
**Comments**: 6
**Labels**: enhancement, performance, hardware

### Description

No clue but I think it may work faster

---

## Issue #N/A: [User] Insert summary of your issue or enhancement..

**Link**: https://github.com/ggml-org/llama.cpp/issues/1418
**State**: closed
**Created**: 2023-05-12T16:18:43+00:00
**Closed**: 2023-05-12T17:44:56+00:00
**Comments**: 2

### Description

 is it possible to run on linux???

---

## Issue #N/A: Make a tag/release

**Link**: https://github.com/ggml-org/llama.cpp/issues/111
**State**: closed
**Created**: 2023-03-14T02:04:37+00:00
**Closed**: 2023-03-14T19:16:27+00:00
**Comments**: 1
**Labels**: wontfix

### Description

Thanks.

---

## Issue #N/A: [User] supporting code llama

**Link**: https://github.com/ggml-org/llama.cpp/issues/2947
**State**: closed
**Created**: 2023-09-01T01:34:44+00:00
**Closed**: 2024-04-05T01:06:33+00:00
**Comments**: 2
**Labels**: stale

### Description

is it possible to use with code llama ?


---

## Issue #N/A: [C0ffymachyne] was -gqa argument removed from the "main"  ? What is a replacement and how to load 70B model without it ?

**Link**: https://github.com/ggml-org/llama.cpp/issues/2975
**State**: closed
**Created**: 2023-09-02T21:07:07+00:00
**Closed**: 2024-04-05T01:06:26+00:00
**Comments**: 7
**Labels**: stale

### Description

No description provided.

---


{
  "frameworks": {
    "vLLM": {
      "name": "vLLM",
      "total_production_issues": 3612,
      "bug_issues": 2225,
      "categories": {
        "concurrency": 396,
        "gpu": 430,
        "api": 497,
        "model": 457,
        "crash": 96,
        "scaling": 184,
        "performance": 155,
        "memory": 44
      },
      "samples": {
        "concurrency": [
          {
            "number": 21175,
            "title": "[Bug]: Wrongly reuse KV, for V1 PD disaggregation with multimodal input",
            "state": "open",
            "created_at": "2025-07-18T10:04:41Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21170,
            "title": "[Bug]: How to Resolve KVC Transmission Timeout",
            "state": "open",
            "created_at": "2025-07-18T09:27:05Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "al scenarios. For instance, if the kv_cache for storing a request times out and no exception handling is performed on the request, it could result in the request being unable to proceed with inference and the block being unable to be released.",
              "handling for these operations."
            ]
          },
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21156,
            "title": "[Bug]: When making a streaming request, the 9-digit integer in the function call result will be truncated to 6 digits",
            "state": "open",
            "created_at": "2025-07-18T02:43:10Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21148,
            "title": "[Bug]: Server hang with google/gemma-3-27b-it and structured decoding",
            "state": "open",
            "created_at": "2025-07-17T22:53:42Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "07-17 22:03:54 [backend_xgrammar.py:167] Failed to advance FSM for request chatcmpl-78eca22187e24416a39e4c12c73dad75 for tokens 0. Please file an issue.",
              "message"
            ]
          }
        ],
        "gpu": [
          {
            "number": 21175,
            "title": "[Bug]: Wrongly reuse KV, for V1 PD disaggregation with multimodal input",
            "state": "open",
            "created_at": "2025-07-18T10:04:41Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21160,
            "title": "[Performance]: `model_weights` generator invoked out of Model loading in EAGLE series models.",
            "state": "open",
            "created_at": "2025-07-18T06:40:37Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21156,
            "title": "[Bug]: When making a streaming request, the 9-digit integer in the function call result will be truncated to 6 digits",
            "state": "open",
            "created_at": "2025-07-18T02:43:10Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21148,
            "title": "[Bug]: Server hang with google/gemma-3-27b-it and structured decoding",
            "state": "open",
            "created_at": "2025-07-17T22:53:42Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "07-17 22:03:54 [backend_xgrammar.py:167] Failed to advance FSM for request chatcmpl-78eca22187e24416a39e4c12c73dad75 for tokens 0. Please file an issue.",
              "message"
            ]
          }
        ],
        "api": [
          {
            "number": 21175,
            "title": "[Bug]: Wrongly reuse KV, for V1 PD disaggregation with multimodal input",
            "state": "open",
            "created_at": "2025-07-18T10:04:41Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21170,
            "title": "[Bug]: How to Resolve KVC Transmission Timeout",
            "state": "open",
            "created_at": "2025-07-18T09:27:05Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "al scenarios. For instance, if the kv_cache for storing a request times out and no exception handling is performed on the request, it could result in the request being unable to proceed with inference and the block being unable to be released.",
              "handling for these operations."
            ]
          },
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21160,
            "title": "[Performance]: `model_weights` generator invoked out of Model loading in EAGLE series models.",
            "state": "open",
            "created_at": "2025-07-18T06:40:37Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21156,
            "title": "[Bug]: When making a streaming request, the 9-digit integer in the function call result will be truncated to 6 digits",
            "state": "open",
            "created_at": "2025-07-18T02:43:10Z",
            "labels": [
              "bug"
            ],
            "errors": []
          }
        ],
        "model": [
          {
            "number": 21175,
            "title": "[Bug]: Wrongly reuse KV, for V1 PD disaggregation with multimodal input",
            "state": "open",
            "created_at": "2025-07-18T10:04:41Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21170,
            "title": "[Bug]: How to Resolve KVC Transmission Timeout",
            "state": "open",
            "created_at": "2025-07-18T09:27:05Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "al scenarios. For instance, if the kv_cache for storing a request times out and no exception handling is performed on the request, it could result in the request being unable to proceed with inference and the block being unable to be released.",
              "handling for these operations."
            ]
          },
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21160,
            "title": "[Performance]: `model_weights` generator invoked out of Model loading in EAGLE series models.",
            "state": "open",
            "created_at": "2025-07-18T06:40:37Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21156,
            "title": "[Bug]: When making a streaming request, the 9-digit integer in the function call result will be truncated to 6 digits",
            "state": "open",
            "created_at": "2025-07-18T02:43:10Z",
            "labels": [
              "bug"
            ],
            "errors": []
          }
        ],
        "crash": [
          {
            "number": 21170,
            "title": "[Bug]: How to Resolve KVC Transmission Timeout",
            "state": "open",
            "created_at": "2025-07-18T09:27:05Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "al scenarios. For instance, if the kv_cache for storing a request times out and no exception handling is performed on the request, it could result in the request being unable to proceed with inference and the block being unable to be released.",
              "handling for these operations."
            ]
          },
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21148,
            "title": "[Bug]: Server hang with google/gemma-3-27b-it and structured decoding",
            "state": "open",
            "created_at": "2025-07-17T22:53:42Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "07-17 22:03:54 [backend_xgrammar.py:167] Failed to advance FSM for request chatcmpl-78eca22187e24416a39e4c12c73dad75 for tokens 0. Please file an issue.",
              "message"
            ]
          },
          {
            "number": 21122,
            "title": "[Bug]: Issue running mistralai/Magistral-Small-2506 on NVIDIA hardware",
            "state": "open",
            "created_at": "2025-07-17T15:02:36Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "not a string exception when running vLLM (v0.9.2) with the mistralai/Magistral-Small-2506 model on NVIDIA H200 hardeware.",
              "```"
            ]
          },
          {
            "number": 21112,
            "title": "[Bug]: TypeError: Object of type <class 'function'> is not serializable",
            "state": "closed",
            "created_at": "2025-07-17T10:01:47Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "(TypeError): ray::vLLM.collective_rpc() (pid=755, ip=172.31.202.67, actor_id=9f52d0baa7a6cc1d4a34e13805000000, repr=<vllm.vLLM object at 0x7fe251cc9790>)",
              "(f\"Object of type {type(obj)} is not serializable\""
            ]
          }
        ],
        "scaling": [
          {
            "number": 21165,
            "title": "[Bug]: Mistral crashes on tool_calls with empty description",
            "state": "open",
            "created_at": "2025-07-18T08:47:33Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "is returned:",
              "\",\"message\":\"1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type 1 validation error for ChatCompletionRequest\\ntools.0.function.description\\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}"
            ]
          },
          {
            "number": 21134,
            "title": "[Bug]: Empty VllmConfig when calling `get_current_vllm_config`, causing VllmConfig `__post__init__` to fail",
            "state": "open",
            "created_at": "2025-07-17T18:46:34Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "traceback.print_stack()\n```"
            ]
          },
          {
            "number": 21120,
            "title": "[Bug]: vllm serve THUDM/GLM-4.1V-9B-Thinking --limit-mm-per-prompt image =32  \uff1a ValueError: `limit_mm_per_prompt` is only supported for multimodal models.",
            "state": "closed",
            "created_at": "2025-07-17T12:43:04Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "(\"`limit_mm_per_prompt` is only supported for \"",
              "`limit_mm_per_prompt` is only supported for multimodal models."
            ]
          },
          {
            "number": 21112,
            "title": "[Bug]: TypeError: Object of type <class 'function'> is not serializable",
            "state": "closed",
            "created_at": "2025-07-17T10:01:47Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "(TypeError): ray::vLLM.collective_rpc() (pid=755, ip=172.31.202.67, actor_id=9f52d0baa7a6cc1d4a34e13805000000, repr=<vllm.vLLM object at 0x7fe251cc9790>)",
              "(f\"Object of type {type(obj)} is not serializable\""
            ]
          },
          {
            "number": 21110,
            "title": "[Bug]: FP8 Attention on H100 - CUDA error: an illegal memory access was encountered",
            "state": "open",
            "created_at": "2025-07-17T09:36:20Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "an illegal memory access was encountered` and the server crashes. I am unable to share an explicit reproducible example currently, but will update when I am able to extract a specific combination of model+config+prompt that fails.",
              "trace</summary>"
            ]
          }
        ],
        "performance": [
          {
            "number": 21160,
            "title": "[Performance]: `model_weights` generator invoked out of Model loading in EAGLE series models.",
            "state": "open",
            "created_at": "2025-07-18T06:40:37Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21148,
            "title": "[Bug]: Server hang with google/gemma-3-27b-it and structured decoding",
            "state": "open",
            "created_at": "2025-07-17T22:53:42Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "07-17 22:03:54 [backend_xgrammar.py:167] Failed to advance FSM for request chatcmpl-78eca22187e24416a39e4c12c73dad75 for tokens 0. Please file an issue.",
              "message"
            ]
          },
          {
            "number": 21141,
            "title": "[Performance]: Opportunities to speed up BlockPool processing",
            "state": "open",
            "created_at": "2025-07-17T20:59:45Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21140,
            "title": "[Performance]: Reduce overhead of FreeKVCacheBlockQueue",
            "state": "closed",
            "created_at": "2025-07-17T20:56:09Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 21122,
            "title": "[Bug]: Issue running mistralai/Magistral-Small-2506 on NVIDIA hardware",
            "state": "open",
            "created_at": "2025-07-17T15:02:36Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "not a string exception when running vLLM (v0.9.2) with the mistralai/Magistral-Small-2506 model on NVIDIA H200 hardeware.",
              "```"
            ]
          }
        ],
        "memory": [
          {
            "number": 21073,
            "title": "[Bug]: vLLM doesn't release memory on deletion of the LLM runner with VLLM_ENABLE_V1_MULTIPROCESSING=0",
            "state": "open",
            "created_at": "2025-07-16T19:08:49Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 21027,
            "title": "[Performance]: The GPU memory usage of vllm v0.9.2 is significantly higher than that of v0.9.1. Why is this? How can it be improved?",
            "state": "open",
            "created_at": "2025-07-16T03:11:40Z",
            "labels": [
              "performance"
            ],
            "errors": []
          },
          {
            "number": 20895,
            "title": "[Bug]: Gemma3 can not be launched in vllm CLI",
            "state": "closed",
            "created_at": "2025-07-14T02:37:03Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "message is arranged at the end (i can not findout the issue for reviewing the error message though).",
              "message : "
            ]
          },
          {
            "number": 20860,
            "title": "[Bug]: Ray + vLLM failing to automatically release GPU memory when tensor parallelism size (tp_size) > 1",
            "state": "open",
            "created_at": "2025-07-12T11:11:25Z",
            "labels": [
              "bug",
              "ray"
            ],
            "errors": [
              "s, \u200b\u200bbut it fails to automatically release GPU memory after completion\u200b\u200b.",
              "s, \u200b\u200bbut it fails to automatically release GPU memory after completion\u200b\u200b."
            ]
          },
          {
            "number": 20743,
            "title": "[Bug]: Intel Arc 140T GPU XPU backend fails with \"doesn't support querying the available free memory\" error",
            "state": "open",
            "created_at": "2025-07-10T07:38:57Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "occurs during KV cache initialization when vLLM attempts to query available GPU memory using `torch.xpu.mem_get_info()`, which is not supported on Intel Arc GPUs.",
              "message"
            ]
          }
        ]
      }
    },
    "llama.cpp": {
      "name": "llama.cpp",
      "total_production_issues": 4740,
      "bug_issues": 2601,
      "categories": {
        "memory": 45,
        "concurrency": 233,
        "gpu": 383,
        "api": 470,
        "model": 406,
        "scaling": 153,
        "crash": 138,
        "performance": 64,
        "other": 2
      },
      "samples": {
        "memory": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14719,
            "title": "Misc. bug: Llama server use some of the vram on another GPU, even I set -mg 1 and -sm 'none'",
            "state": "open",
            "created_at": "2025-07-16T10:00:14Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14692,
            "title": "Eval bug: CUDA error: operation not supported",
            "state": "open",
            "created_at": "2025-07-15T10:15:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "when requesting.",
              "CUDA error: operation not supported"
            ]
          },
          {
            "number": 14458,
            "title": "Misc. bug: oom \uff0cThe process does not exit.",
            "state": "closed",
            "created_at": "2025-06-30T09:09:44Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "out of memory",
              "### First Bad Commit"
            ]
          }
        ],
        "concurrency": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14747,
            "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
            "state": "open",
            "created_at": "2025-07-17T22:31:51Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "[New LWP 33273]"
            ]
          },
          {
            "number": 14745,
            "title": "Compile bug: GGML Vulkan :: \"format not a string literal and no format arguments [-Werror=format-security]\"",
            "state": "open",
            "created_at": "2025-07-17T18:30:40Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "format not a string literal and no format arguments [-Werror=format-security]",
              "format not a string literal and no format arguments [-Werror=format-security]"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14734,
            "title": "Regarding the build for 8060S (gfx1151):",
            "state": "open",
            "created_at": "2025-07-17T12:09:48Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "no matching function for call to 'hipblasGemmEx'",
              "_str)"
            ]
          }
        ],
        "gpu": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14747,
            "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
            "state": "open",
            "created_at": "2025-07-17T22:31:51Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "[New LWP 33273]"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14734,
            "title": "Regarding the build for 8060S (gfx1151):",
            "state": "open",
            "created_at": "2025-07-17T12:09:48Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "no matching function for call to 'hipblasGemmEx'",
              "_str)"
            ]
          },
          {
            "number": 14727,
            "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
            "state": "open",
            "created_at": "2025-07-16T18:41:41Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          }
        ],
        "api": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14747,
            "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
            "state": "open",
            "created_at": "2025-07-17T22:31:51Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "[New LWP 33273]"
            ]
          },
          {
            "number": 14745,
            "title": "Compile bug: GGML Vulkan :: \"format not a string literal and no format arguments [-Werror=format-security]\"",
            "state": "open",
            "created_at": "2025-07-17T18:30:40Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "format not a string literal and no format arguments [-Werror=format-security]",
              "format not a string literal and no format arguments [-Werror=format-security]"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14738,
            "title": "Misc. bug: b5921 release zip on github misses llama-embedding binary",
            "state": "open",
            "created_at": "2025-07-17T13:47:46Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          }
        ],
        "model": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14747,
            "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
            "state": "open",
            "created_at": "2025-07-17T22:31:51Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "[New LWP 33273]"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14734,
            "title": "Regarding the build for 8060S (gfx1151):",
            "state": "open",
            "created_at": "2025-07-17T12:09:48Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "no matching function for call to 'hipblasGemmEx'",
              "_str)"
            ]
          },
          {
            "number": 14727,
            "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
            "state": "open",
            "created_at": "2025-07-16T18:41:41Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          }
        ],
        "scaling": [
          {
            "number": 14752,
            "title": "Eval bug: Nemotron 49b doesnt load correctly",
            "state": "open",
            "created_at": "2025-07-18T07:24:58Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "bcs it doesnt use the second gpu:",
              "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
            ]
          },
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14727,
            "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
            "state": "open",
            "created_at": "2025-07-16T18:41:41Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14709,
            "title": "Compile bug: [SYCL][ARC A770] Regression: \u53cc A770 \u652f\u6301\u5728 b5422 \u53ca\u4ee5\u540e\u7248\u672c\u5931\u6548",
            "state": "open",
            "created_at": "2025-07-16T03:50:20Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "caught at file:D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp, line:3900"
            ]
          },
          {
            "number": 14698,
            "title": "Eval bug: Gemma 3n on Vulkan fails to load",
            "state": "open",
            "created_at": "2025-07-15T13:32:04Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "OutOfDeviceMemory",
              "loading model: unable to allocate Vulkan0 buffer"
            ]
          }
        ],
        "crash": [
          {
            "number": 14740,
            "title": "Misc. bug: out of memory error after PR #13746",
            "state": "open",
            "created_at": "2025-07-17T16:01:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
              "```"
            ]
          },
          {
            "number": 14727,
            "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
            "state": "open",
            "created_at": "2025-07-16T18:41:41Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14713,
            "title": "Misc. bug: llamacpp crashes my PC whenever I close the console for it.",
            "state": "open",
            "created_at": "2025-07-16T06:00:11Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14706,
            "title": "Eval bug: Llama-server on Mac starting at b5478 only producing 2-3 streaming tokens on Qwen3 and Deepseek R1 0528",
            "state": "closed",
            "created_at": "2025-07-16T00:29:03Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14697,
            "title": "Eval bug: Regression: Tool calls still returned in content field as JSON string instead of tool_calls array",
            "state": "open",
            "created_at": "2025-07-15T13:07:17Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "('Only text and image blocks are supported in message content!') }}",
              "('Only user, systemm assistant and tool roles are supported in the custom template made by Unsloth!') }}"
            ]
          }
        ],
        "performance": [
          {
            "number": 14724,
            "title": "Misc. bug: Hybrid models failing to load with assert GGML_ASSERT(kv_size % n_pad == 0)",
            "state": "closed",
            "created_at": "2025-07-16T15:11:45Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "#0  llama_kv_cache_unified::llama_kv_cache_unified(llama_model const&, std::function<bool (int)>&&, ggml_type, ggml_type, bool, bool, bool, unsigned int, unsigned int, unsigned int, unsigned int, llama_swa_type) (this=0x5555585f7e60, "
            ]
          },
          {
            "number": 14697,
            "title": "Eval bug: Regression: Tool calls still returned in content field as JSON string instead of tool_calls array",
            "state": "open",
            "created_at": "2025-07-15T13:07:17Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "('Only text and image blocks are supported in message content!') }}",
              "('Only user, systemm assistant and tool roles are supported in the custom template made by Unsloth!') }}"
            ]
          },
          {
            "number": 14583,
            "title": "Eval bug:[5808-] qwen3 30B vulkan run with  GGG",
            "state": "closed",
            "created_at": "2025-07-08T13:54:07Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14550,
            "title": "Eval bug: Mistral-Small not working on vulkan",
            "state": "closed",
            "created_at": "2025-07-06T11:41:14Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 14469,
            "title": "Eval bug: Gemma vision head (possibly Siglip) yields garbage on vulkan / sycl on Intel N150",
            "state": "open",
            "created_at": "2025-06-30T23:09:30Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          }
        ],
        "other": [
          {
            "number": 14533,
            "title": "Misc. bug: Inconsistency between llama cpp server values and transformers library for reranking",
            "state": "closed",
            "created_at": "2025-07-04T13:05:18Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": []
          },
          {
            "number": 13170,
            "title": "Compile bug: Build fails on ppc64le",
            "state": "closed",
            "created_at": "2025-04-29T05:21:09Z",
            "labels": [
              "bug-unconfirmed"
            ],
            "errors": [
              "s. ",
              "cannot convert \u2018float\u2019 to \u2018__vector float\u2019 {aka \u2018__vector(4) float\u2019} in initialization"
            ]
          }
        ]
      }
    },
    "SGLang": {
      "name": "SGLang",
      "total_production_issues": 2201,
      "bug_issues": 106,
      "categories": {
        "gpu": 74,
        "api": 103,
        "model": 81,
        "performance": 28,
        "concurrency": 61,
        "scaling": 13,
        "memory": 7,
        "crash": 16
      },
      "samples": {
        "gpu": [
          {
            "number": 7951,
            "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
            "state": "open",
            "created_at": "2025-07-11T10:58:55Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": []
          },
          {
            "number": 7162,
            "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
            "state": "closed",
            "created_at": "2025-06-13T21:30:30Z",
            "labels": [
              "bug"
            ],
            "errors": [
              ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
              "trace:"
            ]
          },
          {
            "number": 7026,
            "title": "[Bug] PD+MTP+ DeepEP+dp attention",
            "state": "open",
            "created_at": "2025-06-10T03:48:01Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "CUDA error: an illegal memory access was encountered",
              "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
            ]
          },
          {
            "number": 6933,
            "title": "[Bug] Llama-4-Scout OOM with image requests",
            "state": "open",
            "created_at": "2025-06-06T22:15:48Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "during our image benchmark.",
              "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
            ]
          },
          {
            "number": 6914,
            "title": "[Bug] Scheduler CPU Memory leak.",
            "state": "open",
            "created_at": "2025-06-06T06:59:51Z",
            "labels": [
              "bug"
            ],
            "errors": []
          }
        ],
        "api": [
          {
            "number": 7951,
            "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
            "state": "open",
            "created_at": "2025-07-11T10:58:55Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": []
          },
          {
            "number": 7551,
            "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
            "state": "open",
            "created_at": "2025-06-26T06:19:16Z",
            "labels": [
              "bug",
              "router"
            ],
            "errors": [
              "Messages**: Failures occur silently without indicating resource exhaustion",
              "s**: Send 503 Service Unavailable when at capacity"
            ]
          },
          {
            "number": 7162,
            "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
            "state": "closed",
            "created_at": "2025-06-13T21:30:30Z",
            "labels": [
              "bug"
            ],
            "errors": [
              ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
              "trace:"
            ]
          },
          {
            "number": 7124,
            "title": "[Bug] llama 3 405b fb fp8 issue",
            "state": "open",
            "created_at": "2025-06-12T08:58:27Z",
            "labels": [
              "bug",
              "good first issue",
              "help wanted",
              "high priority"
            ],
            "errors": []
          },
          {
            "number": 7062,
            "title": "[Bug] test_lora.py bug",
            "state": "closed",
            "created_at": "2025-06-10T18:10:20Z",
            "labels": [
              "bug",
              "lora"
            ],
            "errors": []
          }
        ],
        "model": [
          {
            "number": 7951,
            "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
            "state": "open",
            "created_at": "2025-07-11T10:58:55Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": []
          },
          {
            "number": 7551,
            "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
            "state": "open",
            "created_at": "2025-06-26T06:19:16Z",
            "labels": [
              "bug",
              "router"
            ],
            "errors": [
              "Messages**: Failures occur silently without indicating resource exhaustion",
              "s**: Send 503 Service Unavailable when at capacity"
            ]
          },
          {
            "number": 7162,
            "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
            "state": "closed",
            "created_at": "2025-06-13T21:30:30Z",
            "labels": [
              "bug"
            ],
            "errors": [
              ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
              "trace:"
            ]
          },
          {
            "number": 7062,
            "title": "[Bug] test_lora.py bug",
            "state": "closed",
            "created_at": "2025-06-10T18:10:20Z",
            "labels": [
              "bug",
              "lora"
            ],
            "errors": []
          },
          {
            "number": 7026,
            "title": "[Bug] PD+MTP+ DeepEP+dp attention",
            "state": "open",
            "created_at": "2025-06-10T03:48:01Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "CUDA error: an illegal memory access was encountered",
              "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
            ]
          }
        ],
        "performance": [
          {
            "number": 7551,
            "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
            "state": "open",
            "created_at": "2025-06-26T06:19:16Z",
            "labels": [
              "bug",
              "router"
            ],
            "errors": [
              "Messages**: Failures occur silently without indicating resource exhaustion",
              "s**: Send 503 Service Unavailable when at capacity"
            ]
          },
          {
            "number": 7026,
            "title": "[Bug] PD+MTP+ DeepEP+dp attention",
            "state": "open",
            "created_at": "2025-06-10T03:48:01Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "CUDA error: an illegal memory access was encountered",
              "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
            ]
          },
          {
            "number": 6933,
            "title": "[Bug] Llama-4-Scout OOM with image requests",
            "state": "open",
            "created_at": "2025-06-06T22:15:48Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "during our image benchmark.",
              "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
            ]
          },
          {
            "number": 6906,
            "title": "[Bug] FMHA using flashinfer cutlass on Blackwell has low accuracy result",
            "state": "closed",
            "created_at": "2025-06-05T22:15:07Z",
            "labels": [
              "bug",
              "high priority",
              "flashinfer"
            ],
            "errors": []
          },
          {
            "number": 6753,
            "title": "[Bug] PD Failed to register memory on H200",
            "state": "open",
            "created_at": "2025-05-29T23:27:04Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
              "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
            ]
          }
        ],
        "concurrency": [
          {
            "number": 7551,
            "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
            "state": "open",
            "created_at": "2025-06-26T06:19:16Z",
            "labels": [
              "bug",
              "router"
            ],
            "errors": [
              "Messages**: Failures occur silently without indicating resource exhaustion",
              "s**: Send 503 Service Unavailable when at capacity"
            ]
          },
          {
            "number": 7026,
            "title": "[Bug] PD+MTP+ DeepEP+dp attention",
            "state": "open",
            "created_at": "2025-06-10T03:48:01Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "CUDA error: an illegal memory access was encountered",
              "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
            ]
          },
          {
            "number": 6933,
            "title": "[Bug] Llama-4-Scout OOM with image requests",
            "state": "open",
            "created_at": "2025-06-06T22:15:48Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "during our image benchmark.",
              "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
            ]
          },
          {
            "number": 6906,
            "title": "[Bug] FMHA using flashinfer cutlass on Blackwell has low accuracy result",
            "state": "closed",
            "created_at": "2025-06-05T22:15:07Z",
            "labels": [
              "bug",
              "high priority",
              "flashinfer"
            ],
            "errors": []
          },
          {
            "number": 6753,
            "title": "[Bug] PD Failed to register memory on H200",
            "state": "open",
            "created_at": "2025-05-29T23:27:04Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
              "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
            ]
          }
        ],
        "scaling": [
          {
            "number": 7551,
            "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
            "state": "open",
            "created_at": "2025-06-26T06:19:16Z",
            "labels": [
              "bug",
              "router"
            ],
            "errors": [
              "Messages**: Failures occur silently without indicating resource exhaustion",
              "s**: Send 503 Service Unavailable when at capacity"
            ]
          },
          {
            "number": 7162,
            "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
            "state": "closed",
            "created_at": "2025-06-13T21:30:30Z",
            "labels": [
              "bug"
            ],
            "errors": [
              ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
              "trace:"
            ]
          },
          {
            "number": 6753,
            "title": "[Bug] PD Failed to register memory on H200",
            "state": "open",
            "created_at": "2025-05-29T23:27:04Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
              "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
            ]
          },
          {
            "number": 6592,
            "title": "[Bug] load microsoft/MAI-DS-R1 error: KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'",
            "state": "open",
            "created_at": "2025-05-25T13:37:42Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "occurred\uff1a**KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**",
              "'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**"
            ]
          },
          {
            "number": 6496,
            "title": "[Bug] Crash/Hang during CUDA graph capture on H100*2",
            "state": "open",
            "created_at": "2025-05-21T08:19:53Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "Segmentation fault",
              "Segmentation fault"
            ]
          }
        ],
        "memory": [
          {
            "number": 6933,
            "title": "[Bug] Llama-4-Scout OOM with image requests",
            "state": "open",
            "created_at": "2025-06-06T22:15:48Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "during our image benchmark.",
              "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
            ]
          },
          {
            "number": 6914,
            "title": "[Bug] Scheduler CPU Memory leak.",
            "state": "open",
            "created_at": "2025-06-06T06:59:51Z",
            "labels": [
              "bug"
            ],
            "errors": []
          },
          {
            "number": 6753,
            "title": "[Bug] PD Failed to register memory on H200",
            "state": "open",
            "created_at": "2025-05-29T23:27:04Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
              "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
            ]
          },
          {
            "number": 5212,
            "title": "[Bug] Llama4 OOM with 400k input request",
            "state": "closed",
            "created_at": "2025-04-10T00:22:53Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              "CUDA out of memory. Tried to allocate 3.43 GiB. GPU 5 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679812 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
              "CUDA out of memory. Tried to allocate 3.43 GiB. GPU 6 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679813 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
            ]
          },
          {
            "number": 3633,
            "title": "[Bug] fused_moe OOM when run deepseek-r1 with --speculative-algo NEXTN",
            "state": "closed",
            "created_at": "2025-02-17T10:23:32Z",
            "labels": [
              "bug",
              "high priority",
              "inactive",
              "deepseek"
            ],
            "errors": [
              "log:",
              "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 7 has a total capacity of 95.00 GiB of which 638.31 MiB is free. Process 2047768 has 94.37 GiB memory in use. Of the allocat"
            ]
          }
        ],
        "crash": [
          {
            "number": 6514,
            "title": "[Bug] start_profile interface makes server crash",
            "state": "closed",
            "created_at": "2025-05-22T03:27:26Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "s stack is below",
              "'NoneType' object has no attribute 'append'`"
            ]
          },
          {
            "number": 6496,
            "title": "[Bug] Crash/Hang during CUDA graph capture on H100*2",
            "state": "open",
            "created_at": "2025-05-21T08:19:53Z",
            "labels": [
              "bug"
            ],
            "errors": [
              "Segmentation fault",
              "Segmentation fault"
            ]
          },
          {
            "number": 5170,
            "title": "[Bug] Llama 4 CUDA assertion error on long input length with fa3 backend",
            "state": "closed",
            "created_at": "2025-04-08T23:30:59Z",
            "labels": [
              "bug",
              "high priority"
            ],
            "errors": [
              ". Once I remove the `--attention-backend=fa3` flag, the long requests can be served successfully.",
              "log is too long so I just included the first few lines. Let me know if more info is needed!"
            ]
          },
          {
            "number": 5064,
            "title": "[Feature] attention backend default choice",
            "state": "closed",
            "created_at": "2025-04-04T08:13:51Z",
            "labels": [
              "high priority",
              "collaboration",
              "flashinfer",
              "performance",
              "MLLM",
              "deepseek"
            ],
            "errors": []
          },
          {
            "number": 4805,
            "title": "[Feature] VLM performance optimization",
            "state": "closed",
            "created_at": "2025-03-27T05:17:30Z",
            "labels": [
              "high priority",
              "inactive",
              "performance"
            ],
            "errors": []
          }
        ]
      }
    }
  },
  "common_categories": {
    "concurrency": {
      "count": 690,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "gpu": {
      "count": 887,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "api": {
      "count": 1070,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "model": {
      "count": 944,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "crash": {
      "count": 250,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "scaling": {
      "count": 350,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "performance": {
      "count": 247,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "memory": {
      "count": 96,
      "frameworks": [
        "vLLM",
        "llama.cpp",
        "SGLang"
      ]
    },
    "other": {
      "count": 2,
      "frameworks": [
        "llama.cpp"
      ]
    }
  },
  "analysis_date": "2025-07-18T10:59:17.891886"
}
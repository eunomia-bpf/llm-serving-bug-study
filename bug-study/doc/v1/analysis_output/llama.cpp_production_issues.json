{
  "name": "llama.cpp",
  "total_production_issues": 4740,
  "bug_issues": 2601,
  "categories": {
    "memory": 45,
    "concurrency": 233,
    "gpu": 383,
    "api": 470,
    "model": 406,
    "scaling": 153,
    "crash": 138,
    "performance": 64,
    "other": 2
  },
  "samples": {
    "memory": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14719,
        "title": "Misc. bug: Llama server use some of the vram on another GPU, even I set -mg 1 and -sm 'none'",
        "state": "open",
        "created_at": "2025-07-16T10:00:14Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14692,
        "title": "Eval bug: CUDA error: operation not supported",
        "state": "open",
        "created_at": "2025-07-15T10:15:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "when requesting.",
          "CUDA error: operation not supported"
        ]
      },
      {
        "number": 14458,
        "title": "Misc. bug: oom \uff0cThe process does not exit.",
        "state": "closed",
        "created_at": "2025-06-30T09:09:44Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "out of memory",
          "### First Bad Commit"
        ]
      }
    ],
    "concurrency": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14747,
        "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
        "state": "open",
        "created_at": "2025-07-17T22:31:51Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "[New LWP 33273]"
        ]
      },
      {
        "number": 14745,
        "title": "Compile bug: GGML Vulkan :: \"format not a string literal and no format arguments [-Werror=format-security]\"",
        "state": "open",
        "created_at": "2025-07-17T18:30:40Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "format not a string literal and no format arguments [-Werror=format-security]",
          "format not a string literal and no format arguments [-Werror=format-security]"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14734,
        "title": "Regarding the build for 8060S (gfx1151):",
        "state": "open",
        "created_at": "2025-07-17T12:09:48Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "no matching function for call to 'hipblasGemmEx'",
          "_str)"
        ]
      }
    ],
    "gpu": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14747,
        "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
        "state": "open",
        "created_at": "2025-07-17T22:31:51Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "[New LWP 33273]"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14734,
        "title": "Regarding the build for 8060S (gfx1151):",
        "state": "open",
        "created_at": "2025-07-17T12:09:48Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "no matching function for call to 'hipblasGemmEx'",
          "_str)"
        ]
      },
      {
        "number": 14727,
        "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
        "state": "open",
        "created_at": "2025-07-16T18:41:41Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      }
    ],
    "api": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14747,
        "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
        "state": "open",
        "created_at": "2025-07-17T22:31:51Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "[New LWP 33273]"
        ]
      },
      {
        "number": 14745,
        "title": "Compile bug: GGML Vulkan :: \"format not a string literal and no format arguments [-Werror=format-security]\"",
        "state": "open",
        "created_at": "2025-07-17T18:30:40Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "format not a string literal and no format arguments [-Werror=format-security]",
          "format not a string literal and no format arguments [-Werror=format-security]"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14738,
        "title": "Misc. bug: b5921 release zip on github misses llama-embedding binary",
        "state": "open",
        "created_at": "2025-07-17T13:47:46Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      }
    ],
    "model": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14747,
        "title": "Misc. bug: RPC flash attention bug on deepseek models (deepseek/kimi k2)",
        "state": "open",
        "created_at": "2025-07-17T22:31:51Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "[New LWP 33273]"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14734,
        "title": "Regarding the build for 8060S (gfx1151):",
        "state": "open",
        "created_at": "2025-07-17T12:09:48Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "no matching function for call to 'hipblasGemmEx'",
          "_str)"
        ]
      },
      {
        "number": 14727,
        "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
        "state": "open",
        "created_at": "2025-07-16T18:41:41Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      }
    ],
    "scaling": [
      {
        "number": 14752,
        "title": "Eval bug: Nemotron 49b doesnt load correctly",
        "state": "open",
        "created_at": "2025-07-18T07:24:58Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "bcs it doesnt use the second gpu:",
          "alberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$"
        ]
      },
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14727,
        "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
        "state": "open",
        "created_at": "2025-07-16T18:41:41Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14709,
        "title": "Compile bug: [SYCL][ARC A770] Regression: \u53cc A770 \u652f\u6301\u5728 b5422 \u53ca\u4ee5\u540e\u7248\u672c\u5931\u6548",
        "state": "open",
        "created_at": "2025-07-16T03:50:20Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "caught at file:D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp, line:3900"
        ]
      },
      {
        "number": 14698,
        "title": "Eval bug: Gemma 3n on Vulkan fails to load",
        "state": "open",
        "created_at": "2025-07-15T13:32:04Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "OutOfDeviceMemory",
          "loading model: unable to allocate Vulkan0 buffer"
        ]
      }
    ],
    "crash": [
      {
        "number": 14740,
        "title": "Misc. bug: out of memory error after PR #13746",
        "state": "open",
        "created_at": "2025-07-17T16:01:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.",
          "```"
        ]
      },
      {
        "number": 14727,
        "title": "Eval bug: Nondeterministic output with ROCm backend despite zero temperature",
        "state": "open",
        "created_at": "2025-07-16T18:41:41Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14713,
        "title": "Misc. bug: llamacpp crashes my PC whenever I close the console for it.",
        "state": "open",
        "created_at": "2025-07-16T06:00:11Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14706,
        "title": "Eval bug: Llama-server on Mac starting at b5478 only producing 2-3 streaming tokens on Qwen3 and Deepseek R1 0528",
        "state": "closed",
        "created_at": "2025-07-16T00:29:03Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14697,
        "title": "Eval bug: Regression: Tool calls still returned in content field as JSON string instead of tool_calls array",
        "state": "open",
        "created_at": "2025-07-15T13:07:17Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "('Only text and image blocks are supported in message content!') }}",
          "('Only user, systemm assistant and tool roles are supported in the custom template made by Unsloth!') }}"
        ]
      }
    ],
    "performance": [
      {
        "number": 14724,
        "title": "Misc. bug: Hybrid models failing to load with assert GGML_ASSERT(kv_size % n_pad == 0)",
        "state": "closed",
        "created_at": "2025-07-16T15:11:45Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "#0  llama_kv_cache_unified::llama_kv_cache_unified(llama_model const&, std::function<bool (int)>&&, ggml_type, ggml_type, bool, bool, bool, unsigned int, unsigned int, unsigned int, unsigned int, llama_swa_type) (this=0x5555585f7e60, "
        ]
      },
      {
        "number": 14697,
        "title": "Eval bug: Regression: Tool calls still returned in content field as JSON string instead of tool_calls array",
        "state": "open",
        "created_at": "2025-07-15T13:07:17Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "('Only text and image blocks are supported in message content!') }}",
          "('Only user, systemm assistant and tool roles are supported in the custom template made by Unsloth!') }}"
        ]
      },
      {
        "number": 14583,
        "title": "Eval bug:[5808-] qwen3 30B vulkan run with  GGG",
        "state": "closed",
        "created_at": "2025-07-08T13:54:07Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14550,
        "title": "Eval bug: Mistral-Small not working on vulkan",
        "state": "closed",
        "created_at": "2025-07-06T11:41:14Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 14469,
        "title": "Eval bug: Gemma vision head (possibly Siglip) yields garbage on vulkan / sycl on Intel N150",
        "state": "open",
        "created_at": "2025-06-30T23:09:30Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      }
    ],
    "other": [
      {
        "number": 14533,
        "title": "Misc. bug: Inconsistency between llama cpp server values and transformers library for reranking",
        "state": "closed",
        "created_at": "2025-07-04T13:05:18Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": []
      },
      {
        "number": 13170,
        "title": "Compile bug: Build fails on ppc64le",
        "state": "closed",
        "created_at": "2025-04-29T05:21:09Z",
        "labels": [
          "bug-unconfirmed"
        ],
        "errors": [
          "s. ",
          "cannot convert \u2018float\u2019 to \u2018__vector float\u2019 {aka \u2018__vector(4) float\u2019} in initialization"
        ]
      }
    ]
  }
}
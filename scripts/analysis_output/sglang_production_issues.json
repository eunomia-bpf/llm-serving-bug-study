{
  "name": "SGLang",
  "total_production_issues": 2201,
  "bug_issues": 106,
  "categories": {
    "gpu": 74,
    "api": 103,
    "model": 81,
    "performance": 28,
    "concurrency": 61,
    "scaling": 13,
    "memory": 7,
    "crash": 16
  },
  "samples": {
    "gpu": [
      {
        "number": 7951,
        "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
        "state": "open",
        "created_at": "2025-07-11T10:58:55Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": []
      },
      {
        "number": 7162,
        "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
        "state": "closed",
        "created_at": "2025-06-13T21:30:30Z",
        "labels": [
          "bug"
        ],
        "errors": [
          ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
          "trace:"
        ]
      },
      {
        "number": 7026,
        "title": "[Bug] PD+MTP+ DeepEP+dp attention",
        "state": "open",
        "created_at": "2025-06-10T03:48:01Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "CUDA error: an illegal memory access was encountered",
          "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
        ]
      },
      {
        "number": 6933,
        "title": "[Bug] Llama-4-Scout OOM with image requests",
        "state": "open",
        "created_at": "2025-06-06T22:15:48Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "during our image benchmark.",
          "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        ]
      },
      {
        "number": 6914,
        "title": "[Bug] Scheduler CPU Memory leak.",
        "state": "open",
        "created_at": "2025-06-06T06:59:51Z",
        "labels": [
          "bug"
        ],
        "errors": []
      }
    ],
    "api": [
      {
        "number": 7951,
        "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
        "state": "open",
        "created_at": "2025-07-11T10:58:55Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": []
      },
      {
        "number": 7551,
        "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
        "state": "open",
        "created_at": "2025-06-26T06:19:16Z",
        "labels": [
          "bug",
          "router"
        ],
        "errors": [
          "Messages**: Failures occur silently without indicating resource exhaustion",
          "s**: Send 503 Service Unavailable when at capacity"
        ]
      },
      {
        "number": 7162,
        "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
        "state": "closed",
        "created_at": "2025-06-13T21:30:30Z",
        "labels": [
          "bug"
        ],
        "errors": [
          ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
          "trace:"
        ]
      },
      {
        "number": 7124,
        "title": "[Bug] llama 3 405b fb fp8 issue",
        "state": "open",
        "created_at": "2025-06-12T08:58:27Z",
        "labels": [
          "bug",
          "good first issue",
          "help wanted",
          "high priority"
        ],
        "errors": []
      },
      {
        "number": 7062,
        "title": "[Bug] test_lora.py bug",
        "state": "closed",
        "created_at": "2025-06-10T18:10:20Z",
        "labels": [
          "bug",
          "lora"
        ],
        "errors": []
      }
    ],
    "model": [
      {
        "number": 7951,
        "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
        "state": "open",
        "created_at": "2025-07-11T10:58:55Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": []
      },
      {
        "number": 7551,
        "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
        "state": "open",
        "created_at": "2025-06-26T06:19:16Z",
        "labels": [
          "bug",
          "router"
        ],
        "errors": [
          "Messages**: Failures occur silently without indicating resource exhaustion",
          "s**: Send 503 Service Unavailable when at capacity"
        ]
      },
      {
        "number": 7162,
        "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
        "state": "closed",
        "created_at": "2025-06-13T21:30:30Z",
        "labels": [
          "bug"
        ],
        "errors": [
          ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
          "trace:"
        ]
      },
      {
        "number": 7062,
        "title": "[Bug] test_lora.py bug",
        "state": "closed",
        "created_at": "2025-06-10T18:10:20Z",
        "labels": [
          "bug",
          "lora"
        ],
        "errors": []
      },
      {
        "number": 7026,
        "title": "[Bug] PD+MTP+ DeepEP+dp attention",
        "state": "open",
        "created_at": "2025-06-10T03:48:01Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "CUDA error: an illegal memory access was encountered",
          "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
        ]
      }
    ],
    "performance": [
      {
        "number": 7551,
        "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
        "state": "open",
        "created_at": "2025-06-26T06:19:16Z",
        "labels": [
          "bug",
          "router"
        ],
        "errors": [
          "Messages**: Failures occur silently without indicating resource exhaustion",
          "s**: Send 503 Service Unavailable when at capacity"
        ]
      },
      {
        "number": 7026,
        "title": "[Bug] PD+MTP+ DeepEP+dp attention",
        "state": "open",
        "created_at": "2025-06-10T03:48:01Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "CUDA error: an illegal memory access was encountered",
          "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
        ]
      },
      {
        "number": 6933,
        "title": "[Bug] Llama-4-Scout OOM with image requests",
        "state": "open",
        "created_at": "2025-06-06T22:15:48Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "during our image benchmark.",
          "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        ]
      },
      {
        "number": 6906,
        "title": "[Bug] FMHA using flashinfer cutlass on Blackwell has low accuracy result",
        "state": "closed",
        "created_at": "2025-06-05T22:15:07Z",
        "labels": [
          "bug",
          "high priority",
          "flashinfer"
        ],
        "errors": []
      },
      {
        "number": 6753,
        "title": "[Bug] PD Failed to register memory on H200",
        "state": "open",
        "created_at": "2025-05-29T23:27:04Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
          "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
        ]
      }
    ],
    "concurrency": [
      {
        "number": 7551,
        "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
        "state": "open",
        "created_at": "2025-06-26T06:19:16Z",
        "labels": [
          "bug",
          "router"
        ],
        "errors": [
          "Messages**: Failures occur silently without indicating resource exhaustion",
          "s**: Send 503 Service Unavailable when at capacity"
        ]
      },
      {
        "number": 7026,
        "title": "[Bug] PD+MTP+ DeepEP+dp attention",
        "state": "open",
        "created_at": "2025-06-10T03:48:01Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "CUDA error: an illegal memory access was encountered",
          "s might be asynchronously reported at some other API call, so the stacktrace below might be incorrect."
        ]
      },
      {
        "number": 6933,
        "title": "[Bug] Llama-4-Scout OOM with image requests",
        "state": "open",
        "created_at": "2025-06-06T22:15:48Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "during our image benchmark.",
          "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        ]
      },
      {
        "number": 6906,
        "title": "[Bug] FMHA using flashinfer cutlass on Blackwell has low accuracy result",
        "state": "closed",
        "created_at": "2025-06-05T22:15:07Z",
        "labels": [
          "bug",
          "high priority",
          "flashinfer"
        ],
        "errors": []
      },
      {
        "number": 6753,
        "title": "[Bug] PD Failed to register memory on H200",
        "state": "open",
        "created_at": "2025-05-29T23:27:04Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
          "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
        ]
      }
    ],
    "scaling": [
      {
        "number": 7551,
        "title": "[Router]  [Performance] SGLang hits connection limit at ~32k concurrent requests, causing failures",
        "state": "open",
        "created_at": "2025-06-26T06:19:16Z",
        "labels": [
          "bug",
          "router"
        ],
        "errors": [
          "Messages**: Failures occur silently without indicating resource exhaustion",
          "s**: Send 503 Service Unavailable when at capacity"
        ]
      },
      {
        "number": 7162,
        "title": "[Bug] When use Lora, if schedule policy is lpm, then it will raise AttributeError: 'ChunkCache' object has no attribute 'disable'",
        "state": "closed",
        "created_at": "2025-06-13T21:30:30Z",
        "labels": [
          "bug"
        ],
        "errors": [
          ". But if use fcfs schedule policy, then it works. From my understanding, when use Lora, we have to set `disable_radix_cache=True`, then in  `init_memory_pool_and_cache` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/scheduler.py#L541 function, the tree cache is initialized with `ChunkCache` which does not have `disable`.   In `_validate_and_adjust_policy` https://github.com/sgl-project/sglang/blob/0f1dfa1efe7e40860c3ffc9bc8b33f9a319f78dc/python/sglang/srt/managers/schedule_policy.py#L129, the error line is in a try-cache and final error should be `ValueError(f\"Unknown schedule_policy: {policy=}\")` but it does not logged in the output for some reason.  I could not find any documentation to discuss this case.  I think a mechanism should be added to prevent the error at the initialization step when LORA is used. ",
          "trace:"
        ]
      },
      {
        "number": 6753,
        "title": "[Bug] PD Failed to register memory on H200",
        "state": "open",
        "created_at": "2025-05-29T23:27:04Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
          "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
        ]
      },
      {
        "number": 6592,
        "title": "[Bug] load microsoft/MAI-DS-R1 error: KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'",
        "state": "open",
        "created_at": "2025-05-25T13:37:42Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "occurred\uff1a**KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**",
          "'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**"
        ]
      },
      {
        "number": 6496,
        "title": "[Bug] Crash/Hang during CUDA graph capture on H100*2",
        "state": "open",
        "created_at": "2025-05-21T08:19:53Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "Segmentation fault",
          "Segmentation fault"
        ]
      }
    ],
    "memory": [
      {
        "number": 6933,
        "title": "[Bug] Llama-4-Scout OOM with image requests",
        "state": "open",
        "created_at": "2025-06-06T22:15:48Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "during our image benchmark.",
          "CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 137.00 MiB is free. Process 235237 has 946.00 MiB memory in use. Process 235597 has 78.37 GiB memory in use. Of the allocated memory 74.79 GiB is allocated by PyTorch, with 38.04 MiB allocated in private pools (e.g., CUDA Graphs), and 811.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        ]
      },
      {
        "number": 6914,
        "title": "[Bug] Scheduler CPU Memory leak.",
        "state": "open",
        "created_at": "2025-06-06T06:59:51Z",
        "labels": [
          "bug"
        ],
        "errors": []
      },
      {
        "number": 6753,
        "title": "[Bug] PD Failed to register memory on H200",
        "state": "open",
        "created_at": "2025-05-29T23:27:04Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781",
          "(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead"
        ]
      },
      {
        "number": 5212,
        "title": "[Bug] Llama4 OOM with 400k input request",
        "state": "closed",
        "created_at": "2025-04-10T00:22:53Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          "CUDA out of memory. Tried to allocate 3.43 GiB. GPU 5 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679812 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "CUDA out of memory. Tried to allocate 3.43 GiB. GPU 6 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679813 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        ]
      },
      {
        "number": 3633,
        "title": "[Bug] fused_moe OOM when run deepseek-r1 with --speculative-algo NEXTN",
        "state": "closed",
        "created_at": "2025-02-17T10:23:32Z",
        "labels": [
          "bug",
          "high priority",
          "inactive",
          "deepseek"
        ],
        "errors": [
          "log:",
          "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 7 has a total capacity of 95.00 GiB of which 638.31 MiB is free. Process 2047768 has 94.37 GiB memory in use. Of the allocat"
        ]
      }
    ],
    "crash": [
      {
        "number": 6514,
        "title": "[Bug] start_profile interface makes server crash",
        "state": "closed",
        "created_at": "2025-05-22T03:27:26Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "s stack is below",
          "'NoneType' object has no attribute 'append'`"
        ]
      },
      {
        "number": 6496,
        "title": "[Bug] Crash/Hang during CUDA graph capture on H100*2",
        "state": "open",
        "created_at": "2025-05-21T08:19:53Z",
        "labels": [
          "bug"
        ],
        "errors": [
          "Segmentation fault",
          "Segmentation fault"
        ]
      },
      {
        "number": 5170,
        "title": "[Bug] Llama 4 CUDA assertion error on long input length with fa3 backend",
        "state": "closed",
        "created_at": "2025-04-08T23:30:59Z",
        "labels": [
          "bug",
          "high priority"
        ],
        "errors": [
          ". Once I remove the `--attention-backend=fa3` flag, the long requests can be served successfully.",
          "log is too long so I just included the first few lines. Let me know if more info is needed!"
        ]
      },
      {
        "number": 5064,
        "title": "[Feature] attention backend default choice",
        "state": "closed",
        "created_at": "2025-04-04T08:13:51Z",
        "labels": [
          "high priority",
          "collaboration",
          "flashinfer",
          "performance",
          "MLLM",
          "deepseek"
        ],
        "errors": []
      },
      {
        "number": 4805,
        "title": "[Feature] VLM performance optimization",
        "state": "closed",
        "created_at": "2025-03-27T05:17:30Z",
        "labels": [
          "high priority",
          "inactive",
          "performance"
        ],
        "errors": []
      }
    ]
  }
}